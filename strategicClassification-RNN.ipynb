{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import cvxpy as cp\n",
    "import dccp\n",
    "import torch\n",
    "import numpy as np\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import zero_one_loss, confusion_matrix\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.patches as mpatches\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import os, psutil\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "XDIM = 82\n",
    "COST = 0.005\n",
    "SLOPE_C = 0.5\n",
    "X_LOWER_BOUND = -10\n",
    "X_UPPER_BOUND = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, Y, percentage):\n",
    "    num_val = int(len(X)*percentage)\n",
    "    return X[num_val:], Y[num_val:], X[:num_val], Y[:num_val]\n",
    "\n",
    "def shuffle(X, Y):\n",
    "    data = torch.cat((X, Y), 1)\n",
    "    data = data[torch.randperm(data.size()[0])]\n",
    "    X = data[:, :2]\n",
    "    Y = data[:, 2]\n",
    "    return X, Y\n",
    "\n",
    "def conf_mat(Y1, Y2):\n",
    "    num_of_samples = len(Y1)\n",
    "    mat = confusion_matrix(Y1, Y2, labels=[-1, 1])*100/num_of_samples\n",
    "    acc = np.trace(mat)\n",
    "    return mat, acc\n",
    "\n",
    "def pred(X, w, b):\n",
    "    return torch.sign(score(X, w, b))\n",
    "\n",
    "def calc_accuracy(Y, Ypred):\n",
    "    num = len(Y)\n",
    "    temp = Y - Ypred\n",
    "    acc = len(temp[temp == 0])*1./num\n",
    "    return acc\n",
    "\n",
    "def evaluate_model(X, Y, w, b, ccp, strategic):\n",
    "    if not strategic:\n",
    "        Xopt = X\n",
    "    else:\n",
    "        Xopt = ccp.optimize_X(X, w, b)\n",
    "    Ypred = pred(Xopt, w, b)\n",
    "    return calc_accuracy(Y, Ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_financial_distress():\n",
    "    data = pd.read_csv(\"C:/Users/sagil/Desktop/nir project/financial_distress_data/Financial Distress.csv\")\n",
    "\n",
    "    data = data[data.columns.drop(list(data.filter(regex='x80')))] # Since it is a categorical feature with 37 features.\n",
    "    x_dim = len(data.columns) - 3\n",
    "    seq_len = data['Time'].max()\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    data_grouped = data.groupby(['Company']).last()\n",
    "\n",
    "    normalized_data=(data-data.mean())/data.std()\n",
    "\n",
    "    for company_num in data_grouped.index:\n",
    "        x = torch.tensor(normalized_data[data['Company'] == company_num].values)\n",
    "        x = x[:,3:]\n",
    "        x_seq_len = x.size()[0]\n",
    "        if x_seq_len < seq_len:\n",
    "            pad = torch.zeros((seq_len-x_seq_len, x_dim))\n",
    "            x = torch.cat((pad, x), 0)\n",
    "        y = data_grouped.iloc[company_num-1, 1]\n",
    "        y = -1 if y < -0.5 else 1\n",
    "        X.append(x[11:, :])\n",
    "        Y.append(y)\n",
    "\n",
    "    XY = list(zip(X, Y))\n",
    "    random.shuffle(XY)\n",
    "    tmp = [list(t) for t in zip(*XY)]\n",
    "    X = torch.stack(tmp[0])\n",
    "    Y = torch.tensor(tmp[1])\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCP classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCP:\n",
    "    def __init__(self, x_dim, h_dim, funcs):\n",
    "        self.f_derivative = funcs[\"f_derivative\"]\n",
    "        self.g = funcs[\"g\"]\n",
    "        self.c = funcs[\"c\"]\n",
    "        \n",
    "        self.x = cp.Variable(x_dim)\n",
    "        self.xt = cp.Parameter(x_dim)\n",
    "        self.r = cp.Parameter(x_dim)\n",
    "        self.h = cp.Parameter(h_dim)\n",
    "        self.w_hy = cp.Parameter(h_dim)\n",
    "        self.w_hh = cp.Parameter((h_dim, h_dim))\n",
    "        self.w_xh = cp.Parameter((h_dim, x_dim))\n",
    "        self.b = cp.Parameter(h_dim)\n",
    "\n",
    "        target = self.x@self.f_derivative(self.xt, self.h, self.w_hy, self.w_hh, self.w_xh, self.b)-self.g(self.x, self.h, self.w_hy, self.w_hh, self.w_xh, self.b)-self.c(self.x, self.r)\n",
    "        constraints = [self.x >= X_LOWER_BOUND,\n",
    "                       self.x <= X_UPPER_BOUND]\n",
    "        self.prob = cp.Problem(cp.Maximize(target), constraints)\n",
    "        \n",
    "        print(\"problem is DCP:\", self.prob.is_dcp())\n",
    "        print(\"problem is DPP:\", self.prob.is_dpp())\n",
    "        \n",
    "    def ccp(self, r, h):\n",
    "        \"\"\"\n",
    "        numpy to numpy\n",
    "        \"\"\"\n",
    "        self.xt.value = r\n",
    "        self.r.value = r\n",
    "        self.h.value = h\n",
    "        result = self.prob.solve()\n",
    "        diff = np.linalg.norm(self.xt.value - self.x.value)\n",
    "        while diff > 0.0001:\n",
    "            self.xt.value = self.x.value\n",
    "            result = self.prob.solve()\n",
    "            diff = np.linalg.norm(self.x.value - self.xt.value)\n",
    "        return self.x.value\n",
    "    \n",
    "    def optimize_X(self, X, H, w_hy, w_hh, w_xh, b):\n",
    "        \"\"\"\n",
    "        tensor to tensor\n",
    "        \"\"\"\n",
    "        w_hy = w_hy.detach().numpy()\n",
    "        w_hh = w_hh.detach().numpy()\n",
    "        w_xh = w_xh.detach().numpy()\n",
    "        b = b.detach().numpy()\n",
    "        X = X.numpy()\n",
    "        H = H.detach().numpy()\n",
    "        \n",
    "        self.w_hy.value = w_hy\n",
    "        self.w_hh.value = w_hh\n",
    "        self.w_xh.value = w_xh\n",
    "        self.b.value = b\n",
    "        \n",
    "        return torch.stack([torch.from_numpy(self.ccp(x, h)) for x, h in zip(X, H)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DELTA():\n",
    "    \n",
    "    def __init__(self, x_dim, h_dim, funcs):\n",
    "        self.g = funcs[\"g_dpp_form\"]\n",
    "        self.c = funcs[\"c\"]\n",
    "        \n",
    "        self.x = cp.Variable(x_dim)\n",
    "        self.r = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.h__w_hh_hy = cp.Parameter(1, value = np.random.randn(1))\n",
    "        self.w_xh_hy = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.w_b_hy = cp.Parameter(1, value = np.random.randn(1))\n",
    "        self.f_der = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "\n",
    "        target = self.x@self.f_der-self.g(self.x, self.h__w_hh_hy, self.w_xh_hy, self.w_b_hy)-self.c(self.x, self.r)\n",
    "        constraints = [self.x >= X_LOWER_BOUND,\n",
    "                       self.x <= X_UPPER_BOUND]\n",
    "        objective = cp.Maximize(target)\n",
    "        problem = cp.Problem(objective, constraints)\n",
    "        self.layer = CvxpyLayer(problem, parameters=[self.r, self.h__w_hh_hy, self.w_xh_hy, self.w_b_hy, self.f_der],\n",
    "                                variables=[self.x])\n",
    "        \n",
    "        \n",
    "    def optimize_X(self, X, H, w_hy, w_hh, w_xh, b, F_DER):\n",
    "        h__w_hh_hy = H@(w_hy@w_hh).T\n",
    "        h__w_hh_hy = h__w_hh_hy.reshape(h__w_hh_hy.size()[0], 1)\n",
    "        w_xh_hy = w_hy@w_xh\n",
    "        w_b_hy = b@w_hy.T\n",
    "        w_b_hy = w_b_hy.reshape(1)\n",
    "        return self.layer(X, h__w_hh_hy, w_xh_hy, w_b_hy, F_DER)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gain & Cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(x, h, w_hy, w_hh, w_xh, b):\n",
    "    return (h@w_hh.T + x@w_xh.T + b)@w_hy.T\n",
    "\n",
    "def score_dpp_form(x, h__w_hh_hy, w_xh_hy, w_b_hy):\n",
    "    return h__w_hh_hy + x@w_xh_hy.T + w_b_hy\n",
    "\n",
    "def f(x, h, w_hy, w_hh, w_xh, b):\n",
    "    return 0.5*cp.norm(cp.hstack([1, (SLOPE_C*score(x, h, w_hy, w_hh, w_xh, b) + 1)]), 2)\n",
    "\n",
    "def g(x, h, w_hy, w_hh, w_xh, b):\n",
    "    return 0.5*cp.norm(cp.hstack([1, (SLOPE_C*score(x, h, w_hy, w_hh, w_xh, b) - 1)]), 2)\n",
    "\n",
    "def g_dpp_form(x, h__w_hh_hy, w_xh_hy, w_b_hy):\n",
    "    return 0.5*cp.norm(cp.hstack([1, (SLOPE_C*score_dpp_form(x, h__w_hh_hy, w_xh_hy, w_b_hy) - 1)]), 2)\n",
    "\n",
    "def c(x, r):\n",
    "    return COST*cp.sum_squares(x-r)\n",
    "\n",
    "def f_derivative(x, h, w_hy, w_hh, w_xh, b):\n",
    "    return 0.5*SLOPE_C*((SLOPE_C*score(x, h, w_hy, w_hh, w_xh, b) + 1)\n",
    "                        /cp.sqrt((SLOPE_C*score(x, h, w_hy, w_hh, w_xh, b) + 1)**2 + 1))*(w_hy@w_xh)\n",
    "\n",
    "funcs = {\"f\": f, \"g\": g, \"f_derivative\": f_derivative, \"c\": c, \"score\": score,\n",
    "         \"score_dpp_form\": score_dpp_form, \"g_dpp_form\": g_dpp_form}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([317, 3, 82])\n",
      "percent of positive samples: 67.50788643533123%\n"
     ]
    }
   ],
   "source": [
    "X, Y = load_financial_distress()\n",
    "\n",
    "assert(len(X[0][0]) == XDIM)\n",
    "X, Y, Xval, Yval = split_data(X, Y, 0.25)\n",
    "print(X.size())\n",
    "\n",
    "print(\"percent of positive samples: {}%\".format(100 * len(Y[Y == 1]) / len(Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(torch.nn.Module):\n",
    "    def __init__(self, x_dim, h_dim, funcs, strategic=False):\n",
    "        \n",
    "        super(MyRNN, self).__init__()\n",
    "        self.h_dim = h_dim\n",
    "        self.x_dim = x_dim\n",
    "        self.W_hh = torch.nn.parameter.Parameter(math.sqrt(1/h_dim)*(1-2*torch.rand((h_dim, h_dim), dtype=torch.float64, requires_grad=True)))\n",
    "        self.W_xh = torch.nn.parameter.Parameter(math.sqrt(1/x_dim)*(1-2*torch.rand((h_dim, x_dim), dtype=torch.float64, requires_grad=True)))\n",
    "        self.W_hy = torch.nn.parameter.Parameter(math.sqrt(1/h_dim)*(1-2*torch.rand(h_dim, dtype=torch.float64, requires_grad=True)))\n",
    "        self.b = torch.nn.parameter.Parameter(math.sqrt(1/h_dim)*(1-2*torch.rand(h_dim, dtype=torch.float64, requires_grad=True)))\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.strategic = strategic\n",
    "        self.ccp = CCP(x_dim, h_dim, funcs)\n",
    "        self.delta = DELTA(x_dim, h_dim, funcs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size, seq_len, _ = X.size()  # B, 14, 82\n",
    "        # assert(seq_len == 14)\n",
    "        X = X.transpose(1,0)\n",
    "        \n",
    "        H = math.sqrt(1/h_dim)*(1-2*torch.rand((batch_size, h_dim), dtype=torch.float64, requires_grad=False))\n",
    "        for x in X[:-1]:\n",
    "            H = self.sigmoid(H@self.W_hh.T + x@self.W_xh.T + self.b)\n",
    "        \n",
    "        x = X[-1]\n",
    "        if self.strategic:\n",
    "            XT = self.ccp.optimize_X(x, H, self.W_hy, self.W_hh, self.W_xh, self.b)\n",
    "            F_DER = self.get_f_ders(XT, H)\n",
    "            x_opt = self.delta.optimize_X(x, H, self.W_hy, self.W_hh, self.W_xh, self.b, F_DER)\n",
    "            H = (H@self.W_hh.T + x_opt@self.W_xh.T + self.b)\n",
    "        else:\n",
    "            H = (H@self.W_hh.T + x@self.W_xh.T + self.b)\n",
    "        \n",
    "        output = H@self.W_hy.T    \n",
    "        return output\n",
    "    \n",
    "    def optimize_X(self, X):\n",
    "        batch_size, seq_len, _ = X.size()  # B, 14, 82\n",
    "        X = X.transpose(1,0)\n",
    "        \n",
    "        H = math.sqrt(1/h_dim)*(1-2*torch.rand((batch_size, h_dim), dtype=torch.float64, requires_grad=False))\n",
    "        for x in X[:-1]:\n",
    "            H = self.sigmoid(H@self.W_hh.T + x@self.W_xh.T + self.b)\n",
    "        \n",
    "        x = X[-1]\n",
    "        x = self.ccp.optimize_X(x, H, self.W_hy, self.W_hh, self.W_xh, self.b).reshape(1, x.size()[0], x.size()[1])\n",
    "        X = torch.cat((X[:-1], x), 0)\n",
    "        return X.transpose(1,0)\n",
    "    \n",
    "    def score(self, x, h):\n",
    "        return (h@self.W_hh.T + x@self.W_xh.T + self.b)@self.W_hy.T\n",
    "    \n",
    "    def get_f_ders(self, XT, H):\n",
    "        W_xhhy = self.W_hy@self.W_xh\n",
    "        return torch.stack([0.5*SLOPE_C*((SLOPE_C*self.score(xt, h) + 1)/torch.sqrt((SLOPE_C*self.score(xt, h) + 1)**2 + 1))*W_xhhy for xt, h in zip(XT, H)])\n",
    "\n",
    "    def evaluate(self, X, Y):\n",
    "        Y_pred = torch.sign(self.forward(X))\n",
    "        num = len(Y)\n",
    "        temp = Y - Y_pred\n",
    "        acc = len(temp[temp == 0])*1./num\n",
    "        return acc\n",
    "    \n",
    "    def loss(self, Y, Y_pred):\n",
    "        return torch.mean(torch.clamp(1 - Y_pred * Y, min=0))\n",
    "    \n",
    "    def fit(self, X, Y, Xval, Yval, opt, opt_kwargs={\"lr\":1e-3}, batch_size=128, epochs=100, verbose=False, callback=None, calc_train_errors=False):\n",
    "        train_dset = TensorDataset(X, Y)\n",
    "        train_loader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "        opt = opt(self.parameters(), **opt_kwargs)\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_errors = []\n",
    "        val_errors = []\n",
    "\n",
    "        total_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            t1 = time.time()\n",
    "            batch = 1\n",
    "            train_losses.append([])\n",
    "            train_errors.append([])\n",
    "            for Xbatch, Ybatch in train_loader:\n",
    "                opt.zero_grad()\n",
    "                Ybatch_pred = self.forward(Xbatch)\n",
    "                l = self.loss(Ybatch_pred, Ybatch)\n",
    "                l.backward()\n",
    "                opt.step()\n",
    "                train_losses[-1].append(l.item())\n",
    "                if calc_train_errors:\n",
    "                    with torch.no_grad():\n",
    "                        e = self.evaluate(Xbatch, Ybatch)\n",
    "                        train_errors[-1].append(1-e)\n",
    "                    if verbose:\n",
    "                        print(\"batch %03d / %03d | loss: %3.5f | err: %3.5f\" % \n",
    "                              (batch, len(train_loader), np.mean(train_losses[-1]), np.mean(train_errors[-1])))\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        print(\"batch %03d / %03d | loss: %3.5f\" %\n",
    "                              (batch, len(train_loader), np.mean(train_losses[-1])))\n",
    "                batch += 1\n",
    "                if callback is not None:\n",
    "                    callback()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                Yval_pred = self.forward(Xval, evaluation=True)\n",
    "                l = self.loss(Yval_pred, Yval)\n",
    "                val_losses.append(l.item())\n",
    "                val_errors.append(1-self.evaluate(Xval, Yval))\n",
    "                print(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)\n",
    "\n",
    "            t2 = time.time()\n",
    "            if verbose:\n",
    "                print(\"----- epoch %03d / %03d | time: %03d sec | loss: %3.5f | err: %3.5f\" % (epoch + 1, epochs, t2-t1, val_losses[-1], val_errors[-1]))\n",
    "        print(\"training time: {} seconds\".format(time.time()-total_time)) \n",
    "        return train_errors, val_errors, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- training non-strategically----------\n",
      "problem is DCP: True\n",
      "problem is DPP: False\n",
      "batch 001 / 020 | loss: 0.97537\n",
      "batch 002 / 020 | loss: 0.77794\n",
      "batch 003 / 020 | loss: 0.65269\n",
      "batch 004 / 020 | loss: 0.54178\n",
      "batch 005 / 020 | loss: 0.47324\n",
      "batch 006 / 020 | loss: 0.41647\n",
      "batch 007 / 020 | loss: 0.37750\n",
      "batch 008 / 020 | loss: 0.38898\n",
      "batch 009 / 020 | loss: 0.36570\n",
      "batch 010 / 020 | loss: 0.33249\n",
      "batch 011 / 020 | loss: 0.31160\n",
      "batch 012 / 020 | loss: 0.31976\n",
      "batch 013 / 020 | loss: 0.32410\n",
      "batch 014 / 020 | loss: 0.30095\n",
      "batch 015 / 020 | loss: 0.28681\n",
      "batch 016 / 020 | loss: 0.28468\n",
      "batch 017 / 020 | loss: 0.28712\n",
      "batch 018 / 020 | loss: 0.27310\n",
      "batch 019 / 020 | loss: 0.27288\n",
      "batch 020 / 020 | loss: 0.27111\n",
      "206.0078125\n",
      "----- epoch 001 / 004 | time: 000 sec | loss: 0.12081 | err: 0.05714\n",
      "batch 001 / 020 | loss: 0.15857\n",
      "batch 002 / 020 | loss: 0.30494\n",
      "batch 003 / 020 | loss: 0.24678\n",
      "batch 004 / 020 | loss: 0.23850\n",
      "batch 005 / 020 | loss: 0.19216\n",
      "batch 006 / 020 | loss: 0.16967\n",
      "batch 007 / 020 | loss: 0.19117\n",
      "batch 008 / 020 | loss: 0.17506\n",
      "batch 009 / 020 | loss: 0.17565\n",
      "batch 010 / 020 | loss: 0.17972\n",
      "batch 011 / 020 | loss: 0.17774\n",
      "batch 012 / 020 | loss: 0.17456\n",
      "batch 013 / 020 | loss: 0.17767\n",
      "batch 014 / 020 | loss: 0.18037\n",
      "batch 015 / 020 | loss: 0.19043\n",
      "batch 016 / 020 | loss: 0.18099\n",
      "batch 017 / 020 | loss: 0.18462\n",
      "batch 018 / 020 | loss: 0.18460\n",
      "batch 019 / 020 | loss: 0.17488\n",
      "batch 020 / 020 | loss: 0.16617\n",
      "206.0078125\n",
      "----- epoch 002 / 004 | time: 000 sec | loss: 0.11775 | err: 0.03810\n",
      "batch 001 / 020 | loss: 0.37945\n",
      "batch 002 / 020 | loss: 0.29021\n",
      "batch 003 / 020 | loss: 0.19347\n",
      "batch 004 / 020 | loss: 0.17644\n",
      "batch 005 / 020 | loss: 0.15084\n",
      "batch 006 / 020 | loss: 0.13384\n",
      "batch 007 / 020 | loss: 0.11827\n",
      "batch 008 / 020 | loss: 0.12428\n",
      "batch 009 / 020 | loss: 0.12228\n",
      "batch 010 / 020 | loss: 0.12602\n",
      "batch 011 / 020 | loss: 0.13569\n",
      "batch 012 / 020 | loss: 0.14599\n",
      "batch 013 / 020 | loss: 0.13795\n",
      "batch 014 / 020 | loss: 0.13385\n",
      "batch 015 / 020 | loss: 0.12493\n",
      "batch 016 / 020 | loss: 0.12917\n",
      "batch 017 / 020 | loss: 0.13012\n",
      "batch 018 / 020 | loss: 0.12758\n",
      "batch 019 / 020 | loss: 0.12444\n",
      "batch 020 / 020 | loss: 0.13861\n",
      "206.03515625\n",
      "----- epoch 003 / 004 | time: 000 sec | loss: 0.10841 | err: 0.03810\n",
      "batch 001 / 020 | loss: 0.08813\n",
      "batch 002 / 020 | loss: 0.17079\n",
      "batch 003 / 020 | loss: 0.17708\n",
      "batch 004 / 020 | loss: 0.21044\n",
      "batch 005 / 020 | loss: 0.19116\n",
      "batch 006 / 020 | loss: 0.16895\n",
      "batch 007 / 020 | loss: 0.16482\n",
      "batch 008 / 020 | loss: 0.14895\n",
      "batch 009 / 020 | loss: 0.14726\n",
      "batch 010 / 020 | loss: 0.13254\n",
      "batch 011 / 020 | loss: 0.13202\n",
      "batch 012 / 020 | loss: 0.13280\n",
      "batch 013 / 020 | loss: 0.12844\n",
      "batch 014 / 020 | loss: 0.12822\n",
      "batch 015 / 020 | loss: 0.13619\n",
      "batch 016 / 020 | loss: 0.13097\n",
      "batch 017 / 020 | loss: 0.12354\n",
      "batch 018 / 020 | loss: 0.11849\n",
      "batch 019 / 020 | loss: 0.11524\n",
      "batch 020 / 020 | loss: 0.12172\n",
      "206.03515625\n",
      "----- epoch 004 / 004 | time: 000 sec | loss: 0.13747 | err: 0.05714\n",
      "training time: 0.7112863063812256 seconds\n",
      "---------- training non-strategically----------\n",
      "problem is DCP: True\n",
      "problem is DPP: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:163: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 020 | loss: 1.04007\n",
      "batch 002 / 020 | loss: 0.87258\n",
      "batch 003 / 020 | loss: 0.82660\n",
      "batch 004 / 020 | loss: 0.70969\n",
      "batch 005 / 020 | loss: 0.69278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\problems\\problem.py:1055: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 006 / 020 | loss: 0.72931\n",
      "batch 007 / 020 | loss: 0.67081\n",
      "batch 008 / 020 | loss: 0.68905\n",
      "batch 009 / 020 | loss: 0.67726\n",
      "batch 010 / 020 | loss: 0.64815\n",
      "batch 011 / 020 | loss: 0.62410\n",
      "batch 012 / 020 | loss: 0.60231\n",
      "batch 013 / 020 | loss: 0.59352\n",
      "batch 014 / 020 | loss: 0.57526\n",
      "batch 015 / 020 | loss: 0.56653\n",
      "batch 016 / 020 | loss: 0.57760\n",
      "batch 017 / 020 | loss: 0.56978\n",
      "batch 018 / 020 | loss: 0.56805\n",
      "batch 019 / 020 | loss: 0.55881\n",
      "batch 020 / 020 | loss: 0.55910\n",
      "241.92578125\n",
      "----- epoch 001 / 004 | time: 110 sec | loss: 0.40454 | err: 0.07619\n",
      "batch 001 / 020 | loss: 0.38504\n",
      "batch 002 / 020 | loss: 0.38669\n",
      "batch 003 / 020 | loss: 0.43378\n",
      "batch 004 / 020 | loss: 0.47422\n",
      "batch 005 / 020 | loss: 0.39411\n",
      "batch 006 / 020 | loss: 0.44423\n",
      "batch 007 / 020 | loss: 0.42790\n",
      "batch 008 / 020 | loss: 0.42457\n",
      "batch 009 / 020 | loss: 0.42984\n",
      "batch 010 / 020 | loss: 0.42625\n",
      "batch 011 / 020 | loss: 0.42983\n",
      "batch 012 / 020 | loss: 0.42604\n",
      "batch 013 / 020 | loss: 0.42365\n",
      "batch 014 / 020 | loss: 0.42177\n",
      "batch 015 / 020 | loss: 0.42213\n",
      "batch 016 / 020 | loss: 0.42380\n",
      "batch 017 / 020 | loss: 0.41585\n",
      "batch 018 / 020 | loss: 0.40403\n",
      "batch 019 / 020 | loss: 0.40211\n",
      "batch 020 / 020 | loss: 0.39470\n",
      "242.609375\n",
      "----- epoch 002 / 004 | time: 116 sec | loss: 0.33193 | err: 0.11429\n",
      "batch 001 / 020 | loss: 0.36721\n",
      "batch 002 / 020 | loss: 0.43142\n",
      "batch 003 / 020 | loss: 0.35928\n",
      "batch 004 / 020 | loss: 0.30376\n",
      "batch 005 / 020 | loss: 0.31474\n",
      "batch 006 / 020 | loss: 0.28242\n",
      "batch 007 / 020 | loss: 0.29621\n",
      "batch 008 / 020 | loss: 0.27875\n",
      "batch 009 / 020 | loss: 0.26488\n",
      "batch 010 / 020 | loss: 0.28353\n",
      "batch 011 / 020 | loss: 0.27279\n",
      "batch 012 / 020 | loss: 0.25982\n",
      "batch 013 / 020 | loss: 0.25631\n",
      "batch 014 / 020 | loss: 0.25748\n",
      "batch 015 / 020 | loss: 0.24562\n",
      "batch 016 / 020 | loss: 0.25056\n",
      "batch 017 / 020 | loss: 0.24987\n",
      "batch 018 / 020 | loss: 0.24101\n",
      "batch 019 / 020 | loss: 0.24784\n",
      "batch 020 / 020 | loss: 0.24531\n",
      "257.83984375\n",
      "----- epoch 003 / 004 | time: 135 sec | loss: 0.14744 | err: 0.02857\n",
      "batch 001 / 020 | loss: 0.44590\n",
      "batch 002 / 020 | loss: 0.31224\n",
      "batch 003 / 020 | loss: 0.32827\n",
      "batch 004 / 020 | loss: 0.25942\n",
      "batch 005 / 020 | loss: 0.22140\n",
      "batch 006 / 020 | loss: 0.25889\n",
      "batch 007 / 020 | loss: 0.24437\n",
      "batch 008 / 020 | loss: 0.23331\n",
      "batch 009 / 020 | loss: 0.24190\n",
      "batch 010 / 020 | loss: 0.21978\n",
      "batch 011 / 020 | loss: 0.20354\n",
      "batch 012 / 020 | loss: 0.18952\n",
      "batch 013 / 020 | loss: 0.19176\n",
      "batch 014 / 020 | loss: 0.17933\n",
      "batch 015 / 020 | loss: 0.19095\n",
      "batch 016 / 020 | loss: 0.20572\n",
      "batch 017 / 020 | loss: 0.20913\n",
      "batch 018 / 020 | loss: 0.21446\n",
      "batch 019 / 020 | loss: 0.21283\n",
      "batch 020 / 020 | loss: 0.20724\n",
      "260.70703125\n",
      "----- epoch 004 / 004 | time: 175 sec | loss: 0.15131 | err: 0.05714\n",
      "training time: 538.0845420360565 seconds\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 4\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "N, x_dim, h_dim = len(Y), XDIM, 10\n",
    "\n",
    "# non-strategic classification\n",
    "print(\"---------- training non-strategically----------\")\n",
    "non_strategic_model = MyRNN(x_dim, h_dim, funcs, strategic=False)\n",
    "\n",
    "fit_res_non_strategic = non_strategic_model.fit(X, Y, Xval, Yval,\n",
    "                               opt=torch.optim.Adam, opt_kwargs={\"lr\": (1e-2)},\n",
    "                               batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=True, calc_train_errors=False)\n",
    "\n",
    "# strategic classification\n",
    "print(\"---------- training strategically----------\")\n",
    "strategic_model = MyRNN(x_dim, h_dim, funcs, strategic=True)\n",
    "\n",
    "fit_res_strategic = strategic_model.fit(X, Y, Xval, Yval,\n",
    "                               opt=torch.optim.Adam, opt_kwargs={\"lr\": (1e-2)},\n",
    "                               batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=True, calc_train_errors=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9428571428571428\n",
      "0.9428571428571428\n",
      "0.6857142857142857\n"
     ]
    }
   ],
   "source": [
    "# Xval_opt = ccp.optimize_X(Xval, w_strategic, b_strategic)\n",
    "\n",
    "# FpXp = pred(Xval_opt, w_strategic, b_strategic)\n",
    "# FXp = pred(Xval_opt, w_non_strategic, b_non_strategic)\n",
    "# FpX = pred(Xval, w_strategic, b_strategic)\n",
    "# FX = pred(Xval, w_non_strategic, b_non_strategic)\n",
    "\n",
    "Xval_opt = non_strategic_model.optimize_X(Xval)\n",
    "print(non_strategic_model.evaluate(Xval, Yval))\n",
    "print(strategic_model.evaluate(Xval, Yval))\n",
    "print(non_strategic_model.evaluate(Xval_opt, Yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y vs f(x):\\n{}\\naccuracy: {}\".format(*conf_mat(Yval, FX)))\n",
    "print(\"y vs f(x\\'):\\n{}\\naccuracy: {}\".format(*conf_mat(Yval, FXp)))\n",
    "print(\"y vs f\\'(x):\\n{}\\naccuracy: {}\".format(*conf_mat(Yval, FpX)))\n",
    "print(\"y vs f\\'(x\\'):\\n{}\\naccuracy: {}\".format(*conf_mat(Yval, FpXp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "funcPred",
   "language": "python",
   "name": "funcpred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
