{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import cvxpy as cp\n",
    "import dccp\n",
    "import torch\n",
    "import numpy as np\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import zero_one_loss, confusion_matrix\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.patches as mpatches\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import os, psutil\n",
    "from datetime import datetime\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "TRAIN_SLOPE = 1\n",
    "EVAL_SLOPE = 5\n",
    "X_LOWER_BOUND = -10\n",
    "X_UPPER_BOUND = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spam_data():\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    path = r\"C:\\Users\\sagil\\Desktop\\nir_project\\tip_spam_data\\IS_journal_tip_spam.arff\"\n",
    "    data, meta = arff.loadarff(path)\n",
    "    df = pd.DataFrame(data)\n",
    "    most_disc = ['qTips_plc', 'rating_plc', 'qEmail_tip', 'qContacts_tip', 'qURL_tip', 'qPhone_tip', 'qNumeriChar_tip', 'sentistrength_tip', 'combined_tip', 'qWords_tip', 'followers_followees_gph', 'qunigram_avg_tip', 'qTips_usr', 'indeg_gph', 'qCapitalChar_tip', 'class1']\n",
    "    df = df[most_disc]\n",
    "    df[\"class1\"].replace({b'spam': -1, b'notspam': 1}, inplace=True)\n",
    "    df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "    Y = df['class1'].values\n",
    "    X = df.drop('class1', axis = 1).values\n",
    "    X -= np.mean(X, axis=0)\n",
    "    X /= np.std(X, axis=0)\n",
    "    return torch.from_numpy(X), torch.from_numpy(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_spam_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 1e-4\n",
    "\n",
    "class CAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(15, 12, bias = True) # Encoder\n",
    "        self.fc2 = nn.Linear(12, 15, bias = True) # Decoder\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def encoder(self, x):\n",
    "        h1 = self.sigmoid(self.fc1(x.view(-1, 15)))\n",
    "        return h1\n",
    "\n",
    "    def decoder(self,z):\n",
    "        h2 = (self.fc2(z))\n",
    "        return h2\n",
    "\n",
    "    def forward(self, x):\n",
    "            h1 = self.encoder(x)\n",
    "            h2 = self.decoder(h1)\n",
    "            return h1, h2\n",
    "\n",
    "mse_loss = nn.MSELoss(size_average = False)\n",
    "\n",
    "def loss_function(W, x, recons_x, h, lam):\n",
    "    \"\"\"Compute the Contractive AutoEncoder Loss\n",
    "    Evalutes the CAE loss, which is composed as the summation of a Mean\n",
    "    Squared Error and the weighted l2-norm of the Jacobian of the hidden\n",
    "    units with respect to the inputs.\n",
    "    See reference below for an in-depth discussion:\n",
    "      #1: http://wiseodd.github.io/techblog/2016/12/05/contractive-autoencoder\n",
    "    Args:\n",
    "        `W` (FloatTensor): (N_hidden x N), where N_hidden and N are the\n",
    "          dimensions of the hidden units and input respectively.\n",
    "        `x` (Variable): the input to the network, with dims (N_batch x N)\n",
    "        recons_x (Variable): the reconstruction of the input, with dims\n",
    "          N_batch x N.\n",
    "        `h` (Variable): the hidden units of the network, with dims\n",
    "          batch_size x N_hidden\n",
    "        `lam` (float): the weight given to the jacobian regulariser term\n",
    "    Returns:\n",
    "        Variable: the (scalar) CAE loss\n",
    "    \"\"\"\n",
    "    mse = mse_loss(recons_x, x)\n",
    "    # Since: W is shape of N_hidden x N. So, we do not need to transpose it as\n",
    "    # opposed to #1\n",
    "    dh = h * (1 - h) # Hadamard product produces size N_batch x N_hidden\n",
    "    # Sum through the input dimension to improve efficiency, as suggested in #1\n",
    "    w_sum = torch.sum(Variable(W)**2, dim=1)\n",
    "    # unsqueeze to avoid issues with torch.mv\n",
    "    w_sum = w_sum.unsqueeze(1) # shape N_hidden x 1\n",
    "    contractive_loss = torch.sum(torch.mm(dh**2, w_sum), 0)\n",
    "    return mse + contractive_loss.mul_(lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "Train epoch: 0 [0/7076(0%)]\t Loss: 17.007141\n",
      "Train epoch: 0 [768/7076(11%)]\t Loss: 11.078425\n",
      "Train epoch: 0 [1536/7076(22%)]\t Loss: 64.115066\n",
      "Train epoch: 0 [2304/7076(32%)]\t Loss: 13.035456\n",
      "Train epoch: 0 [3072/7076(43%)]\t Loss: 9.498035\n",
      "Train epoch: 0 [3840/7076(54%)]\t Loss: 12.457956\n",
      "Train epoch: 0 [4608/7076(65%)]\t Loss: 12.073766\n",
      "Train epoch: 0 [5376/7076(76%)]\t Loss: 9.971311\n",
      "Train epoch: 0 [6144/7076(86%)]\t Loss: 15.331076\n",
      "Train epoch: 0 [6912/7076(97%)]\t Loss: 10.501161\n",
      "====> Epoch: 0 Average loss: 14.7556\n",
      "Train epoch: 1 [0/7076(0%)]\t Loss: 12.750724\n",
      "Train epoch: 1 [768/7076(11%)]\t Loss: 9.905615\n",
      "Train epoch: 1 [1536/7076(22%)]\t Loss: 18.047053\n",
      "Train epoch: 1 [2304/7076(32%)]\t Loss: 11.176498\n",
      "Train epoch: 1 [3072/7076(43%)]\t Loss: 9.846117\n",
      "Train epoch: 1 [3840/7076(54%)]\t Loss: 9.559809\n",
      "Train epoch: 1 [4608/7076(65%)]\t Loss: 10.406567\n",
      "Train epoch: 1 [5376/7076(76%)]\t Loss: 13.613091\n",
      "Train epoch: 1 [6144/7076(86%)]\t Loss: 16.666820\n",
      "Train epoch: 1 [6912/7076(97%)]\t Loss: 9.563265\n",
      "====> Epoch: 1 Average loss: 12.7585\n",
      "Train epoch: 2 [0/7076(0%)]\t Loss: 10.386406\n",
      "Train epoch: 2 [768/7076(11%)]\t Loss: 12.824635\n",
      "Train epoch: 2 [1536/7076(22%)]\t Loss: 10.338071\n",
      "Train epoch: 2 [2304/7076(32%)]\t Loss: 9.657876\n",
      "Train epoch: 2 [3072/7076(43%)]\t Loss: 11.857116\n",
      "Train epoch: 2 [3840/7076(54%)]\t Loss: 10.201555\n",
      "Train epoch: 2 [4608/7076(65%)]\t Loss: 6.241586\n",
      "Train epoch: 2 [5376/7076(76%)]\t Loss: 9.708998\n",
      "Train epoch: 2 [6144/7076(86%)]\t Loss: 7.744983\n",
      "Train epoch: 2 [6912/7076(97%)]\t Loss: 6.597891\n",
      "====> Epoch: 2 Average loss: 10.9453\n",
      "Train epoch: 3 [0/7076(0%)]\t Loss: 7.989620\n",
      "Train epoch: 3 [768/7076(11%)]\t Loss: 18.493586\n",
      "Train epoch: 3 [1536/7076(22%)]\t Loss: 14.064312\n",
      "Train epoch: 3 [2304/7076(32%)]\t Loss: 22.440072\n",
      "Train epoch: 3 [3072/7076(43%)]\t Loss: 8.415078\n",
      "Train epoch: 3 [3840/7076(54%)]\t Loss: 10.160880\n",
      "Train epoch: 3 [4608/7076(65%)]\t Loss: 7.183251\n",
      "Train epoch: 3 [5376/7076(76%)]\t Loss: 10.030324\n",
      "Train epoch: 3 [6144/7076(86%)]\t Loss: 8.134977\n",
      "Train epoch: 3 [6912/7076(97%)]\t Loss: 3.859915\n",
      "====> Epoch: 3 Average loss: 9.3716\n",
      "Train epoch: 4 [0/7076(0%)]\t Loss: 12.157870\n",
      "Train epoch: 4 [768/7076(11%)]\t Loss: 15.904396\n",
      "Train epoch: 4 [1536/7076(22%)]\t Loss: 10.306592\n",
      "Train epoch: 4 [2304/7076(32%)]\t Loss: 10.701052\n",
      "Train epoch: 4 [3072/7076(43%)]\t Loss: 5.685181\n",
      "Train epoch: 4 [3840/7076(54%)]\t Loss: 4.792208\n",
      "Train epoch: 4 [4608/7076(65%)]\t Loss: 4.780118\n",
      "Train epoch: 4 [5376/7076(76%)]\t Loss: 6.402485\n",
      "Train epoch: 4 [6144/7076(86%)]\t Loss: 16.653729\n",
      "Train epoch: 4 [6912/7076(97%)]\t Loss: 5.742558\n",
      "====> Epoch: 4 Average loss: 8.2815\n",
      "Train epoch: 5 [0/7076(0%)]\t Loss: 3.937311\n",
      "Train epoch: 5 [768/7076(11%)]\t Loss: 8.335356\n",
      "Train epoch: 5 [1536/7076(22%)]\t Loss: 5.204763\n",
      "Train epoch: 5 [2304/7076(32%)]\t Loss: 10.969783\n",
      "Train epoch: 5 [3072/7076(43%)]\t Loss: 5.890823\n",
      "Train epoch: 5 [3840/7076(54%)]\t Loss: 6.164756\n",
      "Train epoch: 5 [4608/7076(65%)]\t Loss: 13.998941\n",
      "Train epoch: 5 [5376/7076(76%)]\t Loss: 3.825135\n",
      "Train epoch: 5 [6144/7076(86%)]\t Loss: 6.856347\n",
      "Train epoch: 5 [6912/7076(97%)]\t Loss: 8.782530\n",
      "====> Epoch: 5 Average loss: 7.5497\n",
      "Train epoch: 6 [0/7076(0%)]\t Loss: 4.008332\n",
      "Train epoch: 6 [768/7076(11%)]\t Loss: 7.083229\n",
      "Train epoch: 6 [1536/7076(22%)]\t Loss: 6.005343\n",
      "Train epoch: 6 [2304/7076(32%)]\t Loss: 5.863013\n",
      "Train epoch: 6 [3072/7076(43%)]\t Loss: 4.895065\n",
      "Train epoch: 6 [3840/7076(54%)]\t Loss: 14.132298\n",
      "Train epoch: 6 [4608/7076(65%)]\t Loss: 5.057927\n",
      "Train epoch: 6 [5376/7076(76%)]\t Loss: 6.095999\n",
      "Train epoch: 6 [6144/7076(86%)]\t Loss: 12.544017\n",
      "Train epoch: 6 [6912/7076(97%)]\t Loss: 6.039218\n",
      "====> Epoch: 6 Average loss: 6.9892\n",
      "Train epoch: 7 [0/7076(0%)]\t Loss: 10.672173\n",
      "Train epoch: 7 [768/7076(11%)]\t Loss: 8.091515\n",
      "Train epoch: 7 [1536/7076(22%)]\t Loss: 6.073701\n",
      "Train epoch: 7 [2304/7076(32%)]\t Loss: 4.559141\n",
      "Train epoch: 7 [3072/7076(43%)]\t Loss: 4.267943\n",
      "Train epoch: 7 [3840/7076(54%)]\t Loss: 6.620957\n",
      "Train epoch: 7 [4608/7076(65%)]\t Loss: 4.769948\n",
      "Train epoch: 7 [5376/7076(76%)]\t Loss: 2.648188\n",
      "Train epoch: 7 [6144/7076(86%)]\t Loss: 13.234586\n",
      "Train epoch: 7 [6912/7076(97%)]\t Loss: 7.019514\n",
      "====> Epoch: 7 Average loss: 6.5069\n",
      "Train epoch: 8 [0/7076(0%)]\t Loss: 4.883251\n",
      "Train epoch: 8 [768/7076(11%)]\t Loss: 5.368032\n",
      "Train epoch: 8 [1536/7076(22%)]\t Loss: 10.170667\n",
      "Train epoch: 8 [2304/7076(32%)]\t Loss: 7.422515\n",
      "Train epoch: 8 [3072/7076(43%)]\t Loss: 10.526312\n",
      "Train epoch: 8 [3840/7076(54%)]\t Loss: 4.845775\n",
      "Train epoch: 8 [4608/7076(65%)]\t Loss: 6.074365\n",
      "Train epoch: 8 [5376/7076(76%)]\t Loss: 8.470846\n",
      "Train epoch: 8 [6144/7076(86%)]\t Loss: 3.634189\n",
      "Train epoch: 8 [6912/7076(97%)]\t Loss: 2.122739\n",
      "====> Epoch: 8 Average loss: 6.0755\n",
      "Train epoch: 9 [0/7076(0%)]\t Loss: 9.144198\n",
      "Train epoch: 9 [768/7076(11%)]\t Loss: 5.619343\n",
      "Train epoch: 9 [1536/7076(22%)]\t Loss: 6.012310\n",
      "Train epoch: 9 [2304/7076(32%)]\t Loss: 9.492079\n",
      "Train epoch: 9 [3072/7076(43%)]\t Loss: 3.677336\n",
      "Train epoch: 9 [3840/7076(54%)]\t Loss: 4.190257\n",
      "Train epoch: 9 [4608/7076(65%)]\t Loss: 2.860022\n",
      "Train epoch: 9 [5376/7076(76%)]\t Loss: 3.577111\n",
      "Train epoch: 9 [6144/7076(86%)]\t Loss: 7.364174\n",
      "Train epoch: 9 [6912/7076(97%)]\t Loss: 3.987121\n",
      "====> Epoch: 9 Average loss: 5.6840\n",
      "Train epoch: 10 [0/7076(0%)]\t Loss: 6.072305\n",
      "Train epoch: 10 [768/7076(11%)]\t Loss: 2.442430\n",
      "Train epoch: 10 [1536/7076(22%)]\t Loss: 8.490371\n",
      "Train epoch: 10 [2304/7076(32%)]\t Loss: 2.794835\n",
      "Train epoch: 10 [3072/7076(43%)]\t Loss: 2.334555\n",
      "Train epoch: 10 [3840/7076(54%)]\t Loss: 3.473464\n",
      "Train epoch: 10 [4608/7076(65%)]\t Loss: 4.550513\n",
      "Train epoch: 10 [5376/7076(76%)]\t Loss: 4.728590\n",
      "Train epoch: 10 [6144/7076(86%)]\t Loss: 2.975765\n",
      "Train epoch: 10 [6912/7076(97%)]\t Loss: 3.101785\n",
      "====> Epoch: 10 Average loss: 5.3334\n",
      "Train epoch: 11 [0/7076(0%)]\t Loss: 19.243821\n",
      "Train epoch: 11 [768/7076(11%)]\t Loss: 2.087688\n",
      "Train epoch: 11 [1536/7076(22%)]\t Loss: 5.510101\n",
      "Train epoch: 11 [2304/7076(32%)]\t Loss: 1.928276\n",
      "Train epoch: 11 [3072/7076(43%)]\t Loss: 2.464957\n",
      "Train epoch: 11 [3840/7076(54%)]\t Loss: 2.532444\n",
      "Train epoch: 11 [4608/7076(65%)]\t Loss: 2.519914\n",
      "Train epoch: 11 [5376/7076(76%)]\t Loss: 4.472444\n",
      "Train epoch: 11 [6144/7076(86%)]\t Loss: 3.894110\n",
      "Train epoch: 11 [6912/7076(97%)]\t Loss: 8.719874\n",
      "====> Epoch: 11 Average loss: 5.0188\n",
      "Train epoch: 12 [0/7076(0%)]\t Loss: 7.430741\n",
      "Train epoch: 12 [768/7076(11%)]\t Loss: 10.747007\n",
      "Train epoch: 12 [1536/7076(22%)]\t Loss: 2.431564\n",
      "Train epoch: 12 [2304/7076(32%)]\t Loss: 9.696888\n",
      "Train epoch: 12 [3072/7076(43%)]\t Loss: 12.691475\n",
      "Train epoch: 12 [3840/7076(54%)]\t Loss: 5.558744\n",
      "Train epoch: 12 [4608/7076(65%)]\t Loss: 1.721491\n",
      "Train epoch: 12 [5376/7076(76%)]\t Loss: 4.986638\n",
      "Train epoch: 12 [6144/7076(86%)]\t Loss: 9.138781\n",
      "Train epoch: 12 [6912/7076(97%)]\t Loss: 2.431833\n",
      "====> Epoch: 12 Average loss: 4.7390\n",
      "Train epoch: 13 [0/7076(0%)]\t Loss: 2.709041\n",
      "Train epoch: 13 [768/7076(11%)]\t Loss: 2.885867\n",
      "Train epoch: 13 [1536/7076(22%)]\t Loss: 3.815196\n",
      "Train epoch: 13 [2304/7076(32%)]\t Loss: 1.780321\n",
      "Train epoch: 13 [3072/7076(43%)]\t Loss: 3.081945\n",
      "Train epoch: 13 [3840/7076(54%)]\t Loss: 2.340994\n",
      "Train epoch: 13 [4608/7076(65%)]\t Loss: 3.106763\n",
      "Train epoch: 13 [5376/7076(76%)]\t Loss: 3.583411\n",
      "Train epoch: 13 [6144/7076(86%)]\t Loss: 2.685133\n",
      "Train epoch: 13 [6912/7076(97%)]\t Loss: 2.322784\n",
      "====> Epoch: 13 Average loss: 4.4893\n",
      "Train epoch: 14 [0/7076(0%)]\t Loss: 2.193325\n",
      "Train epoch: 14 [768/7076(11%)]\t Loss: 4.535345\n",
      "Train epoch: 14 [1536/7076(22%)]\t Loss: 2.924752\n",
      "Train epoch: 14 [2304/7076(32%)]\t Loss: 41.316875\n",
      "Train epoch: 14 [3072/7076(43%)]\t Loss: 17.014679\n",
      "Train epoch: 14 [3840/7076(54%)]\t Loss: 3.618081\n",
      "Train epoch: 14 [4608/7076(65%)]\t Loss: 1.808326\n",
      "Train epoch: 14 [5376/7076(76%)]\t Loss: 2.288750\n",
      "Train epoch: 14 [6144/7076(86%)]\t Loss: 2.957430\n",
      "Train epoch: 14 [6912/7076(97%)]\t Loss: 6.645473\n",
      "====> Epoch: 14 Average loss: 4.2621\n",
      "Train epoch: 15 [0/7076(0%)]\t Loss: 2.077584\n",
      "Train epoch: 15 [768/7076(11%)]\t Loss: 5.047341\n",
      "Train epoch: 15 [1536/7076(22%)]\t Loss: 3.295751\n",
      "Train epoch: 15 [2304/7076(32%)]\t Loss: 4.382675\n",
      "Train epoch: 15 [3072/7076(43%)]\t Loss: 4.426357\n",
      "Train epoch: 15 [3840/7076(54%)]\t Loss: 4.004825\n",
      "Train epoch: 15 [4608/7076(65%)]\t Loss: 2.076647\n",
      "Train epoch: 15 [5376/7076(76%)]\t Loss: 2.079793\n",
      "Train epoch: 15 [6144/7076(86%)]\t Loss: 4.660550\n",
      "Train epoch: 15 [6912/7076(97%)]\t Loss: 2.224559\n",
      "====> Epoch: 15 Average loss: 4.0556\n",
      "Train epoch: 16 [0/7076(0%)]\t Loss: 4.141449\n",
      "Train epoch: 16 [768/7076(11%)]\t Loss: 4.537405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 16 [1536/7076(22%)]\t Loss: 2.383086\n",
      "Train epoch: 16 [2304/7076(32%)]\t Loss: 1.598564\n",
      "Train epoch: 16 [3072/7076(43%)]\t Loss: 1.367749\n",
      "Train epoch: 16 [3840/7076(54%)]\t Loss: 1.476131\n",
      "Train epoch: 16 [4608/7076(65%)]\t Loss: 2.640110\n",
      "Train epoch: 16 [5376/7076(76%)]\t Loss: 1.533226\n",
      "Train epoch: 16 [6144/7076(86%)]\t Loss: 3.977267\n",
      "Train epoch: 16 [6912/7076(97%)]\t Loss: 2.059634\n",
      "====> Epoch: 16 Average loss: 3.8687\n",
      "Train epoch: 17 [0/7076(0%)]\t Loss: 3.997765\n",
      "Train epoch: 17 [768/7076(11%)]\t Loss: 3.449029\n",
      "Train epoch: 17 [1536/7076(22%)]\t Loss: 1.164876\n",
      "Train epoch: 17 [2304/7076(32%)]\t Loss: 1.004632\n",
      "Train epoch: 17 [3072/7076(43%)]\t Loss: 2.389946\n",
      "Train epoch: 17 [3840/7076(54%)]\t Loss: 2.070458\n",
      "Train epoch: 17 [4608/7076(65%)]\t Loss: 1.205492\n",
      "Train epoch: 17 [5376/7076(76%)]\t Loss: 2.223591\n",
      "Train epoch: 17 [6144/7076(86%)]\t Loss: 1.294128\n",
      "Train epoch: 17 [6912/7076(97%)]\t Loss: 1.377809\n",
      "====> Epoch: 17 Average loss: 3.7008\n",
      "Train epoch: 18 [0/7076(0%)]\t Loss: 3.665941\n",
      "Train epoch: 18 [768/7076(11%)]\t Loss: 11.101752\n",
      "Train epoch: 18 [1536/7076(22%)]\t Loss: 1.252785\n",
      "Train epoch: 18 [2304/7076(32%)]\t Loss: 3.706452\n",
      "Train epoch: 18 [3072/7076(43%)]\t Loss: 1.460865\n",
      "Train epoch: 18 [3840/7076(54%)]\t Loss: 3.836378\n",
      "Train epoch: 18 [4608/7076(65%)]\t Loss: 1.469393\n",
      "Train epoch: 18 [5376/7076(76%)]\t Loss: 2.649112\n",
      "Train epoch: 18 [6144/7076(86%)]\t Loss: 7.145570\n",
      "Train epoch: 18 [6912/7076(97%)]\t Loss: 2.568131\n",
      "====> Epoch: 18 Average loss: 3.5507\n",
      "Train epoch: 19 [0/7076(0%)]\t Loss: 1.873214\n",
      "Train epoch: 19 [768/7076(11%)]\t Loss: 3.894401\n",
      "Train epoch: 19 [1536/7076(22%)]\t Loss: 4.605958\n",
      "Train epoch: 19 [2304/7076(32%)]\t Loss: 2.424010\n",
      "Train epoch: 19 [3072/7076(43%)]\t Loss: 1.175568\n",
      "Train epoch: 19 [3840/7076(54%)]\t Loss: 3.200199\n",
      "Train epoch: 19 [4608/7076(65%)]\t Loss: 2.838293\n",
      "Train epoch: 19 [5376/7076(76%)]\t Loss: 2.469760\n",
      "Train epoch: 19 [6144/7076(86%)]\t Loss: 5.164417\n",
      "Train epoch: 19 [6912/7076(97%)]\t Loss: 6.025490\n",
      "====> Epoch: 19 Average loss: 3.4182\n",
      "Train epoch: 20 [0/7076(0%)]\t Loss: 2.690806\n",
      "Train epoch: 20 [768/7076(11%)]\t Loss: 2.924843\n",
      "Train epoch: 20 [1536/7076(22%)]\t Loss: 2.793601\n",
      "Train epoch: 20 [2304/7076(32%)]\t Loss: 1.676620\n",
      "Train epoch: 20 [3072/7076(43%)]\t Loss: 2.686960\n",
      "Train epoch: 20 [3840/7076(54%)]\t Loss: 3.376239\n",
      "Train epoch: 20 [4608/7076(65%)]\t Loss: 2.206149\n",
      "Train epoch: 20 [5376/7076(76%)]\t Loss: 1.450310\n",
      "Train epoch: 20 [6144/7076(86%)]\t Loss: 2.082849\n",
      "Train epoch: 20 [6912/7076(97%)]\t Loss: 3.908757\n",
      "====> Epoch: 20 Average loss: 3.3005\n",
      "Train epoch: 21 [0/7076(0%)]\t Loss: 1.470948\n",
      "Train epoch: 21 [768/7076(11%)]\t Loss: 2.562428\n",
      "Train epoch: 21 [1536/7076(22%)]\t Loss: 2.960022\n",
      "Train epoch: 21 [2304/7076(32%)]\t Loss: 1.452389\n",
      "Train epoch: 21 [3072/7076(43%)]\t Loss: 1.568566\n",
      "Train epoch: 21 [3840/7076(54%)]\t Loss: 3.897004\n",
      "Train epoch: 21 [4608/7076(65%)]\t Loss: 3.235939\n",
      "Train epoch: 21 [5376/7076(76%)]\t Loss: 4.255610\n",
      "Train epoch: 21 [6144/7076(86%)]\t Loss: 1.294182\n",
      "Train epoch: 21 [6912/7076(97%)]\t Loss: 2.982500\n",
      "====> Epoch: 21 Average loss: 3.1954\n",
      "Train epoch: 22 [0/7076(0%)]\t Loss: 1.239552\n",
      "Train epoch: 22 [768/7076(11%)]\t Loss: 1.426710\n",
      "Train epoch: 22 [1536/7076(22%)]\t Loss: 1.956509\n",
      "Train epoch: 22 [2304/7076(32%)]\t Loss: 4.463898\n",
      "Train epoch: 22 [3072/7076(43%)]\t Loss: 2.004434\n",
      "Train epoch: 22 [3840/7076(54%)]\t Loss: 3.055640\n",
      "Train epoch: 22 [4608/7076(65%)]\t Loss: 1.464542\n",
      "Train epoch: 22 [5376/7076(76%)]\t Loss: 1.106336\n",
      "Train epoch: 22 [6144/7076(86%)]\t Loss: 1.748508\n",
      "Train epoch: 22 [6912/7076(97%)]\t Loss: 2.993054\n",
      "====> Epoch: 22 Average loss: 3.0985\n",
      "Train epoch: 23 [0/7076(0%)]\t Loss: 9.767920\n",
      "Train epoch: 23 [768/7076(11%)]\t Loss: 1.013978\n",
      "Train epoch: 23 [1536/7076(22%)]\t Loss: 1.679156\n",
      "Train epoch: 23 [2304/7076(32%)]\t Loss: 1.745251\n",
      "Train epoch: 23 [3072/7076(43%)]\t Loss: 4.372451\n",
      "Train epoch: 23 [3840/7076(54%)]\t Loss: 2.006865\n",
      "Train epoch: 23 [4608/7076(65%)]\t Loss: 1.382488\n",
      "Train epoch: 23 [5376/7076(76%)]\t Loss: 1.875692\n",
      "Train epoch: 23 [6144/7076(86%)]\t Loss: 0.895977\n",
      "Train epoch: 23 [6912/7076(97%)]\t Loss: 1.128101\n",
      "====> Epoch: 23 Average loss: 3.0110\n",
      "Train epoch: 24 [0/7076(0%)]\t Loss: 2.811583\n",
      "Train epoch: 24 [768/7076(11%)]\t Loss: 4.365755\n",
      "Train epoch: 24 [1536/7076(22%)]\t Loss: 2.589313\n",
      "Train epoch: 24 [2304/7076(32%)]\t Loss: 1.070027\n",
      "Train epoch: 24 [3072/7076(43%)]\t Loss: 0.738626\n",
      "Train epoch: 24 [3840/7076(54%)]\t Loss: 1.098754\n",
      "Train epoch: 24 [4608/7076(65%)]\t Loss: 2.006595\n",
      "Train epoch: 24 [5376/7076(76%)]\t Loss: 2.180970\n",
      "Train epoch: 24 [6144/7076(86%)]\t Loss: 2.472761\n",
      "Train epoch: 24 [6912/7076(97%)]\t Loss: 1.758706\n",
      "====> Epoch: 24 Average loss: 2.9311\n",
      "Train epoch: 25 [0/7076(0%)]\t Loss: 1.394623\n",
      "Train epoch: 25 [768/7076(11%)]\t Loss: 2.811890\n",
      "Train epoch: 25 [1536/7076(22%)]\t Loss: 2.412174\n",
      "Train epoch: 25 [2304/7076(32%)]\t Loss: 1.910122\n",
      "Train epoch: 25 [3072/7076(43%)]\t Loss: 1.178453\n",
      "Train epoch: 25 [3840/7076(54%)]\t Loss: 1.390863\n",
      "Train epoch: 25 [4608/7076(65%)]\t Loss: 2.151775\n",
      "Train epoch: 25 [5376/7076(76%)]\t Loss: 4.562242\n",
      "Train epoch: 25 [6144/7076(86%)]\t Loss: 11.740996\n",
      "Train epoch: 25 [6912/7076(97%)]\t Loss: 4.709738\n",
      "====> Epoch: 25 Average loss: 2.8528\n",
      "Train epoch: 26 [0/7076(0%)]\t Loss: 2.476398\n",
      "Train epoch: 26 [768/7076(11%)]\t Loss: 1.050459\n",
      "Train epoch: 26 [1536/7076(22%)]\t Loss: 1.239130\n",
      "Train epoch: 26 [2304/7076(32%)]\t Loss: 1.528107\n",
      "Train epoch: 26 [3072/7076(43%)]\t Loss: 1.982170\n",
      "Train epoch: 26 [3840/7076(54%)]\t Loss: 0.866302\n",
      "Train epoch: 26 [4608/7076(65%)]\t Loss: 2.848546\n",
      "Train epoch: 26 [5376/7076(76%)]\t Loss: 1.920171\n",
      "Train epoch: 26 [6144/7076(86%)]\t Loss: 1.322010\n",
      "Train epoch: 26 [6912/7076(97%)]\t Loss: 2.429454\n",
      "====> Epoch: 26 Average loss: 2.7804\n",
      "Train epoch: 27 [0/7076(0%)]\t Loss: 7.615741\n",
      "Train epoch: 27 [768/7076(11%)]\t Loss: 2.082693\n",
      "Train epoch: 27 [1536/7076(22%)]\t Loss: 2.396065\n",
      "Train epoch: 27 [2304/7076(32%)]\t Loss: 1.723288\n",
      "Train epoch: 27 [3072/7076(43%)]\t Loss: 1.156061\n",
      "Train epoch: 27 [3840/7076(54%)]\t Loss: 4.044380\n",
      "Train epoch: 27 [4608/7076(65%)]\t Loss: 6.590323\n",
      "Train epoch: 27 [5376/7076(76%)]\t Loss: 0.862363\n",
      "Train epoch: 27 [6144/7076(86%)]\t Loss: 1.941364\n",
      "Train epoch: 27 [6912/7076(97%)]\t Loss: 1.306847\n",
      "====> Epoch: 27 Average loss: 2.7091\n",
      "Train epoch: 28 [0/7076(0%)]\t Loss: 2.617610\n",
      "Train epoch: 28 [768/7076(11%)]\t Loss: 5.619198\n",
      "Train epoch: 28 [1536/7076(22%)]\t Loss: 2.234750\n",
      "Train epoch: 28 [2304/7076(32%)]\t Loss: 2.453366\n",
      "Train epoch: 28 [3072/7076(43%)]\t Loss: 1.819229\n",
      "Train epoch: 28 [3840/7076(54%)]\t Loss: 0.797373\n",
      "Train epoch: 28 [4608/7076(65%)]\t Loss: 2.019351\n",
      "Train epoch: 28 [5376/7076(76%)]\t Loss: 2.569790\n",
      "Train epoch: 28 [6144/7076(86%)]\t Loss: 1.017843\n",
      "Train epoch: 28 [6912/7076(97%)]\t Loss: 1.393509\n",
      "====> Epoch: 28 Average loss: 2.6400\n",
      "Train epoch: 29 [0/7076(0%)]\t Loss: 1.345784\n",
      "Train epoch: 29 [768/7076(11%)]\t Loss: 2.538920\n",
      "Train epoch: 29 [1536/7076(22%)]\t Loss: 1.212824\n",
      "Train epoch: 29 [2304/7076(32%)]\t Loss: 5.465715\n",
      "Train epoch: 29 [3072/7076(43%)]\t Loss: 1.319355\n",
      "Train epoch: 29 [3840/7076(54%)]\t Loss: 0.965302\n",
      "Train epoch: 29 [4608/7076(65%)]\t Loss: 2.012571\n",
      "Train epoch: 29 [5376/7076(76%)]\t Loss: 1.247445\n",
      "Train epoch: 29 [6144/7076(86%)]\t Loss: 0.816370\n",
      "Train epoch: 29 [6912/7076(97%)]\t Loss: 7.304938\n",
      "====> Epoch: 29 Average loss: 2.5720\n",
      "Train epoch: 30 [0/7076(0%)]\t Loss: 1.126730\n",
      "Train epoch: 30 [768/7076(11%)]\t Loss: 1.622511\n",
      "Train epoch: 30 [1536/7076(22%)]\t Loss: 6.982218\n",
      "Train epoch: 30 [2304/7076(32%)]\t Loss: 0.880715\n",
      "Train epoch: 30 [3072/7076(43%)]\t Loss: 4.574373\n",
      "Train epoch: 30 [3840/7076(54%)]\t Loss: 1.496738\n",
      "Train epoch: 30 [4608/7076(65%)]\t Loss: 6.598822\n",
      "Train epoch: 30 [5376/7076(76%)]\t Loss: 1.674836\n",
      "Train epoch: 30 [6144/7076(86%)]\t Loss: 2.061513\n",
      "Train epoch: 30 [6912/7076(97%)]\t Loss: 2.080054\n",
      "====> Epoch: 30 Average loss: 2.5050\n",
      "Train epoch: 31 [0/7076(0%)]\t Loss: 1.429750\n",
      "Train epoch: 31 [768/7076(11%)]\t Loss: 1.389284\n",
      "Train epoch: 31 [1536/7076(22%)]\t Loss: 2.586129\n",
      "Train epoch: 31 [2304/7076(32%)]\t Loss: 1.225521\n",
      "Train epoch: 31 [3072/7076(43%)]\t Loss: 2.454165\n",
      "Train epoch: 31 [3840/7076(54%)]\t Loss: 2.453655\n",
      "Train epoch: 31 [4608/7076(65%)]\t Loss: 1.637703\n",
      "Train epoch: 31 [5376/7076(76%)]\t Loss: 1.447562\n",
      "Train epoch: 31 [6144/7076(86%)]\t Loss: 7.675129\n",
      "Train epoch: 31 [6912/7076(97%)]\t Loss: 3.233266\n",
      "====> Epoch: 31 Average loss: 2.4375\n",
      "Train epoch: 32 [0/7076(0%)]\t Loss: 2.887693\n",
      "Train epoch: 32 [768/7076(11%)]\t Loss: 1.041996\n",
      "Train epoch: 32 [1536/7076(22%)]\t Loss: 0.991006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 32 [2304/7076(32%)]\t Loss: 1.231655\n",
      "Train epoch: 32 [3072/7076(43%)]\t Loss: 2.480570\n",
      "Train epoch: 32 [3840/7076(54%)]\t Loss: 0.996914\n",
      "Train epoch: 32 [4608/7076(65%)]\t Loss: 1.407847\n",
      "Train epoch: 32 [5376/7076(76%)]\t Loss: 1.650281\n",
      "Train epoch: 32 [6144/7076(86%)]\t Loss: 1.720076\n",
      "Train epoch: 32 [6912/7076(97%)]\t Loss: 1.056722\n",
      "====> Epoch: 32 Average loss: 2.3727\n",
      "Train epoch: 33 [0/7076(0%)]\t Loss: 0.719012\n",
      "Train epoch: 33 [768/7076(11%)]\t Loss: 1.985002\n",
      "Train epoch: 33 [1536/7076(22%)]\t Loss: 1.034068\n",
      "Train epoch: 33 [2304/7076(32%)]\t Loss: 1.167551\n",
      "Train epoch: 33 [3072/7076(43%)]\t Loss: 0.871024\n",
      "Train epoch: 33 [3840/7076(54%)]\t Loss: 1.291612\n",
      "Train epoch: 33 [4608/7076(65%)]\t Loss: 2.232327\n",
      "Train epoch: 33 [5376/7076(76%)]\t Loss: 1.332566\n",
      "Train epoch: 33 [6144/7076(86%)]\t Loss: 0.955890\n",
      "Train epoch: 33 [6912/7076(97%)]\t Loss: 1.790083\n",
      "====> Epoch: 33 Average loss: 2.3111\n",
      "Train epoch: 34 [0/7076(0%)]\t Loss: 1.029960\n",
      "Train epoch: 34 [768/7076(11%)]\t Loss: 1.408693\n",
      "Train epoch: 34 [1536/7076(22%)]\t Loss: 2.088717\n",
      "Train epoch: 34 [2304/7076(32%)]\t Loss: 1.550740\n",
      "Train epoch: 34 [3072/7076(43%)]\t Loss: 4.632167\n",
      "Train epoch: 34 [3840/7076(54%)]\t Loss: 1.321910\n",
      "Train epoch: 34 [4608/7076(65%)]\t Loss: 1.135669\n",
      "Train epoch: 34 [5376/7076(76%)]\t Loss: 1.589069\n",
      "Train epoch: 34 [6144/7076(86%)]\t Loss: 1.155917\n",
      "Train epoch: 34 [6912/7076(97%)]\t Loss: 1.150791\n",
      "====> Epoch: 34 Average loss: 2.2545\n",
      "Train epoch: 35 [0/7076(0%)]\t Loss: 1.331562\n",
      "Train epoch: 35 [768/7076(11%)]\t Loss: 0.923943\n",
      "Train epoch: 35 [1536/7076(22%)]\t Loss: 0.731657\n",
      "Train epoch: 35 [2304/7076(32%)]\t Loss: 5.547623\n",
      "Train epoch: 35 [3072/7076(43%)]\t Loss: 1.893443\n",
      "Train epoch: 35 [3840/7076(54%)]\t Loss: 1.078069\n",
      "Train epoch: 35 [4608/7076(65%)]\t Loss: 0.788682\n",
      "Train epoch: 35 [5376/7076(76%)]\t Loss: 0.998000\n",
      "Train epoch: 35 [6144/7076(86%)]\t Loss: 1.756484\n",
      "Train epoch: 35 [6912/7076(97%)]\t Loss: 2.071031\n",
      "====> Epoch: 35 Average loss: 2.2023\n",
      "Train epoch: 36 [0/7076(0%)]\t Loss: 1.550588\n",
      "Train epoch: 36 [768/7076(11%)]\t Loss: 1.056997\n",
      "Train epoch: 36 [1536/7076(22%)]\t Loss: 0.898775\n",
      "Train epoch: 36 [2304/7076(32%)]\t Loss: 1.324850\n",
      "Train epoch: 36 [3072/7076(43%)]\t Loss: 2.190869\n",
      "Train epoch: 36 [3840/7076(54%)]\t Loss: 1.551078\n",
      "Train epoch: 36 [4608/7076(65%)]\t Loss: 1.900634\n",
      "Train epoch: 36 [5376/7076(76%)]\t Loss: 0.607782\n",
      "Train epoch: 36 [6144/7076(86%)]\t Loss: 0.802375\n",
      "Train epoch: 36 [6912/7076(97%)]\t Loss: 6.646490\n",
      "====> Epoch: 36 Average loss: 2.1557\n",
      "Train epoch: 37 [0/7076(0%)]\t Loss: 1.849606\n",
      "Train epoch: 37 [768/7076(11%)]\t Loss: 0.862278\n",
      "Train epoch: 37 [1536/7076(22%)]\t Loss: 1.244002\n",
      "Train epoch: 37 [2304/7076(32%)]\t Loss: 2.233708\n",
      "Train epoch: 37 [3072/7076(43%)]\t Loss: 1.220486\n",
      "Train epoch: 37 [3840/7076(54%)]\t Loss: 0.966339\n",
      "Train epoch: 37 [4608/7076(65%)]\t Loss: 11.224050\n",
      "Train epoch: 37 [5376/7076(76%)]\t Loss: 1.576959\n",
      "Train epoch: 37 [6144/7076(86%)]\t Loss: 1.462834\n",
      "Train epoch: 37 [6912/7076(97%)]\t Loss: 1.900971\n",
      "====> Epoch: 37 Average loss: 2.1123\n",
      "Train epoch: 38 [0/7076(0%)]\t Loss: 1.025775\n",
      "Train epoch: 38 [768/7076(11%)]\t Loss: 0.894214\n",
      "Train epoch: 38 [1536/7076(22%)]\t Loss: 0.834754\n",
      "Train epoch: 38 [2304/7076(32%)]\t Loss: 1.525895\n",
      "Train epoch: 38 [3072/7076(43%)]\t Loss: 0.526320\n",
      "Train epoch: 38 [3840/7076(54%)]\t Loss: 1.735851\n",
      "Train epoch: 38 [4608/7076(65%)]\t Loss: 1.799487\n",
      "Train epoch: 38 [5376/7076(76%)]\t Loss: 5.415501\n",
      "Train epoch: 38 [6144/7076(86%)]\t Loss: 0.620121\n",
      "Train epoch: 38 [6912/7076(97%)]\t Loss: 0.795672\n",
      "====> Epoch: 38 Average loss: 2.0730\n",
      "Train epoch: 39 [0/7076(0%)]\t Loss: 2.068176\n",
      "Train epoch: 39 [768/7076(11%)]\t Loss: 2.346548\n",
      "Train epoch: 39 [1536/7076(22%)]\t Loss: 0.590649\n",
      "Train epoch: 39 [2304/7076(32%)]\t Loss: 1.396598\n",
      "Train epoch: 39 [3072/7076(43%)]\t Loss: 0.897264\n",
      "Train epoch: 39 [3840/7076(54%)]\t Loss: 0.609502\n",
      "Train epoch: 39 [4608/7076(65%)]\t Loss: 0.650800\n",
      "Train epoch: 39 [5376/7076(76%)]\t Loss: 0.635995\n",
      "Train epoch: 39 [6144/7076(86%)]\t Loss: 1.748977\n",
      "Train epoch: 39 [6912/7076(97%)]\t Loss: 0.574918\n",
      "====> Epoch: 39 Average loss: 2.0356\n",
      "Train epoch: 40 [0/7076(0%)]\t Loss: 0.493303\n",
      "Train epoch: 40 [768/7076(11%)]\t Loss: 1.082901\n",
      "Train epoch: 40 [1536/7076(22%)]\t Loss: 1.476648\n",
      "Train epoch: 40 [2304/7076(32%)]\t Loss: 1.752249\n",
      "Train epoch: 40 [3072/7076(43%)]\t Loss: 2.984781\n",
      "Train epoch: 40 [3840/7076(54%)]\t Loss: 1.088686\n",
      "Train epoch: 40 [4608/7076(65%)]\t Loss: 6.577710\n",
      "Train epoch: 40 [5376/7076(76%)]\t Loss: 0.668252\n",
      "Train epoch: 40 [6144/7076(86%)]\t Loss: 2.288069\n",
      "Train epoch: 40 [6912/7076(97%)]\t Loss: 1.625176\n",
      "====> Epoch: 40 Average loss: 2.0002\n",
      "Train epoch: 41 [0/7076(0%)]\t Loss: 1.350012\n",
      "Train epoch: 41 [768/7076(11%)]\t Loss: 1.823735\n",
      "Train epoch: 41 [1536/7076(22%)]\t Loss: 1.249767\n",
      "Train epoch: 41 [2304/7076(32%)]\t Loss: 1.155771\n",
      "Train epoch: 41 [3072/7076(43%)]\t Loss: 0.447434\n",
      "Train epoch: 41 [3840/7076(54%)]\t Loss: 0.567107\n",
      "Train epoch: 41 [4608/7076(65%)]\t Loss: 1.503548\n",
      "Train epoch: 41 [5376/7076(76%)]\t Loss: 1.492019\n",
      "Train epoch: 41 [6144/7076(86%)]\t Loss: 0.869190\n",
      "Train epoch: 41 [6912/7076(97%)]\t Loss: 1.150325\n",
      "====> Epoch: 41 Average loss: 1.9658\n",
      "Train epoch: 42 [0/7076(0%)]\t Loss: 0.454278\n",
      "Train epoch: 42 [768/7076(11%)]\t Loss: 2.661867\n",
      "Train epoch: 42 [1536/7076(22%)]\t Loss: 2.194417\n",
      "Train epoch: 42 [2304/7076(32%)]\t Loss: 0.612972\n",
      "Train epoch: 42 [3072/7076(43%)]\t Loss: 0.546760\n",
      "Train epoch: 42 [3840/7076(54%)]\t Loss: 0.707307\n",
      "Train epoch: 42 [4608/7076(65%)]\t Loss: 1.422845\n",
      "Train epoch: 42 [5376/7076(76%)]\t Loss: 5.551832\n",
      "Train epoch: 42 [6144/7076(86%)]\t Loss: 0.846025\n",
      "Train epoch: 42 [6912/7076(97%)]\t Loss: 1.874263\n",
      "====> Epoch: 42 Average loss: 1.9337\n",
      "Train epoch: 43 [0/7076(0%)]\t Loss: 1.215578\n",
      "Train epoch: 43 [768/7076(11%)]\t Loss: 1.055395\n",
      "Train epoch: 43 [1536/7076(22%)]\t Loss: 0.995068\n",
      "Train epoch: 43 [2304/7076(32%)]\t Loss: 5.762351\n",
      "Train epoch: 43 [3072/7076(43%)]\t Loss: 0.567129\n",
      "Train epoch: 43 [3840/7076(54%)]\t Loss: 1.902811\n",
      "Train epoch: 43 [4608/7076(65%)]\t Loss: 1.413608\n",
      "Train epoch: 43 [5376/7076(76%)]\t Loss: 1.363272\n",
      "Train epoch: 43 [6144/7076(86%)]\t Loss: 2.843550\n",
      "Train epoch: 43 [6912/7076(97%)]\t Loss: 1.826981\n",
      "====> Epoch: 43 Average loss: 1.9016\n",
      "Train epoch: 44 [0/7076(0%)]\t Loss: 1.846276\n",
      "Train epoch: 44 [768/7076(11%)]\t Loss: 1.030516\n",
      "Train epoch: 44 [1536/7076(22%)]\t Loss: 1.623315\n",
      "Train epoch: 44 [2304/7076(32%)]\t Loss: 0.730610\n",
      "Train epoch: 44 [3072/7076(43%)]\t Loss: 1.138275\n",
      "Train epoch: 44 [3840/7076(54%)]\t Loss: 1.489162\n",
      "Train epoch: 44 [4608/7076(65%)]\t Loss: 1.128873\n",
      "Train epoch: 44 [5376/7076(76%)]\t Loss: 1.305178\n",
      "Train epoch: 44 [6144/7076(86%)]\t Loss: 0.654545\n",
      "Train epoch: 44 [6912/7076(97%)]\t Loss: 1.644520\n",
      "====> Epoch: 44 Average loss: 1.8718\n",
      "Train epoch: 45 [0/7076(0%)]\t Loss: 1.091360\n",
      "Train epoch: 45 [768/7076(11%)]\t Loss: 1.884474\n",
      "Train epoch: 45 [1536/7076(22%)]\t Loss: 1.403380\n",
      "Train epoch: 45 [2304/7076(32%)]\t Loss: 2.193097\n",
      "Train epoch: 45 [3072/7076(43%)]\t Loss: 2.039826\n",
      "Train epoch: 45 [3840/7076(54%)]\t Loss: 2.255989\n",
      "Train epoch: 45 [4608/7076(65%)]\t Loss: 1.732185\n",
      "Train epoch: 45 [5376/7076(76%)]\t Loss: 1.188491\n",
      "Train epoch: 45 [6144/7076(86%)]\t Loss: 1.045750\n",
      "Train epoch: 45 [6912/7076(97%)]\t Loss: 1.555190\n",
      "====> Epoch: 45 Average loss: 1.8405\n",
      "Train epoch: 46 [0/7076(0%)]\t Loss: 0.800103\n",
      "Train epoch: 46 [768/7076(11%)]\t Loss: 1.613025\n",
      "Train epoch: 46 [1536/7076(22%)]\t Loss: 1.281655\n",
      "Train epoch: 46 [2304/7076(32%)]\t Loss: 0.513453\n",
      "Train epoch: 46 [3072/7076(43%)]\t Loss: 0.588119\n",
      "Train epoch: 46 [3840/7076(54%)]\t Loss: 1.074288\n",
      "Train epoch: 46 [4608/7076(65%)]\t Loss: 0.569593\n",
      "Train epoch: 46 [5376/7076(76%)]\t Loss: 0.373900\n",
      "Train epoch: 46 [6144/7076(86%)]\t Loss: 0.657806\n",
      "Train epoch: 46 [6912/7076(97%)]\t Loss: 2.964657\n",
      "====> Epoch: 46 Average loss: 1.8101\n",
      "Train epoch: 47 [0/7076(0%)]\t Loss: 0.935463\n",
      "Train epoch: 47 [768/7076(11%)]\t Loss: 1.486127\n",
      "Train epoch: 47 [1536/7076(22%)]\t Loss: 2.066747\n",
      "Train epoch: 47 [2304/7076(32%)]\t Loss: 0.677480\n",
      "Train epoch: 47 [3072/7076(43%)]\t Loss: 4.527839\n",
      "Train epoch: 47 [3840/7076(54%)]\t Loss: 0.717633\n",
      "Train epoch: 47 [4608/7076(65%)]\t Loss: 0.580989\n",
      "Train epoch: 47 [5376/7076(76%)]\t Loss: 1.198644\n",
      "Train epoch: 47 [6144/7076(86%)]\t Loss: 0.804755\n",
      "Train epoch: 47 [6912/7076(97%)]\t Loss: 1.456762\n",
      "====> Epoch: 47 Average loss: 1.7819\n",
      "Train epoch: 48 [0/7076(0%)]\t Loss: 1.125777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 48 [768/7076(11%)]\t Loss: 1.949248\n",
      "Train epoch: 48 [1536/7076(22%)]\t Loss: 1.198720\n",
      "Train epoch: 48 [2304/7076(32%)]\t Loss: 1.274551\n",
      "Train epoch: 48 [3072/7076(43%)]\t Loss: 2.975221\n",
      "Train epoch: 48 [3840/7076(54%)]\t Loss: 1.248746\n",
      "Train epoch: 48 [4608/7076(65%)]\t Loss: 0.647761\n",
      "Train epoch: 48 [5376/7076(76%)]\t Loss: 1.526706\n",
      "Train epoch: 48 [6144/7076(86%)]\t Loss: 1.439393\n",
      "Train epoch: 48 [6912/7076(97%)]\t Loss: 2.212897\n",
      "====> Epoch: 48 Average loss: 1.7536\n",
      "Train epoch: 49 [0/7076(0%)]\t Loss: 0.905838\n",
      "Train epoch: 49 [768/7076(11%)]\t Loss: 1.543641\n",
      "Train epoch: 49 [1536/7076(22%)]\t Loss: 1.479976\n",
      "Train epoch: 49 [2304/7076(32%)]\t Loss: 1.577976\n",
      "Train epoch: 49 [3072/7076(43%)]\t Loss: 1.019518\n",
      "Train epoch: 49 [3840/7076(54%)]\t Loss: 0.458398\n",
      "Train epoch: 49 [4608/7076(65%)]\t Loss: 0.862520\n",
      "Train epoch: 49 [5376/7076(76%)]\t Loss: 0.541237\n",
      "Train epoch: 49 [6144/7076(86%)]\t Loss: 0.916780\n",
      "Train epoch: 49 [6912/7076(97%)]\t Loss: 0.731394\n",
      "====> Epoch: 49 Average loss: 1.7240\n"
     ]
    }
   ],
   "source": [
    "model = CAE()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "train_dset = TensorDataset(X, Y)\n",
    "train_loader = DataLoader(train_dset, batch_size=64, shuffle=True)\n",
    "print(len(X[0]))\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    \n",
    "\n",
    "    for idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        hidden_representation, recons_x = model(data)\n",
    "\n",
    "        # Get the weights\n",
    "        # model.state_dict().keys()\n",
    "        # change the key by seeing the keys manually.\n",
    "        # (In future I will try to make it automatic)\n",
    "        W = model.state_dict()['fc1.weight']\n",
    "        loss = loss_function(W, data.view(-1, 15), recons_x,\n",
    "                             hidden_representation, lam)\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 12 == 0:\n",
    "            print('Train epoch: {} [{}/{}({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
    "                  epoch, idx*len(data), len(train_loader.dataset),\n",
    "                  100*idx/len(train_loader),\n",
    "                  loss.data[0]/len(data)))\n",
    "\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "         epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "    \n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4247, -1.5674, -0.3062, -0.3391, -0.1670, -0.2742, -0.3558, -0.4824,\n",
      "          0.2893, -0.0022, -0.2651,  1.1561, -0.2323, -0.1833, -0.1973]])\n",
      "tensor([[-0.3345, -1.4632, -0.3104, -0.2819, -0.0866, -0.1944, -0.1665, -0.3003,\n",
      "          0.0552,  0.2045,  0.0570,  1.0599, -0.3873, -0.3635, -0.3822]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[0.3667, 0.2293, 0.4659, 0.4175, 0.7066, 0.8994, 0.5776, 0.4499, 0.4000,\n",
      "         0.8964, 0.6821, 0.3903]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = X[:1]\n",
    "x_hidden, x_recons = model(x)\n",
    "print(x)\n",
    "print(x_recons)\n",
    "print(x_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(X):\n",
    "    return model(X)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = torch.autograd.functional.jacobian(func, X[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 15])\n"
     ]
    }
   ],
   "source": [
    "print(J.size())\n",
    "# print(torch.transpose(J, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V = torch.svd(J.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 12])\n",
      "torch.Size([12])\n",
      "torch.Size([12, 12])\n",
      "tensor([0.3768, 0.3099, 0.2881, 0.2663, 0.2573, 0.2476, 0.2388, 0.2152, 0.2019,\n",
      "        0.1639, 0.0953, 0.0841])\n"
     ]
    }
   ],
   "source": [
    "print(U.size())\n",
    "print(S.size())\n",
    "print(V.size())\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False, False])\n"
     ]
    }
   ],
   "source": [
    "eps = 0.1\n",
    "print(S>eps)\n",
    "Bx = U[:, S>eps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 12])\n",
      "torch.Size([15, 10])\n"
     ]
    }
   ],
   "source": [
    "print(U.size())\n",
    "print(Bx.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([1.3333, 1.6667, 1.3333]), tensor([0.3333]))\n"
     ]
    }
   ],
   "source": [
    "class DELTA():\n",
    "    def __init__(self, x_dim):\n",
    "        \n",
    "        self.x = cp.Variable(x_dim)\n",
    "        self.v = cp.Variable(1)\n",
    "        self.r = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.w = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.B = cp.Parameter((x_dim, 1), value = np.random.randn(x_dim, 1))\n",
    "\n",
    "        target = self.x@self.w - cp.sum_squares(self.x - self.r)\n",
    "        constraints = [self.x >= X_LOWER_BOUND,\n",
    "                       self.x <= X_UPPER_BOUND, self.B@self.v == self.x-self.r]\n",
    "        objective = cp.Maximize(target)\n",
    "        problem = cp.Problem(objective, constraints)\n",
    "        self.layer = CvxpyLayer(problem, parameters=[self.r, self.w, self.B],\n",
    "                                variables=[self.x, self.v])\n",
    "        \n",
    "    def optimize_x(self, x, w, B):\n",
    "        return self.layer(x, w, B)\n",
    "\n",
    "x_dim = 3\n",
    "delta = DELTA(x_dim)\n",
    "\n",
    "B = torch.Tensor([[1],[2],[1]])\n",
    "w = torch.ones(x_dim)\n",
    "x = torch.ones(x_dim)\n",
    "x_opt = delta.optimize_x(x, w, B)\n",
    "print(x_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "funcPred",
   "language": "python",
   "name": "funcpred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
