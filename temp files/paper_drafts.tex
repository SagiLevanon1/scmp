% Generated by GrindEQ Word-to-LaTeX 
\documentclass{article} % use \documentstyle for old LaTeX compilers

\usepackage[english]{babel} % 'french', 'german', 'spanish', 'danish', etc.
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{txfonts}
\usepackage{mathdots}
\usepackage[classicReIm]{kpfonts}
\usepackage{graphicx}

% You can include more LaTeX packages here 


\begin{document}

%\selectlanguage{english} % remove comment delimiter ('%') and select language if required


\noindent We consider a binary strategic classification task. 

\noindent let $X\subseteq {\mathbb{R}}^d$ be a population and let D be a distribution over X.

\noindent let $h:X\to \{-1,\ 1\}$ be a target classifier and let $c:X\times X\to {\mathbb{R}}^+$  be a non-negative cost function.

\noindent In this setting, there is a judge who wishes to publish a classifier $f:X\to \{-1,\ 1\}$ which maximizes his prediction accuracy over a strategically modified dataset. Formally, the judge wishes to publish a classifier f which maximizes $P_{x\sim D}\left[f\left(\mathrm{\Delta }x\right)=h\left(x\right)\right]$, where $\mathrm{\Delta }x=argmax_{x^'}\left.f\left(x^'\right)-c\left(x^',\ x\right)\right.$.

\noindent We focus on the case where f is linear (i.e., of the form $f\left(x\right)=sign(w^Tx+b)$ for some $w\in {\mathbb{R}}^d,\ b\in \mathbb{R}$)

\noindent given a set of labeled examples  $\{\left(x_1,\ y_1\right)\dots \left(x_n,\ y_n\right)\}$, drawn from the distribution D we denote the judge's loss as follows:
\[loss=\sum^n_{i=1}{I\{f\left(\mathrm{\Delta }x\right)\ \neq h\left(x\right)\}}\] 
To minimize this loss, we use the help of a differentiable optimization layer. This layer can solve convex optimization problems while still allowing backpropagation gradients to flow through it. We use it to solve the argmax problem of $\mathrm{\Delta }x$.\textit{ }However, the sign function is not convex.

\noindent For this reason, we use a sigmoid-like function that smoothens the sign function. Then, for each sample x, we approximate a convex function which has the same solution to the argmax problem as the non-convex sigmoid-like function.

\noindent To smooth the sign function, we use the function:
\[sign^*\left(x\right)=\frac{1}{2}\left(\sqrt{{\left(tx+1\right)}^2+1}-\sqrt{{\left(tx-1\right)}^2+1}\right)\ for\ some\ positive\ constant\ t.\] 
To find the approximated function, we use the convex-concave procedure (ccp).

\noindent Then, we use gradient decent techniques to minimize the loss. The loss of batch of samples B is calculated as follows:
\[foreach\ x\ in\ B:\] 
\[1.\ aproximate\ a\ convex\ function\ g\ s.t\ \mathrm{\Delta }x=\ argmax_{x^'}\left.g\left(x^'\right)-c\left(x^',\ x\right)\right.2.\ find\ \mathrm{\Delta }x\ using\ the\ differentiable\ optimization\ layer\ 3.\ calculate\ l=hinge_{loss}(sign^*\left(w^T\mathrm{\Delta }x+b\right),\ h\left(x\right))\] 
\[Loss=\frac{1}{\left|B\right|}\sum_{x\in B}{l_x}\] 
\textit{}

\noindent 

\noindent 

\noindent 


\end{document}

