{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import cvxpy as cp\n",
    "import dccp\n",
    "import torch\n",
    "import numpy as np\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import zero_one_loss, confusion_matrix\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.patches as mpatches\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import os, psutil\n",
    "from datetime import datetime\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "TRAIN_SLOPE = 2\n",
    "EVAL_SLOPE = 5\n",
    "X_LOWER_BOUND = -10\n",
    "X_UPPER_BOUND = 10\n",
    "SEED = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, Y, percentage):\n",
    "    num_val = int(len(X)*percentage)\n",
    "    return X[num_val:], Y[num_val:], X[:num_val], Y[:num_val]\n",
    "\n",
    "def shuffle(X, Y):\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    data = torch.cat((Y, X), 1)\n",
    "    data = data[torch.randperm(data.size()[0])]\n",
    "    X = data[:, 1:]\n",
    "    Y = data[:, 0]\n",
    "    return X, Y\n",
    "\n",
    "def conf_mat(Y1, Y2):\n",
    "    num_of_samples = len(Y1)\n",
    "    mat = confusion_matrix(Y1, Y2, labels=[-1, 1])*100/num_of_samples\n",
    "    acc = np.trace(mat)\n",
    "    return mat, acc\n",
    "\n",
    "def calc_accuracy(Y, Ypred):\n",
    "    num = len(Y)\n",
    "    temp = Y - Ypred\n",
    "    acc = len(temp[temp == 0])*1./num\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCP classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCP:\n",
    "    def __init__(self, x_dim, batch_size, funcs, scale):\n",
    "        self.f_derivative = funcs[\"f_derivative\"]\n",
    "        self.g = funcs[\"g\"]\n",
    "        self.c = funcs[\"c\"]\n",
    "        \n",
    "        self.x = cp.Variable((batch_size, x_dim))\n",
    "        self.xt = cp.Parameter((batch_size, x_dim))\n",
    "        self.r = cp.Parameter((batch_size, x_dim))\n",
    "        self.w = cp.Parameter(x_dim)\n",
    "        self.b = cp.Parameter(1)\n",
    "        self.slope = cp.Parameter(1)\n",
    "\n",
    "        target = cp.diag(self.x@(self.f_derivative(self.xt, self.w, self.b, self.slope).T))-self.g(self.x, self.w, self.b, self.slope)-self.c(self.x, self.r, x_dim, scale)\n",
    "        constraints = [self.x >= X_LOWER_BOUND,\n",
    "                       self.x <= X_UPPER_BOUND]\n",
    "        self.prob = cp.Problem(cp.Maximize(cp.sum(target)), constraints)\n",
    "        \n",
    "    def ccp(self, r):\n",
    "        \"\"\"\n",
    "        numpy to numpy\n",
    "        \"\"\"\n",
    "        self.xt.value = r\n",
    "        self.r.value = r\n",
    "        result = self.prob.solve()\n",
    "        diff = np.linalg.norm(self.xt.value - self.x.value)\n",
    "        cnt = 0\n",
    "        while diff > 0.001 and cnt < 100:\n",
    "            cnt += 1\n",
    "            self.xt.value = self.x.value\n",
    "            result = self.prob.solve()\n",
    "            diff = np.linalg.norm(self.x.value - self.xt.value)\n",
    "        return self.x.value\n",
    "    \n",
    "    def optimize_X(self, X, w, b, slope):\n",
    "        \"\"\"\n",
    "        tensor to tensor\n",
    "        \"\"\"\n",
    "        w = w.detach().numpy()\n",
    "        b = b.detach().numpy()\n",
    "        slope = np.full(1, slope)\n",
    "        X = X.numpy()\n",
    "        \n",
    "        self.w.value = w\n",
    "        self.b.value = b\n",
    "        self.slope.value = slope\n",
    "        return torch.from_numpy(self.ccp(X))\n",
    "        # return torch.stack([torch.from_numpy(self.ccp(x)) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DELTA():\n",
    "    \n",
    "    def __init__(self, x_dim, funcs, scale):\n",
    "        self.g = funcs[\"g\"]\n",
    "        self.c = funcs[\"c\"]\n",
    "        \n",
    "        self.x = cp.Variable(x_dim)\n",
    "        self.r = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.w = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.b = cp.Parameter(1, value = np.random.randn(1))\n",
    "        self.f_der = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "\n",
    "        target = self.x@self.f_der-self.g(self.x, self.w, self.b, TRAIN_SLOPE)-self.c(self.x, self.r, x_dim, scale)\n",
    "        constraints = [self.x >= X_LOWER_BOUND,\n",
    "                       self.x <= X_UPPER_BOUND]\n",
    "        objective = cp.Maximize(target)\n",
    "        problem = cp.Problem(objective, constraints)\n",
    "        self.layer = CvxpyLayer(problem, parameters=[self.r, self.w, self.b, self.f_der],\n",
    "                                variables=[self.x])\n",
    "        \n",
    "    def optimize_X(self, X, w, b, F_DER):\n",
    "        return self.layer(X, w, b, F_DER)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gain & Cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(x, w, b):\n",
    "    return x@w + b\n",
    "\n",
    "def f(x, w, b, slope):\n",
    "    return 0.5*cp.norm(cp.hstack([1, (slope*score(x, w, b) + 1)]), 2)\n",
    "\n",
    "def g(x, w, b, slope):\n",
    "    return 0.5*cp.norm(cp.hstack([1, (slope*score(x, w, b) - 1)]), 2)\n",
    "\n",
    "def c(x, r, x_dim, scale):\n",
    "    return (scale)*cp.sum_squares(x-r)\n",
    "\n",
    "def f_derivative(x, w, b, slope):\n",
    "    return 0.5*cp.multiply(slope*((slope*score(x, w, b) + 1)/cp.sqrt((slope*score(x, w, b) + 1)**2 + 1)), w)\n",
    "    \n",
    "def f_batch(x, w, b, slope):\n",
    "    return 0.5*cp.norm(cp.vstack([np.ones(x.shape[0]), (slope*score(x, w, b) + 1)]), 2, axis=0)\n",
    "\n",
    "def g_batch(x, w, b, slope):\n",
    "    return 0.5*cp.norm(cp.vstack([np.ones((1, x.shape[0])), cp.reshape((slope*score(x, w, b) - 1), (1, x.shape[0]))]), 2, axis=0)\n",
    "\n",
    "def c_batch(x, r, x_dim, scale):\n",
    "    return (scale)*cp.square(cp.norm(x-r, 2, axis=1))\n",
    "\n",
    "def f_derivative_batch(x, w, b, slope):\n",
    "    nablas = 0.5*slope*((slope*score(x, w, b) + 1)/cp.sqrt((slope*score(x, w, b) + 1)**2 + 1))\n",
    "    return cp.reshape(nablas, (nablas.shape[0], 1))@cp.reshape(w, (1, x.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyStrategicModel(torch.nn.Module):\n",
    "    def __init__(self, x_dim, batch_size, funcs, funcs_batch, train_slope, eval_slope, scale, strategic=False):\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "\n",
    "        super(MyStrategicModel, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.train_slope, self.eval_slope = train_slope, eval_slope\n",
    "        self.w = torch.nn.parameter.Parameter(math.sqrt(1/x_dim)*(1-2*torch.rand(x_dim, dtype=torch.float64, requires_grad=True)))\n",
    "        self.b = torch.nn.parameter.Parameter(math.sqrt(1/x_dim)*(1-2*torch.rand(1, dtype=torch.float64, requires_grad=True)))\n",
    "#         self.w = torch.nn.parameter.Parameter(torch.zeros(x_dim, dtype=torch.float64, requires_grad=True))\n",
    "#         self.b = torch.nn.parameter.Parameter(torch.zeros(1, dtype=torch.float64, requires_grad=True))\n",
    "        self.strategic = strategic\n",
    "        self.ccp = CCP(x_dim, batch_size, funcs_batch, scale)\n",
    "        self.delta = DELTA(x_dim, funcs, scale)\n",
    "        self.ccp_time = 0\n",
    "        self.total_time = 0\n",
    "\n",
    "    def forward(self, X, evaluation=False):\n",
    "        if self.strategic:\n",
    "            if evaluation:\n",
    "                t1 = time.time()\n",
    "                XT = self.ccp.optimize_X(X, self.w, self.b, self.eval_slope)\n",
    "                self.ccp_time += time.time()-t1\n",
    "                X_opt = XT\n",
    "            else:\n",
    "                t1 = time.time()\n",
    "                XT = self.ccp.optimize_X(X, self.w, self.b, self.train_slope)\n",
    "                self.ccp_time += time.time()-t1\n",
    "                F_DER = self.get_f_ders(XT, self.train_slope)\n",
    "                X_opt = self.delta.optimize_X(X, self.w, self.b, F_DER) # Xopt should be equal to XT but we do it again for the gradients\n",
    "            output = self.score(X_opt)\n",
    "        else:\n",
    "            output = self.score(X)        \n",
    "        return output\n",
    "    \n",
    "    def optimize_X(self, X, evaluation=False):\n",
    "        slope = self.eval_slope if evaluation else self.train_slope\n",
    "        return self.ccp.optimize_X(X, self.w, self.b, slope)\n",
    "    \n",
    "    def normalize_weights(self):\n",
    "        with torch.no_grad():\n",
    "            norm = torch.sqrt(torch.sum(self.w**2) + self.b**2)\n",
    "            self.w /= norm\n",
    "            self.b /= norm\n",
    "\n",
    "    def score(self, x):\n",
    "        return x@self.w + self.b\n",
    "    \n",
    "    def get_f_ders(self, XT, slope):\n",
    "        # return torch.stack([0.5*slope*((slope*self.score(xt) + 1)/torch.sqrt((slope*self.score(xt) + 1)**2 + 1))*self.w for xt in XT])\n",
    "        nablas = 0.5*slope*((slope*self.score(XT) + 1)/torch.sqrt((slope*self.score(XT) + 1)**2 + 1))\n",
    "        return torch.reshape(nablas, (len(nablas), 1))@torch.reshape(self.w, (1, len(self.w)))\n",
    "\n",
    "    def calc_accuracy(self, Y, Y_pred):\n",
    "        Y_pred = torch.sign(Y_pred)\n",
    "        num = len(Y)\n",
    "        temp = Y - Y_pred\n",
    "        acc = len(temp[temp == 0])*1./num        \n",
    "        return acc\n",
    "    \n",
    "    def evaluate(self, X, Y):      \n",
    "        return self.calc_accuracy(Y, self.forward(X, evaluation=True))\n",
    "    \n",
    "    def loss(self, Y, Y_pred):\n",
    "        return torch.mean(torch.clamp(1 - Y_pred * Y, min=0))\n",
    "    \n",
    "    def save_model(self, train_errors, val_errors, train_losses, val_losses, info, path, comment=None):\n",
    "        if comment is not None:\n",
    "            path += \"/\" + comment\n",
    "            \n",
    "        filename = path + \"/model.pt\"\n",
    "        if not os.path.exists(os.path.dirname(filename)):\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        torch.save(self.state_dict(), filename)\n",
    "                \n",
    "        pd.DataFrame(np.array(train_errors)).to_csv(path + '/train_errors.csv')\n",
    "        pd.DataFrame(np.array(val_errors)).to_csv(path + '/val_errors.csv')\n",
    "        pd.DataFrame(np.array(train_losses)).to_csv(path + '/train_losses.csv')\n",
    "        pd.DataFrame(np.array(val_losses)).to_csv(path + '/val_losses.csv')\n",
    "        \n",
    "        with open(path + \"/info.txt\", \"w\") as f:\n",
    "            f.write(info)\n",
    "    \n",
    "    def load_model(self, filename):\n",
    "        self.load_state_dict(torch.load(filename))\n",
    "        self.eval()\n",
    "    \n",
    "    def fit(self, path, X, Y, Xval, Yval, opt, opt_kwargs={\"lr\":1e-3}, batch_size=128, epochs=100, verbose=False, callback=None, comment=None):\n",
    "        train_dset = TensorDataset(X, Y)\n",
    "        train_loader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "        test_dset = TensorDataset(Xval, Yval)\n",
    "        test_loader = DataLoader(test_dset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        opt = opt(self.parameters(), **opt_kwargs)\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_errors = []\n",
    "        val_errors = []\n",
    "        \n",
    "        best_val_error = 1\n",
    "        consecutive_no_improvement = 0\n",
    "\n",
    "        total_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            t1 = time.time()\n",
    "            batch = 1\n",
    "            train_losses.append([])\n",
    "            train_errors.append([])\n",
    "            for Xbatch, Ybatch in train_loader:\n",
    "#                 try:\n",
    "                opt.zero_grad()\n",
    "                Ybatch_pred = self.forward(Xbatch)\n",
    "                l = self.loss(Ybatch, Ybatch_pred)\n",
    "                l.backward()\n",
    "                opt.step()\n",
    "                train_losses[-1].append(l.item())\n",
    "                with torch.no_grad():\n",
    "                    e = self.calc_accuracy(Ybatch, Ybatch_pred)\n",
    "                    train_errors[-1].append(1-e)\n",
    "                if verbose:\n",
    "                    print(\"batch %03d / %03d | loss: %3.5f | err: %3.5f\" %\n",
    "                          (batch, len(train_loader), np.mean(train_losses[-1]), np.mean(train_errors[-1])))\n",
    "                batch += 1\n",
    "                if callback is not None:\n",
    "                    callback()\n",
    "#                 except:\n",
    "#                     print(\"failed\")\n",
    "                if batch == 11:\n",
    "                    print(\"breaked\")\n",
    "                    break\n",
    "            break\n",
    "            with torch.no_grad():\n",
    "                total_loss = 0\n",
    "                total_error = 0\n",
    "                batch = 0\n",
    "                for Xbatch, Ybatch in test_loader:\n",
    "#                     try:\n",
    "                    Yval_pred = self.forward(Xbatch, evaluation=True)\n",
    "                    val_loss = self.loss(Ybatch, Yval_pred).item()\n",
    "                    total_loss += val_loss\n",
    "                    val_error = 1-self.calc_accuracy(Ybatch, Yval_pred)\n",
    "                    total_error += val_error\n",
    "                    batch += 1\n",
    "#                     except:\n",
    "#                         print(\"failed\")\n",
    "                        \n",
    "                avg_loss = total_loss/batch\n",
    "                avg_error = total_error/batch\n",
    "                val_losses.append(avg_loss)\n",
    "                val_errors.append(avg_error)\n",
    "                if avg_error < best_val_error:\n",
    "                        consecutive_no_improvement = 0\n",
    "                        best_val_error = avg_error\n",
    "                        info = \"training time in seconds: {}\\nepoch: {}\\nbatch size: {}\\ntrain slope: {}\\neval slope: {}\\nlearning rate: {}\\nvalidation loss: {}\\nvalidation error: {}\\n\".format(\n",
    "                        time.time()-total_time, epoch, batch_size, self.train_slope, self.eval_slope, opt_kwargs[\"lr\"], avg_loss, avg_error)\n",
    "                        self.save_model(train_errors, val_errors, train_losses, val_losses, info, path, comment)\n",
    "                        print(\"model saved!\")\n",
    "\n",
    "                else:\n",
    "                    consecutive_no_improvement += 1\n",
    "                    if consecutive_no_improvement >= 4:\n",
    "                        break\n",
    "                \n",
    "            t2 = time.time()\n",
    "            if verbose:\n",
    "                print(\"------------- epoch %03d / %03d | time: %03d sec | loss: %3.5f | err: %3.5f\" % (epoch + 1, epochs, t2-t1, val_losses[-1], val_errors[-1]))\n",
    "        \n",
    "        self.total_time = time.time()-total_time\n",
    "        print(\"training time: {} seconds\".format(self.total_time)) \n",
    "        return train_errors, val_errors, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sklearn_data(x_dim, N, informative_frac=1, shift_range=1, scale_range=1, noise_frac=0.01):\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    n_informative = int(informative_frac*x_dim)\n",
    "    n_redundant = x_dim - n_informative\n",
    "    shift_arr = shift_range*np.random.randn(x_dim)\n",
    "    scale_arr = scale_range*np.random.randn(x_dim)\n",
    "    X, Y = make_classification(n_samples=N, n_features=x_dim, n_informative=n_informative, n_redundant=n_redundant,\n",
    "                               flip_y=noise_frac, shift=shift_arr, scale=scale_arr, random_state=0)\n",
    "    Y[Y == 0] = -1\n",
    "    X -= np.mean(X, axis=0)\n",
    "    X /= np.std(X, axis=0)\n",
    "    return torch.from_numpy(X), torch.from_numpy(Y)\n",
    "\n",
    "def load_spam_data():\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    path = r\"C:\\Users\\sagil\\Desktop\\nir_project\\tip_spam_data\\IS_journal_tip_spam.arff\"\n",
    "    data, meta = arff.loadarff(path)\n",
    "    df = pd.DataFrame(data)\n",
    "    most_disc = ['qTips_plc', 'rating_plc', 'qEmail_tip', 'qContacts_tip', 'qURL_tip', 'qPhone_tip', 'qNumeriChar_tip', 'sentistrength_tip', 'combined_tip', 'qWords_tip', 'followers_followees_gph', 'qunigram_avg_tip', 'qTips_usr', 'indeg_gph', 'qCapitalChar_tip', 'class1']\n",
    "    df = df[most_disc]\n",
    "    df[\"class1\"].replace({b'spam': -1, b'notspam': 1}, inplace=True)\n",
    "    df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "    Y = df['class1'].values\n",
    "    X = df.drop('class1', axis = 1).values\n",
    "    x_dim = len(X[0])\n",
    "    X -= np.mean(X, axis=0)\n",
    "    X /= np.std(X, axis=0)\n",
    "    X /= math.sqrt(x_dim)\n",
    "    return torch.from_numpy(X), torch.from_numpy(Y)\n",
    "\n",
    "def load_card_fraud_data():\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    df = pd.read_csv('C:/Users/sagil/Desktop/nir_project/card_fraud_dataset/creditcard.csv')\n",
    "\n",
    "    rob_scaler = RobustScaler()\n",
    "\n",
    "    df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "    df.drop(['Time','Amount'], axis=1, inplace=True)\n",
    "    scaled_amount = df['scaled_amount']\n",
    "    df.drop(['scaled_amount'], axis=1, inplace=True)\n",
    "    df.insert(0, 'scaled_amount', scaled_amount)\n",
    "\n",
    "    df[\"Class\"].replace({1: -1, 0: 1}, inplace=True)\n",
    "    df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "    # amount of fraud classes 492 rows.\n",
    "    fraud_df = df.loc[df['Class'] == -1]\n",
    "    non_fraud_df = df.loc[df['Class'] == 1][:492]\n",
    "\n",
    "    normal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n",
    "\n",
    "    # Shuffle dataframe rows\n",
    "    df = normal_distributed_df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "    Y = df['Class'].values\n",
    "    X = df.drop('Class', axis = 1).values\n",
    "    x_dim = len(X[0])\n",
    "    X -= np.mean(X, axis=0)\n",
    "    X /= np.std(X, axis=0)\n",
    "    X /= math.sqrt(x_dim)\n",
    "    return torch.from_numpy(X), torch.from_numpy(Y)\n",
    "\n",
    "def load_credit_default_data():\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    url = 'https://raw.githubusercontent.com/ustunb/actionable-recourse/master/examples/paper/data/credit_processed.csv'\n",
    "    df = pd.read_csv(url)\n",
    "    df[\"NoDefaultNextMonth\"].replace({0: -1}, inplace=True)\n",
    "    df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "    df = df.drop(['Married', 'Single', 'Age_lt_25', 'Age_in_25_to_40', 'Age_in_40_to_59', 'Age_geq_60'], axis = 1)\n",
    "\n",
    "    fraud_df = df.loc[df[\"NoDefaultNextMonth\"] == -1]\n",
    "    non_fraud_df = df.loc[df[\"NoDefaultNextMonth\"] == 1][:6636]\n",
    "\n",
    "    normal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n",
    "\n",
    "    # Shuffle dataframe rows\n",
    "    df = normal_distributed_df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df.loc[:, df.columns != \"NoDefaultNextMonth\"] = scaler.fit_transform(df.drop(\"NoDefaultNextMonth\", axis=1)) \n",
    "    Y, X = df.iloc[:, 0].values, df.iloc[:, 1:].values\n",
    "    x_dim = len(X[0])\n",
    "    X -= np.mean(X, axis=0)\n",
    "    X /= np.std(X, axis=0)\n",
    "    X /= math.sqrt(x_dim)\n",
    "    return torch.from_numpy(X), torch.from_numpy(Y)\n",
    "\n",
    "def load_financial_distress_data():\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    data = pd.read_csv(\"C:/Users/sagil/Desktop/nir_project/financial_distress_data/Financial Distress.csv\")\n",
    "\n",
    "    data = data[data.columns.drop(list(data.filter(regex='x80')))] # Since it is a categorical feature with 37 features.\n",
    "    x_dim = len(data.columns) - 3\n",
    "    data.drop(['Time'], axis=1, inplace=True)\n",
    "\n",
    "    data_grouped = data.groupby(['Company']).last()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    data_grouped.loc[:, data_grouped.columns != \"Financial Distress\"] = scaler.fit_transform(data_grouped.drop(\"Financial Distress\", axis=1))\n",
    "\n",
    "    # Shuffle dataframe rows\n",
    "    data_grouped = data_grouped.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "    Y, X = data_grouped.iloc[:, 0].values, data_grouped.iloc[:, 1:].values\n",
    "    for y in range(0,len(Y)): # Coverting target variable from continuous to binary form\n",
    "        if Y[y] < -0.5:\n",
    "              Y[y] = -1\n",
    "        else:\n",
    "              Y[y] = 1\n",
    "    x_dim = len(X[0])\n",
    "    X -= np.mean(X, axis=0)\n",
    "    X /= np.std(X, axis=0)\n",
    "    X /= math.sqrt(x_dim)\n",
    "    return torch.from_numpy(X), torch.from_numpy(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_datas = []\n",
    "\n",
    "# distress \n",
    "X, Y = load_financial_distress_data()\n",
    "X, Y, Xval, Yval = split_data(X, Y, 0.25)\n",
    "n = len(X)\n",
    "effective_n = n - n%24\n",
    "X, Y = X[:effective_n], Y[:effective_n]\n",
    "n = len(Xval)\n",
    "effective_n = n - n%24\n",
    "Xval, Yval = Xval[:effective_n], Yval[:effective_n]\n",
    "training_datas.append({\"X\": X,\n",
    "                        \"Y\": Y,\n",
    "                        \"Xval\": Xval,\n",
    "                        \"Yval\": Yval,\n",
    "                        \"epochs\": 7,\n",
    "                        \"batch_size\": 24,\n",
    "                        \"name\": \"distress\"})\n",
    "\n",
    "# fraud dataset\n",
    "X, Y = load_card_fraud_data()\n",
    "X, Y, Xval, Yval = split_data(X, Y, 0.25)\n",
    "n = len(X)\n",
    "effective_n = n - n%24\n",
    "X, Y = X[:effective_n], Y[:effective_n]\n",
    "n = len(Xval)\n",
    "effective_n = n - n%24\n",
    "Xval, Yval = Xval[:effective_n], Yval[:effective_n]\n",
    "training_datas.append({\"X\": X,\n",
    "                        \"Y\": Y,\n",
    "                        \"Xval\": Xval,\n",
    "                        \"Yval\": Yval,\n",
    "                        \"epochs\": 7,\n",
    "                        \"batch_size\": 24, \n",
    "                        \"name\": \"fraud\"})\n",
    "\n",
    "\n",
    "# credit data\n",
    "X, Y = load_credit_default_data()\n",
    "X, Y = X[:3000], Y[:3000]\n",
    "X, Y, Xval, Yval = split_data(X, Y, 0.25)\n",
    "n = len(X)\n",
    "effective_n = n - n%24\n",
    "X, Y = X[:effective_n], Y[:effective_n]\n",
    "n = len(Xval)\n",
    "effective_n = n - n%24\n",
    "Xval, Yval = Xval[:effective_n], Yval[:effective_n]\n",
    "training_datas.append({\"X\": X,\n",
    "                        \"Y\": Y,\n",
    "                        \"Xval\": Xval,\n",
    "                        \"Yval\": Yval,\n",
    "                        \"epochs\": 7,\n",
    "                        \"batch_size\": 64, \n",
    "                        \"name\": \"credit\"})\n",
    "\n",
    "# spam dataset\n",
    "X, Y = load_spam_data()\n",
    "X, Y, Xval, Yval = split_data(X, Y, 0.25)\n",
    "n = len(X)\n",
    "effective_n = n - n%24\n",
    "X, Y = X[:effective_n], Y[:effective_n]\n",
    "n = len(Xval)\n",
    "effective_n = n - n%24\n",
    "Xval, Yval = Xval[:effective_n], Yval[:effective_n]\n",
    "training_datas.append({\"X\": X,\n",
    "                        \"Y\": Y,\n",
    "                        \"Xval\": Xval,\n",
    "                        \"Yval\": Yval,\n",
    "                        \"epochs\": 7,\n",
    "                        \"batch_size\": 128, \n",
    "                        \"name\": \"spam\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:163: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 013 | loss: 0.89757 | err: 0.33333\n",
      "batch 002 / 013 | loss: 1.00498 | err: 0.41667\n",
      "batch 003 / 013 | loss: 0.92932 | err: 0.38889\n",
      "batch 004 / 013 | loss: 0.80200 | err: 0.33333\n",
      "batch 005 / 013 | loss: 0.70121 | err: 0.29167\n",
      "batch 006 / 013 | loss: 0.63705 | err: 0.26389\n",
      "batch 007 / 013 | loss: 0.56297 | err: 0.23214\n",
      "batch 008 / 013 | loss: 0.53042 | err: 0.20833\n",
      "batch 009 / 013 | loss: 0.50730 | err: 0.19907\n",
      "batch 010 / 013 | loss: 0.47833 | err: 0.18750\n",
      "breaked\n",
      "training time: 12.630822658538818 seconds\n",
      "batch 001 / 030 | loss: 1.13260 | err: 0.75000\n",
      "batch 002 / 030 | loss: 1.06588 | err: 0.64583\n",
      "batch 003 / 030 | loss: 0.89686 | err: 0.52778\n",
      "batch 004 / 030 | loss: 0.74101 | err: 0.42708\n",
      "batch 005 / 030 | loss: 0.69943 | err: 0.39167\n",
      "batch 006 / 030 | loss: 0.64033 | err: 0.35417\n",
      "batch 007 / 030 | loss: 0.61426 | err: 0.33333\n",
      "batch 008 / 030 | loss: 0.60405 | err: 0.32292\n",
      "batch 009 / 030 | loss: 0.57561 | err: 0.30556\n",
      "batch 010 / 030 | loss: 0.52649 | err: 0.27917\n",
      "breaked\n",
      "training time: 11.334643363952637 seconds\n",
      "batch 001 / 093 | loss: 0.99290 | err: 0.50000\n",
      "batch 002 / 093 | loss: 0.87740 | err: 0.41667\n",
      "batch 003 / 093 | loss: 0.99725 | err: 0.47222\n",
      "batch 004 / 093 | loss: 0.99523 | err: 0.45833\n",
      "batch 005 / 093 | loss: 1.04902 | err: 0.47500\n",
      "batch 006 / 093 | loss: 1.01097 | err: 0.46528\n",
      "batch 007 / 093 | loss: 0.97902 | err: 0.45238\n",
      "batch 008 / 093 | loss: 0.99093 | err: 0.45833\n",
      "batch 009 / 093 | loss: 0.99796 | err: 0.45370\n",
      "batch 010 / 093 | loss: 0.97267 | err: 0.44167\n",
      "breaked\n",
      "training time: 6.865786552429199 seconds\n",
      "batch 001 / 221 | loss: 0.92669 | err: 0.33333\n",
      "batch 002 / 221 | loss: 1.01780 | err: 0.43750\n",
      "batch 003 / 221 | loss: 1.12735 | err: 0.51389\n",
      "batch 004 / 221 | loss: 1.27947 | err: 0.59375\n",
      "batch 005 / 221 | loss: 1.11022 | err: 0.51667\n",
      "batch 006 / 221 | loss: 1.06536 | err: 0.50000\n",
      "batch 007 / 221 | loss: 0.99673 | err: 0.47024\n",
      "batch 008 / 221 | loss: 0.91473 | err: 0.43229\n",
      "batch 009 / 221 | loss: 0.88285 | err: 0.42130\n",
      "batch 010 / 221 | loss: 0.86658 | err: 0.41667\n",
      "breaked\n",
      "training time: 5.980453014373779 seconds\n"
     ]
    }
   ],
   "source": [
    "PATH = \"C:/Users/sagil/Desktop/nir_project/models/real_dataset_runtimes\"\n",
    "\n",
    "for training_data in training_datas:\n",
    "    path = PATH + \"/\" + training_data[\"name\"]\n",
    "    \n",
    "    # load dataset\n",
    "    X = training_data[\"X\"]\n",
    "    Y = training_data[\"Y\"]\n",
    "    Xval = training_data[\"Xval\"]\n",
    "    Yval = training_data[\"Yval\"]\n",
    "    \n",
    "    # save dataset splits\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    pd.DataFrame(X.numpy()).to_csv(path + '/X.csv')\n",
    "    pd.DataFrame(Y.numpy()).to_csv(path + '/Y.csv')\n",
    "    pd.DataFrame(Xval.numpy()).to_csv(path + '/Xval.csv')\n",
    "    pd.DataFrame(Yval.numpy()).to_csv(path + '/Yval.csv')\n",
    "    \n",
    "    # training parameters\n",
    "    x_dim = len(X[0])\n",
    "    epochs = training_data[\"epochs\"]\n",
    "    batch_size = 24 # training_data[\"batch_size\"]\n",
    "    scale = 1\n",
    "    \n",
    "    funcs = {\"f\": f, \"g\": g, \"f_derivative\": f_derivative, \"c\": c, \"score\": score}\n",
    "    funcs_batch = {\"f\": f_batch, \"g\": g_batch, \"f_derivative\": f_derivative_batch, \"c\": c_batch, \"score\": score}\n",
    "\n",
    "\n",
    "    strategic_model = MyStrategicModel(x_dim, batch_size, funcs, funcs_batch, TRAIN_SLOPE, EVAL_SLOPE, scale=scale, strategic=True)\n",
    "    strategic_model.fit(path, X, Y, Xval, Yval,\n",
    "                opt=torch.optim.Adam, opt_kwargs={\"lr\": 5*(1e-1)},\n",
    "                batch_size=batch_size, epochs=epochs, verbose=True,\n",
    "               comment=\"batched\")\n",
    "    \n",
    "    runtimes = [strategic_model.total_time/10, strategic_model.ccp_time/10]     \n",
    "    pd.DataFrame(np.array(runtimes)).to_csv(path + '/results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "funcPred",
   "language": "python",
   "name": "funcpred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
