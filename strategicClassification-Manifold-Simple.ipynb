{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import cvxpy as cp\n",
    "import dccp\n",
    "import torch\n",
    "import numpy as np\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import zero_one_loss, confusion_matrix\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.patches as mpatches\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import os, psutil\n",
    "from datetime import datetime\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd.functional import jacobian\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "XDIM=2\n",
    "\n",
    "TRAIN_SLOPE = 1\n",
    "EVAL_SLOPE = 1\n",
    "X_LOWER_BOUND = -30\n",
    "X_UPPER_BOUND = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, Y, percentage):\n",
    "    num_val = int(len(X)*percentage)\n",
    "    return X[num_val:], Y[num_val:], X[:num_val], Y[:num_val]\n",
    "\n",
    "def shuffle(X, Y):\n",
    "    data = torch.cat((X, Y), 1)\n",
    "    data = data[torch.randperm(data.size()[0])]\n",
    "    X = data[:, :2]\n",
    "    Y = data[:, 2]\n",
    "    return X, Y\n",
    "\n",
    "def conf_mat(Y1, Y2):\n",
    "    num_of_samples = len(Y1)\n",
    "    mat = confusion_matrix(Y1, Y2, labels=[-1, 1])*100/num_of_samples\n",
    "    acc = np.trace(mat)\n",
    "    return mat, acc\n",
    "\n",
    "def calc_accuracy(Y, Ypred):\n",
    "    num = len(Y)\n",
    "    temp = Y - Ypred\n",
    "    acc = len(temp[temp == 0])*1./num\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_custom_normal_data(N, x_dim, pos_mean, pos_std, neg_mean, neg_std):\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    pos_samples_num = N//2\n",
    "    neg_samples_num = N - pos_samples_num\n",
    "    posX = torch.randn((pos_samples_num, x_dim))*pos_std + pos_mean\n",
    "    negX = torch.randn((neg_samples_num, x_dim))*neg_std + neg_mean\n",
    "    \n",
    "    X = torch.cat((posX, negX), 0)\n",
    "    Y = torch.unsqueeze(torch.cat((torch.ones(len(posX)), -torch.ones(len(negX))), 0), 1)\n",
    "\n",
    "    X, Y = shuffle(X, Y)\n",
    "    return X, Y\n",
    "\n",
    "def gen_custom_sin_data(N, shuff=True):\n",
    "    \n",
    "    def func(x):\n",
    "        return -(x**2)\n",
    "    \n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    pos_samples_num = N//2\n",
    "    neg_samples_num = N - pos_samples_num\n",
    "    \n",
    "    posX = torch.linspace(-5, 0, pos_samples_num)\n",
    "    posX = torch.stack([posX, func(posX)])\n",
    "    posX = torch.transpose(posX, 1, 0)\n",
    "    \n",
    "    negX = torch.linspace(0, 5, neg_samples_num)\n",
    "    negX = torch.stack([negX, func(negX)])\n",
    "    negX = torch.transpose(negX, 1, 0)\n",
    "    \n",
    "    X = torch.cat((posX, negX), 0)\n",
    "    Y = torch.unsqueeze(torch.cat((torch.ones(len(posX)), -torch.ones(len(negX))), 0), 1)\n",
    "    if shuff:\n",
    "        X, Y = shuffle(X, Y)\n",
    "    else:\n",
    "        Y = Y[:, 0]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCP classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCP:\n",
    "    def __init__(self, x_dim, h_dim, funcs):\n",
    "        self.f_derivative = funcs[\"f_derivative\"]\n",
    "        self.g = funcs[\"g\"]\n",
    "        self.c = funcs[\"c\"]\n",
    "        \n",
    "        self.x = cp.Variable(x_dim)\n",
    "        self.xt = cp.Parameter(x_dim)\n",
    "        self.r = cp.Parameter(x_dim)\n",
    "        self.w = cp.Parameter(x_dim)\n",
    "        self.b = cp.Parameter(1)\n",
    "        self.slope = cp.Parameter(1)\n",
    "        \n",
    "\n",
    "        target = self.x@self.f_derivative(self.xt, self.w, self.b, self.slope)-self.g(self.x, self.w, self.b, self.slope)-self.c(self.x, self.r, x_dim)\n",
    "        constraints = [self.x >= X_LOWER_BOUND,\n",
    "                       self.x <= X_UPPER_BOUND]\n",
    "        self.prob = cp.Problem(cp.Maximize(target), constraints)\n",
    "        \n",
    "    def ccp(self, r):\n",
    "        \"\"\"\n",
    "        numpy to numpy\n",
    "        \"\"\"\n",
    "        self.xt.value = r\n",
    "        self.r.value = r\n",
    "        result = self.prob.solve()\n",
    "        diff = np.linalg.norm(self.xt.value - self.x.value)\n",
    "        cnt = 0\n",
    "        while diff > 0.0001 and cnt < 10:\n",
    "            cnt += 1\n",
    "            self.xt.value = self.x.value\n",
    "            result = self.prob.solve()\n",
    "            diff = np.linalg.norm(self.x.value - self.xt.value)\n",
    "        return self.x.value\n",
    "    \n",
    "    def optimize_X(self, X, w, b, B_SPAN, slope):\n",
    "        \"\"\"\n",
    "        tensor to tensor\n",
    "        \"\"\"\n",
    "        X = X.numpy()\n",
    "        w = w.detach().numpy()\n",
    "        b = b.detach().numpy()\n",
    "        slope = np.full(1, slope)\n",
    "        \n",
    "        self.w.value = w\n",
    "        self.b.value = b\n",
    "        self.slope.value = slope\n",
    "        \n",
    "        return torch.stack([torch.from_numpy(self.ccp(x)) for x in X])\n",
    "    \n",
    "    \n",
    "class CCP_MANIFOLD:\n",
    "    def __init__(self, x_dim, h_dim, funcs):\n",
    "        self.f_derivative = funcs[\"f_derivative\"]\n",
    "        self.g = funcs[\"g\"]\n",
    "        self.c = funcs[\"c\"]\n",
    "        \n",
    "        self.x = cp.Variable(x_dim)\n",
    "        self.v = cp.Variable(h_dim)\n",
    "        self.xt = cp.Parameter(x_dim)\n",
    "        self.r = cp.Parameter(x_dim)\n",
    "        self.w = cp.Parameter(x_dim)\n",
    "        self.b = cp.Parameter(1)\n",
    "        self.B_span = cp.Parameter((x_dim, h_dim))\n",
    "        self.slope = cp.Parameter(1)\n",
    "        \n",
    "\n",
    "        target = self.x@self.f_derivative(self.xt, self.w, self.b, self.slope)-self.g(self.x, self.w, self.b, self.slope)-self.c(self.x, self.r, x_dim)\n",
    "        constraints = [self.x >= X_LOWER_BOUND,\n",
    "                       self.x <= X_UPPER_BOUND,\n",
    "                      self.B_span@self.v == self.x-self.r]\n",
    "        self.prob = cp.Problem(cp.Maximize(target), constraints)\n",
    "        \n",
    "    def ccp(self, r, B_span):\n",
    "        \"\"\"\n",
    "        numpy to numpy\n",
    "        \"\"\"\n",
    "        self.xt.value = r\n",
    "        self.r.value = r\n",
    "        self.B_span.value = B_span\n",
    "        result = self.prob.solve()\n",
    "        diff = np.linalg.norm(self.xt.value - self.x.value)\n",
    "        cnt = 0\n",
    "        while diff > 0.0001 and cnt < 10:\n",
    "            cnt += 1\n",
    "            self.xt.value = self.x.value\n",
    "            result = self.prob.solve()\n",
    "            diff = np.linalg.norm(self.x.value - self.xt.value)\n",
    "        return self.x.value\n",
    "    \n",
    "    def optimize_X(self, X, w, b, B_SPAN, slope):\n",
    "        \"\"\"\n",
    "        tensor to tensor\n",
    "        \"\"\"\n",
    "        X = X.numpy()\n",
    "        w = w.detach().numpy()\n",
    "        b = b.detach().numpy()\n",
    "        B_SPAN = B_SPAN.numpy()\n",
    "        slope = np.full(1, slope)\n",
    "        \n",
    "        self.w.value = w\n",
    "        self.b.value = b\n",
    "        self.slope.value = slope\n",
    "        \n",
    "        return torch.stack([torch.from_numpy(self.ccp(x, B_span)) for x, B_span in zip(X, B_SPAN)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DELTA():\n",
    "    \n",
    "    def __init__(self, x_dim, h_dim, funcs):\n",
    "        self.g = funcs[\"g\"]\n",
    "        self.c = funcs[\"c\"]\n",
    "        \n",
    "        self.x = cp.Variable(x_dim)\n",
    "        self.r = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.w = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.b = cp.Parameter(1, value = np.random.randn(1))\n",
    "        self.f_der = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "\n",
    "        target = self.x@self.f_der-self.g(self.x, self.w, self.b, TRAIN_SLOPE)-self.c(self.x, self.r, x_dim)\n",
    "        constraints = [self.x >= X_LOWER_BOUND,\n",
    "                       self.x <= X_UPPER_BOUND]\n",
    "        objective = cp.Maximize(target)\n",
    "        problem = cp.Problem(objective, constraints)\n",
    "        self.layer = CvxpyLayer(problem, parameters=[self.r, self.w, self.b, self.f_der],\n",
    "                                variables=[self.x])\n",
    "        \n",
    "        \n",
    "    def optimize_X(self, X, w, b, F_DER, B_SPAN):\n",
    "        return self.layer(X, w, b, F_DER)[0]\n",
    "    \n",
    "class DELTA_MANIFOLD():\n",
    "    \n",
    "    def __init__(self, x_dim, h_dim, funcs):\n",
    "        self.g = funcs[\"g\"]\n",
    "        self.c = funcs[\"c\"]\n",
    "        \n",
    "        self.x = cp.Variable(x_dim)\n",
    "        self.v = cp.Variable(h_dim)\n",
    "        self.r = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.w = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.b = cp.Parameter(1, value = np.random.randn(1))\n",
    "        self.f_der = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.B_span = cp.Parameter((x_dim, h_dim), value = np.random.randn(x_dim, h_dim))\n",
    "\n",
    "        target = self.x@self.f_der-self.g(self.x, self.w, self.b, TRAIN_SLOPE)-self.c(self.x, self.r, x_dim)\n",
    "        constraints = [self.x >= X_LOWER_BOUND,\n",
    "                       self.x <= X_UPPER_BOUND,\n",
    "                      self.B_span@self.v == self.x-self.r]\n",
    "        objective = cp.Maximize(target)\n",
    "        problem = cp.Problem(objective, constraints)\n",
    "        self.layer = CvxpyLayer(problem, parameters=[self.r, self.w, self.b, self.f_der, self.B_span],\n",
    "                                variables=[self.x, self.v])\n",
    "        \n",
    "        \n",
    "    def optimize_X(self, X, w, b, F_DER, B_SPAN):\n",
    "        return self.layer(X, w, b, F_DER, B_SPAN)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gain & Cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(x, w, b):\n",
    "    return x@w + b\n",
    "\n",
    "def f(x, w, b, slope):\n",
    "    return 0.5*cp.norm(cp.hstack([1, (slope*score(x, w, b) + 1)]), 2)\n",
    "\n",
    "def g(x, w, b, slope):\n",
    "    return 0.5*cp.norm(cp.hstack([1, (slope*score(x, w, b) - 1)]), 2)\n",
    "\n",
    "def c(x, r, x_dim):\n",
    "    return cp.sum_squares(x-r)/70\n",
    "\n",
    "def f_derivative(x, w, b, slope):\n",
    "    return 0.5*cp.multiply(slope*((slope*score(x, w, b) + 1)/cp.sqrt((slope*score(x, w, b) + 1)**2 + 1)), w)\n",
    "\n",
    "funcs = {\"f\": f, \"g\": g, \"f_derivative\": f_derivative, \"c\": c, \"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim, h2_dim, lamb=0):\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        super(CAE, self).__init__()\n",
    "        \n",
    "        self.lamb = lamb\n",
    "        self.x_dim = x_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.h2_dim = h2_dim\n",
    "        self.fc1 = nn.Linear(x_dim, h2_dim, bias = True) # Encoder\n",
    "#         self.fc2 = nn.Linear(h2_dim, h2_dim, bias = True)\n",
    "        self.fc3 = nn.Linear(h2_dim, h_dim, bias = True)\n",
    "        self.fc4 = nn.Linear(h_dim, h2_dim, bias = True)\n",
    "        self.fc5 = nn.Linear(h2_dim, h2_dim, bias = True)\n",
    "#         self.fc6 = nn.Linear(h2_dim, h2_dim, bias = True)\n",
    "        self.fc7 = nn.Linear(h2_dim, x_dim, bias = True)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encoder(self, x):\n",
    "        o1 = self.sigmoid(self.fc1(x))\n",
    "#         o2 = self.sigmoid(self.fc2(o1))\n",
    "        o2 = self.sigmoid(self.fc3(o1))\n",
    "        return o2\n",
    "    \n",
    "    def decoder(self, z):\n",
    "        o1 = self.sigmoid(self.fc4(z))\n",
    "        o2 = self.sigmoid(self.fc5(o1))\n",
    "#         o3 = self.sigmoid(self.fc6(o2))\n",
    "        return self.fc7(o2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = self.encoder(x)\n",
    "        h2 = self.decoder(h1)\n",
    "        return h1, h2\n",
    "        \n",
    "    def save_model(self, path, comment=None):\n",
    "        if comment is not None:\n",
    "            path += \"/\" + comment\n",
    "            \n",
    "        filename = path + \"/model.pt\"\n",
    "        if not os.path.exists(os.path.dirname(filename)):\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        torch.save(self.state_dict(), filename)\n",
    "    \n",
    "    def load_model(self, filename):\n",
    "        self.load_state_dict(torch.load(filename))\n",
    "        self.eval()\n",
    "        \n",
    "    def get_spans(self, X):\n",
    "        def func(x):\n",
    "            return self.forward(x)[0]\n",
    "        \n",
    "        B_SPANS = []\n",
    "        for x in X:\n",
    "            J = jacobian(func, x)\n",
    "            U, S, _ = torch.svd(torch.transpose(J, 0, 1))\n",
    "            B_span = U\n",
    "            B_SPANS.append(B_span)\n",
    "        return torch.stack(B_SPANS)\n",
    "    \n",
    "    def contractive_loss(self, x):\n",
    "        def func(x):\n",
    "            return self.encoder(x)\n",
    "        J = jacobian(func, x, create_graph=True)\n",
    "        c_loss = torch.norm(J, 2)**2\n",
    "        return c_loss\n",
    "    \n",
    "    def reconstruction_loss(self, x, x_recons):\n",
    "        mse_loss = nn.MSELoss(size_average = True)\n",
    "        r_loss = mse_loss(x_recons, x)\n",
    "        return r_loss\n",
    "        \n",
    "    def loss(self, x, x_recons, h):\n",
    "        r_loss = self.reconstruction_loss(x, x_recons)\n",
    "        c_loss = self.contractive_loss(x)\n",
    "        return r_loss, c_loss\n",
    "\n",
    "    def fit(self, path, X, Xval, opt, opt_kwargs={\"lr\":1e-3}, batch_size=128, epochs=100, verbose=False, comment=None):\n",
    "        train_dset = TensorDataset(X, torch.ones(len(X)))\n",
    "        train_loader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "        opt = opt(self.parameters(), **opt_kwargs)\n",
    "        \n",
    "        best_val_loss = 100\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_r_loss = 0\n",
    "            total_c_loss = 0\n",
    "            train_loss = 0\n",
    "            self.train()\n",
    "            \n",
    "            for idx, (Xbatch, _) in enumerate(train_loader):\n",
    "                Xbatch = Variable(Xbatch)\n",
    "                opt.zero_grad()\n",
    "                \n",
    "                hidden_representation, recons_x = self.forward(Xbatch)\n",
    "                r_loss, c_loss = self.loss(Xbatch, recons_x, hidden_representation)\n",
    "                l = r_loss + self.lamb*c_loss\n",
    "                l.backward()\n",
    "                train_loss += l.item()\n",
    "                total_r_loss += r_loss.item()\n",
    "                total_c_loss += c_loss.item()\n",
    "                opt.step()\n",
    "\n",
    "            \n",
    "            hidden_representation, recons_x = self.forward(Xval)\n",
    "            r_loss, c_loss = self.loss(Xval, recons_x, hidden_representation)\n",
    "            l = r_loss + self.lamb*c_loss\n",
    "            if l.item() < best_val_loss:\n",
    "                self.save_model(path, comment)\n",
    "                best_val_loss = l.item()\n",
    "                print(\"model saved!\")\n",
    "                \n",
    "            if verbose:\n",
    "                print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "                     epoch, l.item()), \" reconstruction loss: \", r_loss.item(), \"contractive_loss: \", c_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyStrategicModel(torch.nn.Module):\n",
    "    def __init__(self, x_dim, h_dim, funcs, train_slope, eval_slope, strategic=False, manifold=False):\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        super(MyStrategicModel, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.train_slope, self.eval_slope = train_slope, eval_slope\n",
    "        self.w = torch.nn.parameter.Parameter(math.sqrt(1/x_dim)*(1-2*torch.rand(x_dim, dtype=torch.float64, requires_grad=True)))\n",
    "        self.b = torch.nn.parameter.Parameter(math.sqrt(1/x_dim)*(1-2*torch.rand(1, dtype=torch.float64, requires_grad=True)))\n",
    "        self.strategic = strategic\n",
    "        self.manifold = manifold\n",
    "        if self.manifold:\n",
    "            self.ccp_train = CCP_MANIFOLD(self.x_dim, self.h_dim, funcs)\n",
    "            self.delta = DELTA_MANIFOLD(self.x_dim, self.h_dim, funcs)\n",
    "        else:\n",
    "            self.ccp_train = CCP(self.x_dim, self.h_dim, funcs)\n",
    "            self.delta = DELTA(self.x_dim, self.h_dim, funcs)\n",
    "        \n",
    "        self.ccp_test = CCP_MANIFOLD(self.x_dim, self.h_dim, funcs)\n",
    "\n",
    "    def forward(self, X, B_SPANS, evaluation=False):\n",
    "        if self.strategic:            \n",
    "            if evaluation:\n",
    "                XT = self.ccp_train.optimize_X(X, self.w, self.b, B_SPANS, self.eval_slope)\n",
    "                X_opt = XT\n",
    "            else:\n",
    "                XT = self.ccp_train.optimize_X(X, self.w, self.b, B_SPANS, self.train_slope)\n",
    "                F_DER = self.get_f_ders(XT, self.train_slope)\n",
    "                X_opt = self.delta.optimize_X(X, self.w, self.b, F_DER, B_SPANS) # Xopt should be equal to XT but we do it again for the gradients\n",
    "                \n",
    "            output = self.score(X_opt)\n",
    "        else:\n",
    "            output = self.score(X)        \n",
    "        return output\n",
    "    \n",
    "    def optimize_X(self, X, B_SPANS):\n",
    "        return self.ccp_test.optimize_X(X, self.w, self.b, B_SPANS, self.eval_slope)\n",
    "    \n",
    "    def normalize_weights(self):\n",
    "        with torch.no_grad():\n",
    "            norm = torch.sqrt(torch.sum(self.w**2) + self.b**2)\n",
    "            self.w /= norm\n",
    "            self.b /= norm\n",
    "    \n",
    "    def score(self, x):\n",
    "        return x@self.w + self.b\n",
    "    \n",
    "    def get_f_ders(self, XT, slope):\n",
    "        return torch.stack([0.5*slope*((slope*self.score(xt) + 1)/torch.sqrt((slope*self.score(xt) + 1)**2 + 1))*self.w for xt in XT])\n",
    "\n",
    "    def calc_accuracy(self, Y, Y_pred):\n",
    "        Y_pred = torch.sign(Y_pred)\n",
    "        num = len(Y)\n",
    "        temp = Y - Y_pred\n",
    "        acc = len(temp[temp == 0])*1./num        \n",
    "        return acc\n",
    "    \n",
    "    def evaluate(self, X, B_SPANS, Y):      \n",
    "        return self.calc_accuracy(Y, self.forward(X, B_SPANS, evaluation=True))\n",
    "    \n",
    "    def loss(self, Y, Y_pred):\n",
    "        return torch.mean(torch.clamp(1 - Y_pred * Y, min=0))\n",
    "    \n",
    "    def save_model(self, train_errors, val_errors, train_losses, val_losses, info, path, comment=None):\n",
    "        if comment is not None:\n",
    "            path += \"/\" + comment\n",
    "            \n",
    "        filename = path + \"/model.pt\"\n",
    "        if not os.path.exists(os.path.dirname(filename)):\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        torch.save(self.state_dict(), filename)\n",
    "        \n",
    "        pd.DataFrame(np.array(train_errors)).to_csv(path + '/train_errors.csv')\n",
    "        pd.DataFrame(np.array(val_errors)).to_csv(path + '/val_errors.csv')\n",
    "        pd.DataFrame(np.array(train_losses)).to_csv(path + '/train_losses.csv')\n",
    "        pd.DataFrame(np.array(val_losses)).to_csv(path + '/val_losses.csv')\n",
    "        \n",
    "        with open(path + \"/info.txt\", \"w\") as f:\n",
    "            f.write(info)\n",
    "    \n",
    "    def load_model(self, filename):\n",
    "        self.load_state_dict(torch.load(filename))\n",
    "        self.eval()\n",
    "    \n",
    "    def fit(self, path, X, B_SPANS, Y, Xval, B_SPANSval, Yval, opt, opt_kwargs={\"lr\":1e-3}, batch_size=128, epochs=100, verbose=False, callback=None, comment=None):\n",
    "        train_dset = TensorDataset(X, B_SPANS, Y)\n",
    "        train_loader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "        opt = opt(self.parameters(), **opt_kwargs)\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_errors = []\n",
    "        val_errors = []\n",
    "        \n",
    "        best_val_error = 1\n",
    "        consecutive_no_improvement = 0\n",
    "\n",
    "        total_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            t1 = time.time()\n",
    "            batch = 1\n",
    "            train_losses.append([])\n",
    "            train_errors.append([])\n",
    "            for Xbatch, B_SPANSbatch, Ybatch in train_loader:\n",
    "                opt.zero_grad()\n",
    "                Ybatch_pred = self.forward(Xbatch, B_SPANSbatch)\n",
    "                l = self.loss(Ybatch, Ybatch_pred)\n",
    "                l.backward()\n",
    "                opt.step()\n",
    "                train_losses[-1].append(l.item())\n",
    "                with torch.no_grad():\n",
    "                    e = self.calc_accuracy(Ybatch, Ybatch_pred)\n",
    "                    train_errors[-1].append(1-e)\n",
    "                if verbose:\n",
    "                    print(\"batch %03d / %03d | loss: %3.5f | err: %3.5f\" %\n",
    "                          (batch, len(train_loader), np.mean(train_losses[-1]), np.mean(train_errors[-1])))\n",
    "                batch += 1\n",
    "                if callback is not None:\n",
    "                    callback()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                Yval_pred = self.forward(Xval, B_SPANSval, evaluation=True)\n",
    "                val_loss = self.loss(Yval, Yval_pred).item()\n",
    "                val_losses.append(val_loss)\n",
    "                val_error = 1-self.calc_accuracy(Yval, Yval_pred)\n",
    "                val_errors.append(val_error)\n",
    "                if val_error < best_val_error:\n",
    "                    consecutive_no_improvement = 0\n",
    "                    best_val_error = val_error\n",
    "                    info = \"training time in seconds: {}\\nepoch: {}\\nbatch size: {}\\ntrain slope: {}\\neval slope: {}\\nlearning rate: {}\\nvalidation loss: {}\\nvalidation error: {}\\n\".format(\n",
    "                    time.time()-total_time, epoch, batch_size, self.train_slope, self.eval_slope, opt_kwargs[\"lr\"], val_loss, val_error)\n",
    "                    self.save_model(train_errors, val_errors, train_losses, val_losses, info, path, comment)\n",
    "                    print(\"model saved!\")\n",
    "                else:\n",
    "                    consecutive_no_improvement += 1\n",
    "                    if consecutive_no_improvement >= 4:\n",
    "                        break\n",
    "                \n",
    "            t2 = time.time()\n",
    "            if verbose:\n",
    "                print(\"----- epoch %03d / %03d | time: %03d sec | loss: %3.5f | err: %3.5f\" % (epoch + 1, epochs, t2-t1, val_losses[-1], val_errors[-1]))\n",
    "        print(\"training time: {} seconds\".format(time.time()-total_time)) \n",
    "        return train_errors, val_errors, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1200, 2]) torch.Size([1200])\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "/* global mpl */\n",
       "window.mpl = {};\n",
       "\n",
       "mpl.get_websocket_type = function () {\n",
       "    if (typeof WebSocket !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof MozWebSocket !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert(\n",
       "            'Your browser does not have WebSocket support. ' +\n",
       "                'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "                'Firefox 4 and 5 are also supported but you ' +\n",
       "                'have to enable WebSockets in about:config.'\n",
       "        );\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = this.ws.binaryType !== undefined;\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById('mpl-warnings');\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent =\n",
       "                'This browser does not support binary websocket messages. ' +\n",
       "                'Performance may be slow.';\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = document.createElement('div');\n",
       "    this.root.setAttribute('style', 'display: inline-block');\n",
       "    this._root_extra_style(this.root);\n",
       "\n",
       "    parent_element.appendChild(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen = function () {\n",
       "        fig.send_message('supports_binary', { value: fig.supports_binary });\n",
       "        fig.send_message('send_image_mode', {});\n",
       "        if (mpl.ratio !== 1) {\n",
       "            fig.send_message('set_dpi_ratio', { dpi_ratio: mpl.ratio });\n",
       "        }\n",
       "        fig.send_message('refresh', {});\n",
       "    };\n",
       "\n",
       "    this.imageObj.onload = function () {\n",
       "        if (fig.image_mode === 'full') {\n",
       "            // Full images could contain transparency (where diff images\n",
       "            // almost always do), so we need to clear the canvas so that\n",
       "            // there is no ghosting.\n",
       "            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "        }\n",
       "        fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "    };\n",
       "\n",
       "    this.imageObj.onunload = function () {\n",
       "        fig.ws.close();\n",
       "    };\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_header = function () {\n",
       "    var titlebar = document.createElement('div');\n",
       "    titlebar.classList =\n",
       "        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n",
       "    var titletext = document.createElement('div');\n",
       "    titletext.classList = 'ui-dialog-title';\n",
       "    titletext.setAttribute(\n",
       "        'style',\n",
       "        'width: 100%; text-align: center; padding: 3px;'\n",
       "    );\n",
       "    titlebar.appendChild(titletext);\n",
       "    this.root.appendChild(titlebar);\n",
       "    this.header = titletext;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = (this.canvas_div = document.createElement('div'));\n",
       "    canvas_div.setAttribute(\n",
       "        'style',\n",
       "        'border: 1px solid #ddd;' +\n",
       "            'box-sizing: content-box;' +\n",
       "            'clear: both;' +\n",
       "            'min-height: 1px;' +\n",
       "            'min-width: 1px;' +\n",
       "            'outline: 0;' +\n",
       "            'overflow: hidden;' +\n",
       "            'position: relative;' +\n",
       "            'resize: both;'\n",
       "    );\n",
       "\n",
       "    function on_keyboard_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.key_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    canvas_div.addEventListener(\n",
       "        'keydown',\n",
       "        on_keyboard_event_closure('key_press')\n",
       "    );\n",
       "    canvas_div.addEventListener(\n",
       "        'keyup',\n",
       "        on_keyboard_event_closure('key_release')\n",
       "    );\n",
       "\n",
       "    this._canvas_extra_style(canvas_div);\n",
       "    this.root.appendChild(canvas_div);\n",
       "\n",
       "    var canvas = (this.canvas = document.createElement('canvas'));\n",
       "    canvas.classList.add('mpl-canvas');\n",
       "    canvas.setAttribute('style', 'box-sizing: content-box;');\n",
       "\n",
       "    this.context = canvas.getContext('2d');\n",
       "\n",
       "    var backingStore =\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        this.context.webkitBackingStorePixelRatio ||\n",
       "        this.context.mozBackingStorePixelRatio ||\n",
       "        this.context.msBackingStorePixelRatio ||\n",
       "        this.context.oBackingStorePixelRatio ||\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n",
       "        'canvas'\n",
       "    ));\n",
       "    rubberband_canvas.setAttribute(\n",
       "        'style',\n",
       "        'box-sizing: content-box; position: absolute; left: 0; top: 0; z-index: 1;'\n",
       "    );\n",
       "\n",
       "    var resizeObserver = new ResizeObserver(function (entries) {\n",
       "        var nentries = entries.length;\n",
       "        for (var i = 0; i < nentries; i++) {\n",
       "            var entry = entries[i];\n",
       "            var width, height;\n",
       "            if (entry.contentBoxSize) {\n",
       "                if (entry.contentBoxSize instanceof Array) {\n",
       "                    // Chrome 84 implements new version of spec.\n",
       "                    width = entry.contentBoxSize[0].inlineSize;\n",
       "                    height = entry.contentBoxSize[0].blockSize;\n",
       "                } else {\n",
       "                    // Firefox implements old version of spec.\n",
       "                    width = entry.contentBoxSize.inlineSize;\n",
       "                    height = entry.contentBoxSize.blockSize;\n",
       "                }\n",
       "            } else {\n",
       "                // Chrome <84 implements even older version of spec.\n",
       "                width = entry.contentRect.width;\n",
       "                height = entry.contentRect.height;\n",
       "            }\n",
       "\n",
       "            // Keep the size of the canvas and rubber band canvas in sync with\n",
       "            // the canvas container.\n",
       "            if (entry.devicePixelContentBoxSize) {\n",
       "                // Chrome 84 implements new version of spec.\n",
       "                canvas.setAttribute(\n",
       "                    'width',\n",
       "                    entry.devicePixelContentBoxSize[0].inlineSize\n",
       "                );\n",
       "                canvas.setAttribute(\n",
       "                    'height',\n",
       "                    entry.devicePixelContentBoxSize[0].blockSize\n",
       "                );\n",
       "            } else {\n",
       "                canvas.setAttribute('width', width * mpl.ratio);\n",
       "                canvas.setAttribute('height', height * mpl.ratio);\n",
       "            }\n",
       "            canvas.setAttribute(\n",
       "                'style',\n",
       "                'width: ' + width + 'px; height: ' + height + 'px;'\n",
       "            );\n",
       "\n",
       "            rubberband_canvas.setAttribute('width', width);\n",
       "            rubberband_canvas.setAttribute('height', height);\n",
       "\n",
       "            // And update the size in Python. We ignore the initial 0/0 size\n",
       "            // that occurs as the element is placed into the DOM, which should\n",
       "            // otherwise not happen due to the minimum size styling.\n",
       "            if (width != 0 && height != 0) {\n",
       "                fig.request_resize(width, height);\n",
       "            }\n",
       "        }\n",
       "    });\n",
       "    resizeObserver.observe(canvas_div);\n",
       "\n",
       "    function on_mouse_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.mouse_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousedown',\n",
       "        on_mouse_event_closure('button_press')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseup',\n",
       "        on_mouse_event_closure('button_release')\n",
       "    );\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousemove',\n",
       "        on_mouse_event_closure('motion_notify')\n",
       "    );\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseenter',\n",
       "        on_mouse_event_closure('figure_enter')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseleave',\n",
       "        on_mouse_event_closure('figure_leave')\n",
       "    );\n",
       "\n",
       "    canvas_div.addEventListener('wheel', function (event) {\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        on_mouse_event_closure('scroll')(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.appendChild(canvas);\n",
       "    canvas_div.appendChild(rubberband_canvas);\n",
       "\n",
       "    this.rubberband_context = rubberband_canvas.getContext('2d');\n",
       "    this.rubberband_context.strokeStyle = '#000000';\n",
       "\n",
       "    this._resize_canvas = function (width, height, forward) {\n",
       "        if (forward) {\n",
       "            canvas_div.style.width = width + 'px';\n",
       "            canvas_div.style.height = height + 'px';\n",
       "        }\n",
       "    };\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    this.rubberband_canvas.addEventListener('contextmenu', function (_e) {\n",
       "        event.preventDefault();\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus() {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'mpl-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'mpl-button-group';\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'mpl-button-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        var button = (fig.buttons[name] = document.createElement('button'));\n",
       "        button.classList = 'mpl-widget';\n",
       "        button.setAttribute('role', 'button');\n",
       "        button.setAttribute('aria-disabled', 'false');\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "\n",
       "        var icon_img = document.createElement('img');\n",
       "        icon_img.src = '_images/' + image + '.png';\n",
       "        icon_img.srcset = '_images/' + image + '_large.png 2x';\n",
       "        icon_img.alt = tooltip;\n",
       "        button.appendChild(icon_img);\n",
       "\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    var fmt_picker = document.createElement('select');\n",
       "    fmt_picker.classList = 'mpl-widget';\n",
       "    toolbar.appendChild(fmt_picker);\n",
       "    this.format_dropdown = fmt_picker;\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = document.createElement('option');\n",
       "        option.selected = fmt === mpl.default_extension;\n",
       "        option.innerHTML = fmt;\n",
       "        fmt_picker.appendChild(option);\n",
       "    }\n",
       "\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', { width: x_pixels, height: y_pixels });\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_message = function (type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function () {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function (fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1], msg['forward']);\n",
       "        fig.send_message('refresh', {});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function (fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0,\n",
       "        0,\n",
       "        fig.canvas.width / mpl.ratio,\n",
       "        fig.canvas.height / mpl.ratio\n",
       "    );\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function (fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function (fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch (cursor) {\n",
       "        case 0:\n",
       "            cursor = 'pointer';\n",
       "            break;\n",
       "        case 1:\n",
       "            cursor = 'default';\n",
       "            break;\n",
       "        case 2:\n",
       "            cursor = 'crosshair';\n",
       "            break;\n",
       "        case 3:\n",
       "            cursor = 'move';\n",
       "            break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_message = function (fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function (fig, _msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function (fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n",
       "    for (var key in msg) {\n",
       "        if (!(key in fig.buttons)) {\n",
       "            continue;\n",
       "        }\n",
       "        fig.buttons[key].disabled = !msg[key];\n",
       "        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n",
       "    if (msg['mode'] === 'PAN') {\n",
       "        fig.buttons['Pan'].classList.add('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    } else if (msg['mode'] === 'ZOOM') {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.add('active');\n",
       "    } else {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message('ack', {});\n",
       "};\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function (fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = 'image/png';\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src\n",
       "                );\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data\n",
       "            );\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        } else if (\n",
       "            typeof evt.data === 'string' &&\n",
       "            evt.data.slice(0, 21) === 'data:image/png;base64'\n",
       "        ) {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig['handle_' + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\n",
       "                \"No handler for the '\" + msg_type + \"' message type: \",\n",
       "                msg\n",
       "            );\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\n",
       "                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n",
       "                    e,\n",
       "                    e.stack,\n",
       "                    msg\n",
       "                );\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "};\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function (e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e) {\n",
       "        e = window.event;\n",
       "    }\n",
       "    if (e.target) {\n",
       "        targ = e.target;\n",
       "    } else if (e.srcElement) {\n",
       "        targ = e.srcElement;\n",
       "    }\n",
       "    if (targ.nodeType === 3) {\n",
       "        // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "    }\n",
       "\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    var boundingRect = targ.getBoundingClientRect();\n",
       "    var x = e.pageX - (boundingRect.left + document.body.scrollLeft);\n",
       "    var y = e.pageY - (boundingRect.top + document.body.scrollTop);\n",
       "\n",
       "    return { x: x, y: y };\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys(original) {\n",
       "    return Object.keys(original).reduce(function (obj, key) {\n",
       "        if (typeof original[key] !== 'object') {\n",
       "            obj[key] = original[key];\n",
       "        }\n",
       "        return obj;\n",
       "    }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function (event, name) {\n",
       "    var canvas_pos = mpl.findpos(event);\n",
       "\n",
       "    if (name === 'button_press') {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {\n",
       "        x: x,\n",
       "        y: y,\n",
       "        button: event.button,\n",
       "        step: event.step,\n",
       "        guiEvent: simpleKeys(event),\n",
       "    });\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (_event, _name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.key_event = function (event, name) {\n",
       "    // Prevent repeat events\n",
       "    if (name === 'key_press') {\n",
       "        if (event.which === this._key) {\n",
       "            return;\n",
       "        } else {\n",
       "            this._key = event.which;\n",
       "        }\n",
       "    }\n",
       "    if (name === 'key_release') {\n",
       "        this._key = null;\n",
       "    }\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which !== 17) {\n",
       "        value += 'ctrl+';\n",
       "    }\n",
       "    if (event.altKey && event.which !== 18) {\n",
       "        value += 'alt+';\n",
       "    }\n",
       "    if (event.shiftKey && event.which !== 16) {\n",
       "        value += 'shift+';\n",
       "    }\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function (name) {\n",
       "    if (name === 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message('toolbar_button', { name: name });\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";/* global mpl */\n",
       "\n",
       "var comm_websocket_adapter = function (comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function () {\n",
       "        comm.close();\n",
       "    };\n",
       "    ws.send = function (m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function (msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data']);\n",
       "    });\n",
       "    return ws;\n",
       "};\n",
       "\n",
       "mpl.mpl_figure_comm = function (comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = document.getElementById(id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm);\n",
       "\n",
       "    function ondownload(figure, _format) {\n",
       "        window.open(figure.canvas.toDataURL());\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element;\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error('Failed to find cell for figure', id, fig);\n",
       "        return;\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function (fig, msg) {\n",
       "    var width = fig.canvas.width / mpl.ratio;\n",
       "    fig.root.removeEventListener('remove', this._remove_fig_handler);\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable();\n",
       "    fig.parent_element.innerHTML =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "    fig.close_ws(fig, msg);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.close_ws = function (fig, msg) {\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function (_remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width / mpl.ratio;\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message('ack', {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () {\n",
       "        fig.push_to_output();\n",
       "    }, 1000);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'btn-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'btn-group';\n",
       "    var button;\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'btn-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        button = fig.buttons[name] = document.createElement('button');\n",
       "        button.classList = 'btn btn-default';\n",
       "        button.href = '#';\n",
       "        button.title = name;\n",
       "        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message pull-right';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = document.createElement('div');\n",
       "    buttongrp.classList = 'btn-group inline pull-right';\n",
       "    button = document.createElement('button');\n",
       "    button.classList = 'btn btn-mini btn-primary';\n",
       "    button.href = '#';\n",
       "    button.title = 'Stop Interaction';\n",
       "    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n",
       "    button.addEventListener('click', function (_evt) {\n",
       "        fig.handle_close(fig, {});\n",
       "    });\n",
       "    button.addEventListener(\n",
       "        'mouseover',\n",
       "        on_mouseover_closure('Stop Interaction')\n",
       "    );\n",
       "    buttongrp.appendChild(button);\n",
       "    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n",
       "    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._remove_fig_handler = function () {\n",
       "    this.close_ws(this, {});\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (el) {\n",
       "    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n",
       "    el.addEventListener('remove', this._remove_fig_handler);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (el) {\n",
       "    // this is important to make the div 'focusable\n",
       "    el.setAttribute('tabindex', 0);\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    } else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (event, _name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager) {\n",
       "        manager = IPython.keyboard_manager;\n",
       "    }\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which === 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "};\n",
       "\n",
       "mpl.find_output_cell = function (html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i = 0; i < ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code') {\n",
       "            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] === html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "};\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel !== null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target(\n",
       "        'matplotlib',\n",
       "        mpl.mpl_figure_comm\n",
       "    );\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"640\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1200, 2]) torch.Size([400, 2])\n",
      "percent of positive samples: 50.333333333333336%\n"
     ]
    }
   ],
   "source": [
    "N = 2000\n",
    "x_dim = 2\n",
    "h_dim = 1\n",
    "h_dim2 = 20\n",
    "X, Y = gen_custom_sin_data(N)\n",
    "X, Y, Xval, Yval = split_data(X, Y, 0.4)\n",
    "Xval, Yval, Xtest, Ytest = split_data(Xval, Yval, 0.5)\n",
    "\n",
    "print(X.size(), Y.size())\n",
    "visualize_data(X, Y)\n",
    "print(X.size(), Xval.size())\n",
    "print(\"percent of positive samples: {}%\".format(100 * len(Y[Y == 1]) / len(Y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved!\n",
      "====> Epoch: 0 Average loss: 52.1230  reconstruction loss:  52.122942832240895 contractive_loss:  0.07314185198830825\n",
      "model saved!\n",
      "====> Epoch: 1 Average loss: 40.5996  reconstruction loss:  40.59964531173208 contractive_loss:  0.018530086353149247\n",
      "model saved!\n",
      "====> Epoch: 2 Average loss: 32.4609  reconstruction loss:  32.46090892089385 contractive_loss:  0.00587116691642836\n",
      "model saved!\n",
      "====> Epoch: 3 Average loss: 31.5265  reconstruction loss:  31.526528208106427 contractive_loss:  0.003590343110369815\n",
      "====> Epoch: 4 Average loss: 31.6957  reconstruction loss:  31.695706060617905 contractive_loss:  0.002186180443766812\n",
      "model saved!\n",
      "====> Epoch: 5 Average loss: 31.4285  reconstruction loss:  31.428477793211176 contractive_loss:  0.0014266636241389013\n",
      "model saved!\n",
      "====> Epoch: 6 Average loss: 31.4076  reconstruction loss:  31.40761233389826 contractive_loss:  0.0010260920070327409\n",
      "model saved!\n",
      "====> Epoch: 7 Average loss: 31.3971  reconstruction loss:  31.397050885114385 contractive_loss:  0.0008038172200390998\n",
      "====> Epoch: 8 Average loss: 31.4353  reconstruction loss:  31.435272775707464 contractive_loss:  0.0006647507046047407\n",
      "====> Epoch: 9 Average loss: 31.4119  reconstruction loss:  31.41194540922291 contractive_loss:  0.0005695474339857207\n",
      "====> Epoch: 10 Average loss: 31.4283  reconstruction loss:  31.428298706161808 contractive_loss:  0.0004990746040765011\n",
      "====> Epoch: 11 Average loss: 31.3985  reconstruction loss:  31.398519054838236 contractive_loss:  0.0004509882564186815\n",
      "====> Epoch: 12 Average loss: 31.4029  reconstruction loss:  31.402927733454106 contractive_loss:  0.000414897750354255\n",
      "====> Epoch: 13 Average loss: 31.3999  reconstruction loss:  31.39991014980926 contractive_loss:  0.0003877307686632957\n",
      "====> Epoch: 14 Average loss: 31.3979  reconstruction loss:  31.39793084051441 contractive_loss:  0.000369719001907891\n",
      "====> Epoch: 15 Average loss: 31.4032  reconstruction loss:  31.403213884299166 contractive_loss:  0.0003588769460481864\n",
      "====> Epoch: 16 Average loss: 31.3971  reconstruction loss:  31.39709730701434 contractive_loss:  0.00035461153659032015\n",
      "====> Epoch: 17 Average loss: 31.3976  reconstruction loss:  31.39755987384713 contractive_loss:  0.0003573530253219099\n",
      "====> Epoch: 18 Average loss: 31.4233  reconstruction loss:  31.423265387622187 contractive_loss:  0.0003677999631562493\n",
      "====> Epoch: 19 Average loss: 31.3988  reconstruction loss:  31.398764062526897 contractive_loss:  0.0003870111269711834\n",
      "====> Epoch: 20 Average loss: 31.4042  reconstruction loss:  31.40415108622726 contractive_loss:  0.00041893309515400617\n",
      "====> Epoch: 21 Average loss: 31.4086  reconstruction loss:  31.408629980515663 contractive_loss:  0.0004662424388220509\n",
      "model saved!\n",
      "====> Epoch: 22 Average loss: 31.3970  reconstruction loss:  31.397010051158066 contractive_loss:  0.0005405290566619631\n",
      "====> Epoch: 23 Average loss: 31.4353  reconstruction loss:  31.435344624880734 contractive_loss:  0.0006577718890850196\n",
      "====> Epoch: 24 Average loss: 31.4152  reconstruction loss:  31.415191001844846 contractive_loss:  0.0008458861758338391\n",
      "====> Epoch: 25 Average loss: 31.4209  reconstruction loss:  31.42088845278048 contractive_loss:  0.0012040682096306266\n",
      "====> Epoch: 26 Average loss: 31.4065  reconstruction loss:  31.40649087051819 contractive_loss:  0.0018858873383194987\n",
      "====> Epoch: 27 Average loss: 31.3992  reconstruction loss:  31.399245306473112 contractive_loss:  0.0036212240567047257\n",
      "====> Epoch: 28 Average loss: 31.3982  reconstruction loss:  31.398227996488632 contractive_loss:  0.009464122444414039\n",
      "====> Epoch: 29 Average loss: 31.4126  reconstruction loss:  31.41259256266433 contractive_loss:  0.03522698145763557\n",
      "model saved!\n",
      "====> Epoch: 30 Average loss: 31.3969  reconstruction loss:  31.396841101053678 contractive_loss:  0.17187759365015023\n",
      "model saved!\n",
      "====> Epoch: 31 Average loss: 31.3843  reconstruction loss:  31.384289964090684 contractive_loss:  0.26344848423462214\n",
      "model saved!\n",
      "====> Epoch: 32 Average loss: 31.0020  reconstruction loss:  31.001930184874638 contractive_loss:  0.48269321678417537\n",
      "model saved!\n",
      "====> Epoch: 33 Average loss: 27.3943  reconstruction loss:  27.39411994367994 contractive_loss:  2.282178669491294\n",
      "model saved!\n",
      "====> Epoch: 34 Average loss: 18.7165  reconstruction loss:  18.716341929133613 contractive_loss:  1.5252993393108878\n",
      "model saved!\n",
      "====> Epoch: 35 Average loss: 12.2329  reconstruction loss:  12.232570854319254 contractive_loss:  3.4226468615687384\n",
      "model saved!\n",
      "====> Epoch: 36 Average loss: 10.5636  reconstruction loss:  10.563402471094305 contractive_loss:  2.2091788735025877\n",
      "====> Epoch: 37 Average loss: 11.0936  reconstruction loss:  11.093513122475576 contractive_loss:  1.1047348464103544\n",
      "model saved!\n",
      "====> Epoch: 38 Average loss: 9.4894  reconstruction loss:  9.48929254224152 contractive_loss:  0.90219382002647\n",
      "model saved!\n",
      "====> Epoch: 39 Average loss: 7.3700  reconstruction loss:  7.369874565802514 contractive_loss:  0.8022159608853754\n",
      "model saved!\n",
      "====> Epoch: 40 Average loss: 6.4874  reconstruction loss:  6.487274781970066 contractive_loss:  1.0141438109938345\n",
      "model saved!\n",
      "====> Epoch: 41 Average loss: 6.0441  reconstruction loss:  6.044015051971965 contractive_loss:  1.1790219355311753\n",
      "model saved!\n",
      "====> Epoch: 42 Average loss: 5.7405  reconstruction loss:  5.740348676330807 contractive_loss:  1.043938907906005\n",
      "model saved!\n",
      "====> Epoch: 43 Average loss: 5.6271  reconstruction loss:  5.627014786764817 contractive_loss:  1.0079169474470422\n",
      "====> Epoch: 44 Average loss: 5.6359  reconstruction loss:  5.635807543435487 contractive_loss:  0.8904274918718208\n",
      "model saved!\n",
      "====> Epoch: 45 Average loss: 5.4965  reconstruction loss:  5.496402556094831 contractive_loss:  0.9992696534999169\n",
      "model saved!\n",
      "====> Epoch: 46 Average loss: 5.2128  reconstruction loss:  5.212741086640547 contractive_loss:  1.059686775544205\n",
      "model saved!\n",
      "====> Epoch: 47 Average loss: 5.1633  reconstruction loss:  5.163134440048476 contractive_loss:  1.2657529400596712\n",
      "model saved!\n",
      "====> Epoch: 48 Average loss: 5.1180  reconstruction loss:  5.117905794769797 contractive_loss:  1.3590816981446332\n",
      "====> Epoch: 49 Average loss: 5.3210  reconstruction loss:  5.3208485715623155 contractive_loss:  1.3876299630271134\n",
      "model saved!\n",
      "====> Epoch: 50 Average loss: 4.9606  reconstruction loss:  4.96041587632978 contractive_loss:  1.3771444997324156\n",
      "model saved!\n",
      "====> Epoch: 51 Average loss: 4.7866  reconstruction loss:  4.786466954098394 contractive_loss:  1.3733149328295362\n",
      "model saved!\n",
      "====> Epoch: 52 Average loss: 4.6777  reconstruction loss:  4.6775847216530595 contractive_loss:  1.118815202302174\n",
      "model saved!\n",
      "====> Epoch: 53 Average loss: 4.6238  reconstruction loss:  4.623681069427704 contractive_loss:  1.105383779951118\n",
      "model saved!\n",
      "====> Epoch: 54 Average loss: 4.6085  reconstruction loss:  4.608362626350581 contractive_loss:  1.1475746810495582\n",
      "model saved!\n",
      "====> Epoch: 55 Average loss: 4.5995  reconstruction loss:  4.599340757266296 contractive_loss:  1.2610827195019059\n",
      "model saved!\n",
      "====> Epoch: 56 Average loss: 4.5103  reconstruction loss:  4.510149980870736 contractive_loss:  1.3279047788227398\n",
      "====> Epoch: 57 Average loss: 4.6456  reconstruction loss:  4.6455126567092835 contractive_loss:  1.3410858768398253\n",
      "====> Epoch: 58 Average loss: 4.5366  reconstruction loss:  4.536487072709426 contractive_loss:  1.421211411356467\n",
      "====> Epoch: 59 Average loss: 4.6354  reconstruction loss:  4.635213390021536 contractive_loss:  1.5303305378585936\n",
      "====> Epoch: 60 Average loss: 4.6952  reconstruction loss:  4.695008557372019 contractive_loss:  1.4453238235907107\n",
      "====> Epoch: 61 Average loss: 4.8173  reconstruction loss:  4.817194493781869 contractive_loss:  1.4849664124456978\n",
      "model saved!\n",
      "====> Epoch: 62 Average loss: 4.4523  reconstruction loss:  4.452191057557542 contractive_loss:  1.567416976341606\n",
      "====> Epoch: 63 Average loss: 4.5146  reconstruction loss:  4.514464815205462 contractive_loss:  1.6163879088216853\n",
      "model saved!\n",
      "====> Epoch: 64 Average loss: 4.4484  reconstruction loss:  4.448265342158522 contractive_loss:  1.5997656154111246\n",
      "====> Epoch: 65 Average loss: 4.4822  reconstruction loss:  4.482036981048412 contractive_loss:  1.6878302990523453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved!\n",
      "====> Epoch: 66 Average loss: 4.4187  reconstruction loss:  4.418531469941298 contractive_loss:  1.660240294025856\n",
      "model saved!\n",
      "====> Epoch: 67 Average loss: 4.3885  reconstruction loss:  4.388352191984438 contractive_loss:  1.6698907657886526\n",
      "====> Epoch: 68 Average loss: 4.4313  reconstruction loss:  4.431104251296076 contractive_loss:  1.6811304510385496\n",
      "====> Epoch: 69 Average loss: 4.4390  reconstruction loss:  4.438851401131911 contractive_loss:  1.7031997031508197\n",
      "model saved!\n",
      "====> Epoch: 70 Average loss: 4.3791  reconstruction loss:  4.378949393509943 contractive_loss:  1.6974770729248645\n",
      "====> Epoch: 71 Average loss: 4.3869  reconstruction loss:  4.386749062072116 contractive_loss:  1.7173031953755422\n",
      "model saved!\n",
      "====> Epoch: 72 Average loss: 4.3668  reconstruction loss:  4.366675107074451 contractive_loss:  1.7211057727437635\n",
      "====> Epoch: 73 Average loss: 4.3898  reconstruction loss:  4.389597250227273 contractive_loss:  1.7560906338347113\n",
      "====> Epoch: 74 Average loss: 4.4524  reconstruction loss:  4.452220454501064 contractive_loss:  1.7971240649907987\n",
      "====> Epoch: 75 Average loss: 4.3993  reconstruction loss:  4.399147060614776 contractive_loss:  1.7609695377451071\n",
      "====> Epoch: 76 Average loss: 4.4009  reconstruction loss:  4.4007439660314525 contractive_loss:  1.7910124465236743\n",
      "====> Epoch: 77 Average loss: 4.4169  reconstruction loss:  4.4167692356503245 contractive_loss:  1.8047105522680695\n",
      "model saved!\n",
      "====> Epoch: 78 Average loss: 4.3608  reconstruction loss:  4.360605909636359 contractive_loss:  1.8262805221015372\n",
      "====> Epoch: 79 Average loss: 4.4995  reconstruction loss:  4.49926660901255 contractive_loss:  1.9065658382005766\n",
      "====> Epoch: 80 Average loss: 5.1388  reconstruction loss:  5.138574482790366 contractive_loss:  1.9490269146319608\n",
      "====> Epoch: 81 Average loss: 5.3026  reconstruction loss:  5.3023841882913905 contractive_loss:  2.0933091636620986\n",
      "====> Epoch: 82 Average loss: 4.9826  reconstruction loss:  4.9823893820937615 contractive_loss:  1.8867470515832732\n",
      "====> Epoch: 83 Average loss: 4.4365  reconstruction loss:  4.436317222223972 contractive_loss:  1.9445966880363148\n",
      "====> Epoch: 84 Average loss: 4.3974  reconstruction loss:  4.397211037414511 contractive_loss:  1.8787961563172164\n",
      "====> Epoch: 85 Average loss: 4.3944  reconstruction loss:  4.394217787436893 contractive_loss:  1.8216316269616384\n",
      "model saved!\n",
      "====> Epoch: 86 Average loss: 4.3402  reconstruction loss:  4.339999118264021 contractive_loss:  1.8300953248621987\n",
      "====> Epoch: 87 Average loss: 4.3636  reconstruction loss:  4.363390633348655 contractive_loss:  1.8525568328148014\n",
      "====> Epoch: 88 Average loss: 4.3438  reconstruction loss:  4.34357112275224 contractive_loss:  1.8406732737905123\n",
      "====> Epoch: 89 Average loss: 4.3417  reconstruction loss:  4.341530655169233 contractive_loss:  1.891173906416918\n",
      "====> Epoch: 90 Average loss: 4.3621  reconstruction loss:  4.361956315659352 contractive_loss:  1.8794910282987785\n",
      "model saved!\n",
      "====> Epoch: 91 Average loss: 4.3218  reconstruction loss:  4.3215665461624155 contractive_loss:  1.8905057150673577\n",
      "====> Epoch: 92 Average loss: 4.3690  reconstruction loss:  4.368783721016043 contractive_loss:  1.8497726335437474\n",
      "====> Epoch: 93 Average loss: 4.3599  reconstruction loss:  4.35975373464315 contractive_loss:  1.8498207618956846\n",
      "====> Epoch: 94 Average loss: 4.3468  reconstruction loss:  4.346649142535303 contractive_loss:  1.8614950519614204\n",
      "====> Epoch: 95 Average loss: 4.3597  reconstruction loss:  4.359538845029027 contractive_loss:  1.8686685064913007\n",
      "====> Epoch: 96 Average loss: 4.3519  reconstruction loss:  4.351697812161529 contractive_loss:  1.9333010421324244\n",
      "====> Epoch: 97 Average loss: 4.3718  reconstruction loss:  4.371638912903349 contractive_loss:  2.0328051188740037\n",
      "====> Epoch: 98 Average loss: 4.4022  reconstruction loss:  4.401988123516307 contractive_loss:  2.1218280322297303\n",
      "model saved!\n",
      "====> Epoch: 99 Average loss: 4.3148  reconstruction loss:  4.314533997403029 contractive_loss:  2.180278555271063\n",
      "model saved!\n",
      "====> Epoch: 100 Average loss: 4.3034  reconstruction loss:  4.303152793003809 contractive_loss:  2.196958906964709\n",
      "====> Epoch: 101 Average loss: 4.3184  reconstruction loss:  4.318145997307182 contractive_loss:  2.357764467025295\n",
      "====> Epoch: 102 Average loss: 4.3137  reconstruction loss:  4.313500005593412 contractive_loss:  2.362562286835765\n",
      "====> Epoch: 103 Average loss: 4.3042  reconstruction loss:  4.304002871553648 contractive_loss:  2.4459752559037\n",
      "====> Epoch: 104 Average loss: 4.3420  reconstruction loss:  4.34171691956298 contractive_loss:  2.4338513522767236\n",
      "model saved!\n",
      "====> Epoch: 105 Average loss: 4.2985  reconstruction loss:  4.298221274774486 contractive_loss:  2.4527513271800396\n",
      "model saved!\n",
      "====> Epoch: 106 Average loss: 4.2983  reconstruction loss:  4.298081010088902 contractive_loss:  2.3971094402724367\n",
      "====> Epoch: 107 Average loss: 4.3370  reconstruction loss:  4.336733897939382 contractive_loss:  2.4444603615557368\n",
      "====> Epoch: 108 Average loss: 4.3187  reconstruction loss:  4.318454944314047 contractive_loss:  2.556898219207171\n",
      "model saved!\n",
      "====> Epoch: 109 Average loss: 4.2938  reconstruction loss:  4.293545713700134 contractive_loss:  2.6401559826356147\n",
      "====> Epoch: 110 Average loss: 4.3273  reconstruction loss:  4.327031983804902 contractive_loss:  2.6300272374139038\n",
      "====> Epoch: 111 Average loss: 4.2964  reconstruction loss:  4.296094832229157 contractive_loss:  2.681860585395998\n",
      "====> Epoch: 112 Average loss: 4.3395  reconstruction loss:  4.339234812584705 contractive_loss:  2.6614021254528435\n",
      "====> Epoch: 113 Average loss: 4.3348  reconstruction loss:  4.334583187489178 contractive_loss:  2.6554895358814976\n",
      "====> Epoch: 114 Average loss: 4.3167  reconstruction loss:  4.316459910425699 contractive_loss:  2.6814357443163224\n",
      "====> Epoch: 115 Average loss: 4.3222  reconstruction loss:  4.32194094924068 contractive_loss:  2.7006387630713937\n",
      "model saved!\n",
      "====> Epoch: 116 Average loss: 4.2723  reconstruction loss:  4.272017332291726 contractive_loss:  2.751687946553392\n",
      "====> Epoch: 117 Average loss: 4.2812  reconstruction loss:  4.280879116328567 contractive_loss:  2.730216969602984\n",
      "====> Epoch: 118 Average loss: 4.2959  reconstruction loss:  4.295651085146633 contractive_loss:  2.669661241279503\n",
      "====> Epoch: 119 Average loss: 4.2939  reconstruction loss:  4.293636902553149 contractive_loss:  2.7702917706479564\n",
      "model saved!\n",
      "====> Epoch: 120 Average loss: 4.2587  reconstruction loss:  4.258388971462604 contractive_loss:  2.797655644492896\n",
      "====> Epoch: 121 Average loss: 4.2970  reconstruction loss:  4.296773068647865 contractive_loss:  2.767314969993995\n",
      "model saved!\n",
      "====> Epoch: 122 Average loss: 4.2576  reconstruction loss:  4.257321921247735 contractive_loss:  2.845372294031863\n",
      "====> Epoch: 123 Average loss: 4.3010  reconstruction loss:  4.300722446844252 contractive_loss:  2.885544453699075\n",
      "model saved!\n",
      "====> Epoch: 124 Average loss: 4.2557  reconstruction loss:  4.255428536924555 contractive_loss:  2.811275792659847\n",
      "====> Epoch: 125 Average loss: 4.2681  reconstruction loss:  4.267861263562455 contractive_loss:  2.8602990171402607\n",
      "====> Epoch: 126 Average loss: 4.3256  reconstruction loss:  4.325338435429714 contractive_loss:  2.9041336639412108\n",
      "====> Epoch: 127 Average loss: 4.2931  reconstruction loss:  4.292837592939452 contractive_loss:  2.858122329099327\n",
      "====> Epoch: 128 Average loss: 4.2894  reconstruction loss:  4.289076152395657 contractive_loss:  2.8543527949069456\n",
      "====> Epoch: 129 Average loss: 4.2720  reconstruction loss:  4.271754867358148 contractive_loss:  2.867326376424085\n",
      "====> Epoch: 130 Average loss: 4.3459  reconstruction loss:  4.345585564309766 contractive_loss:  2.900108672109326\n",
      "====> Epoch: 131 Average loss: 4.2850  reconstruction loss:  4.284729203815899 contractive_loss:  2.892999720014769\n",
      "====> Epoch: 132 Average loss: 4.2731  reconstruction loss:  4.272851720380935 contractive_loss:  2.8647923513697364\n",
      "====> Epoch: 133 Average loss: 4.2737  reconstruction loss:  4.273427878336815 contractive_loss:  2.8700169566271208\n",
      "====> Epoch: 134 Average loss: 4.2630  reconstruction loss:  4.262738916166592 contractive_loss:  2.9094615282910063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 135 Average loss: 4.2677  reconstruction loss:  4.267455274982371 contractive_loss:  2.9304634267410288\n",
      "====> Epoch: 136 Average loss: 4.2757  reconstruction loss:  4.275361590296629 contractive_loss:  2.9276543325833524\n",
      "====> Epoch: 137 Average loss: 4.2628  reconstruction loss:  4.262509137560327 contractive_loss:  2.966932167078735\n",
      "====> Epoch: 138 Average loss: 4.2581  reconstruction loss:  4.257774210929474 contractive_loss:  2.966899559444813\n",
      "====> Epoch: 139 Average loss: 4.2673  reconstruction loss:  4.266970020633462 contractive_loss:  2.9893408753537947\n",
      "model saved!\n",
      "====> Epoch: 140 Average loss: 4.2508  reconstruction loss:  4.25045043871825 contractive_loss:  3.002253272720524\n",
      "====> Epoch: 141 Average loss: 4.2697  reconstruction loss:  4.269408557932996 contractive_loss:  2.9971408658476264\n",
      "====> Epoch: 142 Average loss: 4.2656  reconstruction loss:  4.265270055634443 contractive_loss:  3.0227776467718734\n",
      "model saved!\n",
      "====> Epoch: 143 Average loss: 4.2482  reconstruction loss:  4.2478680827574244 contractive_loss:  3.0210104346494155\n",
      "====> Epoch: 144 Average loss: 4.2630  reconstruction loss:  4.262702627041421 contractive_loss:  3.029358536490984\n",
      "====> Epoch: 145 Average loss: 4.2501  reconstruction loss:  4.249797646586279 contractive_loss:  3.07136396424278\n",
      "model saved!\n",
      "====> Epoch: 146 Average loss: 4.2469  reconstruction loss:  4.246549023667496 contractive_loss:  3.045170460155181\n",
      "====> Epoch: 147 Average loss: 4.2871  reconstruction loss:  4.28681842828208 contractive_loss:  3.0600989004220893\n",
      "====> Epoch: 148 Average loss: 4.2754  reconstruction loss:  4.275106790359961 contractive_loss:  3.0306887052606157\n",
      "model saved!\n",
      "====> Epoch: 149 Average loss: 4.2339  reconstruction loss:  4.233636856091881 contractive_loss:  3.072726797757705\n",
      "====> Epoch: 150 Average loss: 4.2886  reconstruction loss:  4.288322392235229 contractive_loss:  3.121989070218442\n",
      "model saved!\n",
      "====> Epoch: 151 Average loss: 4.2326  reconstruction loss:  4.232299847398891 contractive_loss:  3.1008976055963613\n",
      "====> Epoch: 152 Average loss: 4.2672  reconstruction loss:  4.26684184334285 contractive_loss:  3.100723110190604\n",
      "====> Epoch: 153 Average loss: 4.2519  reconstruction loss:  4.251619534634087 contractive_loss:  3.150970666824613\n",
      "====> Epoch: 154 Average loss: 4.2585  reconstruction loss:  4.258230711403021 contractive_loss:  3.093791818502579\n",
      "====> Epoch: 155 Average loss: 4.2697  reconstruction loss:  4.2693894094456075 contractive_loss:  3.0908768447574877\n",
      "====> Epoch: 156 Average loss: 4.2349  reconstruction loss:  4.234587407591475 contractive_loss:  3.1373620482882303\n",
      "====> Epoch: 157 Average loss: 4.2455  reconstruction loss:  4.2451842349373985 contractive_loss:  3.2286134653299174\n",
      "====> Epoch: 158 Average loss: 4.2481  reconstruction loss:  4.247767763946403 contractive_loss:  3.129115647740557\n",
      "====> Epoch: 159 Average loss: 4.2424  reconstruction loss:  4.2421304152994965 contractive_loss:  3.1607762949740743\n",
      "====> Epoch: 160 Average loss: 4.2516  reconstruction loss:  4.251340523293514 contractive_loss:  3.064478724958654\n",
      "====> Epoch: 161 Average loss: 4.2723  reconstruction loss:  4.271981076657634 contractive_loss:  3.164165397525426\n",
      "====> Epoch: 162 Average loss: 4.2517  reconstruction loss:  4.251367089395012 contractive_loss:  3.032086289384744\n",
      "====> Epoch: 163 Average loss: 4.2880  reconstruction loss:  4.287737905554399 contractive_loss:  2.8945536840807526\n",
      "====> Epoch: 164 Average loss: 4.2700  reconstruction loss:  4.269721795534501 contractive_loss:  2.97270279718505\n",
      "====> Epoch: 165 Average loss: 4.2571  reconstruction loss:  4.256844119230141 contractive_loss:  2.920410387851626\n",
      "====> Epoch: 166 Average loss: 4.2389  reconstruction loss:  4.2386543825415215 contractive_loss:  2.848622043071381\n",
      "====> Epoch: 167 Average loss: 4.2587  reconstruction loss:  4.258375384697931 contractive_loss:  2.8092396556953663\n",
      "====> Epoch: 168 Average loss: 4.2735  reconstruction loss:  4.2732062501293955 contractive_loss:  2.7932040589740477\n",
      "====> Epoch: 169 Average loss: 4.3029  reconstruction loss:  4.3026302883579515 contractive_loss:  2.7806140443208416\n",
      "====> Epoch: 170 Average loss: 4.3684  reconstruction loss:  4.368129288635498 contractive_loss:  2.6852889045526167\n",
      "====> Epoch: 171 Average loss: 4.3103  reconstruction loss:  4.310087048959265 contractive_loss:  2.5876814042269514\n",
      "====> Epoch: 172 Average loss: 4.2460  reconstruction loss:  4.245758692239473 contractive_loss:  2.5424324063221\n",
      "====> Epoch: 173 Average loss: 4.2455  reconstruction loss:  4.245270426684218 contractive_loss:  2.4679374233332467\n",
      "model saved!\n",
      "====> Epoch: 174 Average loss: 4.2288  reconstruction loss:  4.228539999043084 contractive_loss:  2.4541435950866033\n",
      "====> Epoch: 175 Average loss: 4.2421  reconstruction loss:  4.241811346506642 contractive_loss:  2.4083599820008623\n",
      "====> Epoch: 176 Average loss: 4.3098  reconstruction loss:  4.30954445720394 contractive_loss:  2.455905082840478\n",
      "====> Epoch: 177 Average loss: 4.2364  reconstruction loss:  4.2362106598201965 contractive_loss:  2.3562261528801685\n",
      "model saved!\n",
      "====> Epoch: 178 Average loss: 4.2076  reconstruction loss:  4.207410821320256 contractive_loss:  2.241933497720602\n",
      "model saved!\n",
      "====> Epoch: 179 Average loss: 4.1880  reconstruction loss:  4.187803986509013 contractive_loss:  2.2825873654312643\n",
      "model saved!\n",
      "====> Epoch: 180 Average loss: 4.1798  reconstruction loss:  4.179570860376506 contractive_loss:  2.21939770335762\n",
      "====> Epoch: 181 Average loss: 4.1885  reconstruction loss:  4.188259357945807 contractive_loss:  2.1607991121497454\n",
      "====> Epoch: 182 Average loss: 4.1865  reconstruction loss:  4.186256570055257 contractive_loss:  2.182604231109122\n",
      "====> Epoch: 183 Average loss: 4.2139  reconstruction loss:  4.2137217871494865 contractive_loss:  2.0447362856008033\n",
      "====> Epoch: 184 Average loss: 4.2701  reconstruction loss:  4.269909457920263 contractive_loss:  2.0946679885532564\n",
      "====> Epoch: 185 Average loss: 4.3157  reconstruction loss:  4.3155519749370015 contractive_loss:  1.8752622818314413\n",
      "====> Epoch: 186 Average loss: 4.2348  reconstruction loss:  4.234570608040753 contractive_loss:  1.825750443662806\n",
      "model saved!\n",
      "====> Epoch: 187 Average loss: 4.1272  reconstruction loss:  4.127047730293707 contractive_loss:  1.6843498795068133\n",
      "model saved!\n",
      "====> Epoch: 188 Average loss: 4.1153  reconstruction loss:  4.11513079100843 contractive_loss:  1.6744631266767491\n",
      "model saved!\n",
      "====> Epoch: 189 Average loss: 4.0711  reconstruction loss:  4.070923432094763 contractive_loss:  1.646841120231995\n",
      "====> Epoch: 190 Average loss: 4.1232  reconstruction loss:  4.123012143933251 contractive_loss:  1.652088851380937\n",
      "====> Epoch: 191 Average loss: 4.1027  reconstruction loss:  4.1025025061520894 contractive_loss:  1.522135168406788\n",
      "====> Epoch: 192 Average loss: 4.1565  reconstruction loss:  4.156316608454727 contractive_loss:  1.4832103205046734\n",
      "====> Epoch: 193 Average loss: 4.1289  reconstruction loss:  4.128742895515549 contractive_loss:  1.448709185088487\n",
      "====> Epoch: 194 Average loss: 4.1975  reconstruction loss:  4.197364981499587 contractive_loss:  1.3913533411765457\n",
      "====> Epoch: 195 Average loss: 4.2716  reconstruction loss:  4.271444155265665 contractive_loss:  1.4210988977575147\n",
      "====> Epoch: 196 Average loss: 4.3477  reconstruction loss:  4.347545237126235 contractive_loss:  1.4004943959238672\n",
      "model saved!\n",
      "====> Epoch: 197 Average loss: 4.0220  reconstruction loss:  4.021905390343817 contractive_loss:  1.2636826829445504\n",
      "model saved!\n",
      "====> Epoch: 198 Average loss: 3.9770  reconstruction loss:  3.9769165085783773 contractive_loss:  1.269284343060126\n",
      "====> Epoch: 199 Average loss: 4.0786  reconstruction loss:  4.078488375742711 contractive_loss:  1.2609399639312802\n",
      "====> Epoch: 200 Average loss: 3.9982  reconstruction loss:  3.998070747237131 contractive_loss:  1.2611343296331734\n",
      "====> Epoch: 201 Average loss: 4.1211  reconstruction loss:  4.1209573257234435 contractive_loss:  1.2861705305057334\n",
      "====> Epoch: 202 Average loss: 4.0155  reconstruction loss:  4.01542160882961 contractive_loss:  1.2616513684308721\n",
      "model saved!\n",
      "====> Epoch: 203 Average loss: 3.9191  reconstruction loss:  3.9189868230103473 contractive_loss:  1.249729802689943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 204 Average loss: 4.0501  reconstruction loss:  4.0499191201468205 contractive_loss:  1.3117514235975742\n",
      "====> Epoch: 205 Average loss: 3.9961  reconstruction loss:  3.9960053497974695 contractive_loss:  1.2340546345477061\n",
      "====> Epoch: 206 Average loss: 3.9964  reconstruction loss:  3.996234989559415 contractive_loss:  1.1678341501412384\n",
      "model saved!\n",
      "====> Epoch: 207 Average loss: 3.8744  reconstruction loss:  3.8743319709145947 contractive_loss:  1.1118023367959864\n",
      "====> Epoch: 208 Average loss: 3.9410  reconstruction loss:  3.9409279504617865 contractive_loss:  1.0827063244364803\n",
      "====> Epoch: 209 Average loss: 4.1405  reconstruction loss:  4.140348837624573 contractive_loss:  1.1337087139722668\n",
      "====> Epoch: 210 Average loss: 4.0845  reconstruction loss:  4.084348447259429 contractive_loss:  1.1281090488076737\n",
      "====> Epoch: 211 Average loss: 4.0101  reconstruction loss:  4.00994157207285 contractive_loss:  1.1917586669706346\n",
      "====> Epoch: 212 Average loss: 3.8898  reconstruction loss:  3.8896308895270204 contractive_loss:  1.286052680269985\n",
      "====> Epoch: 213 Average loss: 4.0169  reconstruction loss:  4.016752162844675 contractive_loss:  1.1401596120466608\n",
      "====> Epoch: 214 Average loss: 4.0273  reconstruction loss:  4.027214817720326 contractive_loss:  1.084970127222461\n",
      "====> Epoch: 215 Average loss: 3.8834  reconstruction loss:  3.8833302873997617 contractive_loss:  0.9072976857086257\n",
      "====> Epoch: 216 Average loss: 4.0844  reconstruction loss:  4.084331663485518 contractive_loss:  0.950492706943181\n",
      "====> Epoch: 217 Average loss: 3.9409  reconstruction loss:  3.9407561121023025 contractive_loss:  1.0070288967237442\n",
      "model saved!\n",
      "====> Epoch: 218 Average loss: 3.7537  reconstruction loss:  3.7536299723775493 contractive_loss:  1.0529206167454668\n",
      "model saved!\n",
      "====> Epoch: 219 Average loss: 3.7226  reconstruction loss:  3.722464651633347 contractive_loss:  1.0353045486985926\n",
      "model saved!\n",
      "====> Epoch: 220 Average loss: 3.6930  reconstruction loss:  3.6928993300055297 contractive_loss:  1.0334514319485595\n",
      "====> Epoch: 221 Average loss: 4.0929  reconstruction loss:  4.092759639280559 contractive_loss:  1.047257586295165\n",
      "====> Epoch: 222 Average loss: 3.7557  reconstruction loss:  3.755540894083619 contractive_loss:  1.1422126852249328\n",
      "====> Epoch: 223 Average loss: 3.7837  reconstruction loss:  3.7835987329482843 contractive_loss:  1.0577299031522056\n",
      "====> Epoch: 224 Average loss: 3.8838  reconstruction loss:  3.8837024338284687 contractive_loss:  1.0721956060832154\n",
      "====> Epoch: 225 Average loss: 3.7714  reconstruction loss:  3.77127200998068 contractive_loss:  0.9871907185713801\n",
      "====> Epoch: 226 Average loss: 4.2032  reconstruction loss:  4.203054173144476 contractive_loss:  0.9738506055562601\n",
      "====> Epoch: 227 Average loss: 3.7009  reconstruction loss:  3.700833480054708 contractive_loss:  0.9752680370127083\n",
      "====> Epoch: 228 Average loss: 4.0612  reconstruction loss:  4.06106692856992 contractive_loss:  0.8851063863359997\n",
      "====> Epoch: 229 Average loss: 6.3053  reconstruction loss:  6.305138933777339 contractive_loss:  1.2449282110653406\n",
      "====> Epoch: 230 Average loss: 4.4834  reconstruction loss:  4.4832434461000386 contractive_loss:  1.1360274617697979\n",
      "====> Epoch: 231 Average loss: 4.2094  reconstruction loss:  4.209350845913202 contractive_loss:  0.8516339278924068\n",
      "====> Epoch: 232 Average loss: 3.7209  reconstruction loss:  3.7208558283838498 contractive_loss:  0.809282284038237\n",
      "====> Epoch: 233 Average loss: 3.8778  reconstruction loss:  3.8777164945827685 contractive_loss:  0.9361399668928782\n",
      "====> Epoch: 234 Average loss: 3.8091  reconstruction loss:  3.809039897355413 contractive_loss:  0.9003759001756111\n",
      "model saved!\n",
      "====> Epoch: 235 Average loss: 3.6258  reconstruction loss:  3.6256691489625603 contractive_loss:  1.012368875670602\n",
      "model saved!\n",
      "====> Epoch: 236 Average loss: 3.3677  reconstruction loss:  3.367645439965788 contractive_loss:  1.0373792103334174\n",
      "====> Epoch: 237 Average loss: 5.2001  reconstruction loss:  5.200005134605343 contractive_loss:  1.3786920155254352\n",
      "====> Epoch: 238 Average loss: 4.0571  reconstruction loss:  4.057032760366749 contractive_loss:  0.9138811434224404\n",
      "====> Epoch: 239 Average loss: 4.7804  reconstruction loss:  4.780200888205197 contractive_loss:  1.5093825519500543\n",
      "====> Epoch: 240 Average loss: 3.7426  reconstruction loss:  3.742465444156108 contractive_loss:  1.2597248124156504\n",
      "====> Epoch: 241 Average loss: 3.6455  reconstruction loss:  3.6453724400397816 contractive_loss:  1.2003160580590468\n",
      "====> Epoch: 242 Average loss: 4.1220  reconstruction loss:  4.1218186367430265 contractive_loss:  1.3272275138428424\n",
      "====> Epoch: 243 Average loss: 3.7440  reconstruction loss:  3.7439240853408973 contractive_loss:  1.1980671410960066\n",
      "====> Epoch: 244 Average loss: 4.4974  reconstruction loss:  4.497200630047343 contractive_loss:  1.5638575237791266\n",
      "====> Epoch: 245 Average loss: 4.0428  reconstruction loss:  4.0426705288732 contractive_loss:  1.4868461224368696\n",
      "====> Epoch: 246 Average loss: 3.5931  reconstruction loss:  3.59290762988496 contractive_loss:  1.473717856910417\n",
      "model saved!\n",
      "====> Epoch: 247 Average loss: 3.3204  reconstruction loss:  3.3203048663558747 contractive_loss:  1.218729888067981\n",
      "====> Epoch: 248 Average loss: 6.2873  reconstruction loss:  6.287093012543533 contractive_loss:  1.7032895844365012\n",
      "====> Epoch: 249 Average loss: 4.6223  reconstruction loss:  4.622203768069879 contractive_loss:  1.4470636250898912\n",
      "====> Epoch: 250 Average loss: 4.7285  reconstruction loss:  4.728370777730284 contractive_loss:  1.5611158234924987\n",
      "====> Epoch: 251 Average loss: 3.7732  reconstruction loss:  3.773099636775332 contractive_loss:  1.040300358242041\n",
      "====> Epoch: 252 Average loss: 3.6784  reconstruction loss:  3.678315853468664 contractive_loss:  0.9705959612356476\n",
      "====> Epoch: 253 Average loss: 4.0551  reconstruction loss:  4.054931543747556 contractive_loss:  1.2994284358021215\n",
      "====> Epoch: 254 Average loss: 3.5558  reconstruction loss:  3.5557025897615455 contractive_loss:  1.3099573139783192\n",
      "====> Epoch: 255 Average loss: 4.2666  reconstruction loss:  4.266421449240845 contractive_loss:  1.8377174421650044\n",
      "====> Epoch: 256 Average loss: 4.1984  reconstruction loss:  4.19817588519901 contractive_loss:  1.8194319357948197\n",
      "====> Epoch: 257 Average loss: 3.3371  reconstruction loss:  3.3369854749824905 contractive_loss:  1.2272679889068565\n",
      "model saved!\n",
      "====> Epoch: 258 Average loss: 3.2000  reconstruction loss:  3.199898412165686 contractive_loss:  1.2795644894490708\n",
      "====> Epoch: 259 Average loss: 3.5941  reconstruction loss:  3.594022414138026 contractive_loss:  1.1316159977189206\n",
      "====> Epoch: 260 Average loss: 3.2453  reconstruction loss:  3.2451461858954853 contractive_loss:  1.2603671355816874\n",
      "model saved!\n",
      "====> Epoch: 261 Average loss: 3.0412  reconstruction loss:  3.041081788623812 contractive_loss:  1.2689339874059942\n",
      "====> Epoch: 262 Average loss: 3.5857  reconstruction loss:  3.5855971491768246 contractive_loss:  1.3816240198889396\n",
      "====> Epoch: 263 Average loss: 4.6370  reconstruction loss:  4.636792566832136 contractive_loss:  2.075610477455862\n",
      "====> Epoch: 264 Average loss: 4.7011  reconstruction loss:  4.700927523216378 contractive_loss:  1.5903695951352967\n",
      "====> Epoch: 265 Average loss: 3.2152  reconstruction loss:  3.215039888968518 contractive_loss:  1.2239549517326564\n",
      "====> Epoch: 266 Average loss: 4.5092  reconstruction loss:  4.509023671552579 contractive_loss:  1.8329032731208847\n",
      "====> Epoch: 267 Average loss: 3.4885  reconstruction loss:  3.4883802517300824 contractive_loss:  1.3631111624630223\n",
      "====> Epoch: 268 Average loss: 4.3630  reconstruction loss:  4.362847882098304 contractive_loss:  1.8313467444119953\n",
      "====> Epoch: 269 Average loss: 4.3063  reconstruction loss:  4.306068452951558 contractive_loss:  2.040060242026234\n",
      "====> Epoch: 270 Average loss: 3.4750  reconstruction loss:  3.474843468550369 contractive_loss:  1.2398401950355031\n",
      "====> Epoch: 271 Average loss: 4.2003  reconstruction loss:  4.200089308022412 contractive_loss:  1.9403095559869026\n",
      "====> Epoch: 272 Average loss: 5.1339  reconstruction loss:  5.13371811823859 contractive_loss:  2.192181755562805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 273 Average loss: 3.7378  reconstruction loss:  3.7375689525336395 contractive_loss:  1.9225244175577196\n",
      "====> Epoch: 274 Average loss: 4.5349  reconstruction loss:  4.534648815661101 contractive_loss:  2.554016790835226\n",
      "====> Epoch: 275 Average loss: 4.4630  reconstruction loss:  4.462690230436909 contractive_loss:  2.9921732571149087\n",
      "====> Epoch: 276 Average loss: 3.9799  reconstruction loss:  3.9795965364308006 contractive_loss:  2.706925928235097\n",
      "====> Epoch: 277 Average loss: 4.1330  reconstruction loss:  4.132864563746791 contractive_loss:  1.6807747367193415\n",
      "====> Epoch: 278 Average loss: 5.0319  reconstruction loss:  5.031672550016235 contractive_loss:  2.581614628840447\n",
      "====> Epoch: 279 Average loss: 4.0100  reconstruction loss:  4.009702856471197 contractive_loss:  2.639109156507587\n",
      "====> Epoch: 280 Average loss: 3.9185  reconstruction loss:  3.9182639990132926 contractive_loss:  2.6565022020382685\n",
      "====> Epoch: 281 Average loss: 3.9449  reconstruction loss:  3.944612799327134 contractive_loss:  2.8887039071407017\n",
      "====> Epoch: 282 Average loss: 3.5395  reconstruction loss:  3.5392652089044008 contractive_loss:  2.4279776865365577\n",
      "====> Epoch: 283 Average loss: 3.2159  reconstruction loss:  3.215703347386054 contractive_loss:  1.942770901768744\n",
      "====> Epoch: 284 Average loss: 3.3618  reconstruction loss:  3.361615444789211 contractive_loss:  1.410608665012685\n",
      "====> Epoch: 285 Average loss: 4.2377  reconstruction loss:  4.237499272907599 contractive_loss:  2.1368186421198367\n",
      "====> Epoch: 286 Average loss: 3.3641  reconstruction loss:  3.3639141607095104 contractive_loss:  1.3869176382941972\n",
      "====> Epoch: 287 Average loss: 3.1510  reconstruction loss:  3.150835721580224 contractive_loss:  1.7161236329928262\n",
      "====> Epoch: 288 Average loss: 3.4048  reconstruction loss:  3.404560177252954 contractive_loss:  2.2268770162089235\n",
      "====> Epoch: 289 Average loss: 3.3773  reconstruction loss:  3.377085545989118 contractive_loss:  2.0663369653169865\n",
      "====> Epoch: 290 Average loss: 3.8306  reconstruction loss:  3.830469125321356 contractive_loss:  1.2907084497843353\n",
      "====> Epoch: 291 Average loss: 3.1260  reconstruction loss:  3.125838141739996 contractive_loss:  1.128562940018507\n",
      "====> Epoch: 292 Average loss: 5.0149  reconstruction loss:  5.014792900919814 contractive_loss:  1.555272978312358\n",
      "====> Epoch: 293 Average loss: 4.0368  reconstruction loss:  4.036629760076014 contractive_loss:  1.9802283620250452\n",
      "====> Epoch: 294 Average loss: 3.5243  reconstruction loss:  3.5241616741277646 contractive_loss:  1.775548260585368\n",
      "====> Epoch: 295 Average loss: 3.1977  reconstruction loss:  3.197496126807695 contractive_loss:  1.6020669275618693\n",
      "====> Epoch: 296 Average loss: 3.7025  reconstruction loss:  3.702267921156211 contractive_loss:  2.12845057986427\n",
      "====> Epoch: 297 Average loss: 3.1091  reconstruction loss:  3.108963509186796 contractive_loss:  1.7582805938665218\n",
      "====> Epoch: 298 Average loss: 3.3649  reconstruction loss:  3.3647778289290518 contractive_loss:  1.2518777979558289\n",
      "====> Epoch: 299 Average loss: 4.1170  reconstruction loss:  4.116819102934888 contractive_loss:  2.2775941471032124\n",
      "====> Epoch: 300 Average loss: 3.5561  reconstruction loss:  3.5559652204872827 contractive_loss:  1.3132695576762619\n",
      "model saved!\n",
      "====> Epoch: 301 Average loss: 2.7337  reconstruction loss:  2.733593985609532 contractive_loss:  1.286812165872065\n",
      "====> Epoch: 302 Average loss: 3.7811  reconstruction loss:  3.780891095990482 contractive_loss:  2.151304593650156\n",
      "====> Epoch: 303 Average loss: 4.8921  reconstruction loss:  4.891916610772895 contractive_loss:  2.3240666071495624\n",
      "====> Epoch: 304 Average loss: 4.3706  reconstruction loss:  4.370350474521049 contractive_loss:  2.8657401826481697\n",
      "====> Epoch: 305 Average loss: 3.9218  reconstruction loss:  3.9214445395030224 contractive_loss:  3.266635473812496\n",
      "====> Epoch: 306 Average loss: 3.2247  reconstruction loss:  3.2244772463976985 contractive_loss:  1.8375448500195115\n",
      "====> Epoch: 307 Average loss: 3.8957  reconstruction loss:  3.8954766877782454 contractive_loss:  2.605973265345942\n",
      "====> Epoch: 308 Average loss: 4.5579  reconstruction loss:  4.5576202448438945 contractive_loss:  3.175327628774376\n",
      "====> Epoch: 309 Average loss: 3.9572  reconstruction loss:  3.956847705200191 contractive_loss:  3.3879360082862564\n",
      "====> Epoch: 310 Average loss: 3.2024  reconstruction loss:  3.2021290387721772 contractive_loss:  2.294558524830495\n",
      "====> Epoch: 311 Average loss: 4.2161  reconstruction loss:  4.215807025317509 contractive_loss:  3.1458862812076265\n",
      "====> Epoch: 312 Average loss: 3.9027  reconstruction loss:  3.90230139206594 contractive_loss:  3.5094375617166724\n",
      "====> Epoch: 313 Average loss: 3.2572  reconstruction loss:  3.2569718868050717 contractive_loss:  2.4407399215623014\n",
      "====> Epoch: 314 Average loss: 3.1042  reconstruction loss:  3.103867696281526 contractive_loss:  2.932734730817477\n",
      "====> Epoch: 315 Average loss: 2.9747  reconstruction loss:  2.9744099873918004 contractive_loss:  2.6971888665414223\n",
      "====> Epoch: 316 Average loss: 3.2627  reconstruction loss:  3.262450866225604 contractive_loss:  2.024699595624904\n",
      "====> Epoch: 317 Average loss: 3.2500  reconstruction loss:  3.2498321185282055 contractive_loss:  1.3577790857110261\n",
      "====> Epoch: 318 Average loss: 3.2177  reconstruction loss:  3.217492602583266 contractive_loss:  1.7787315180796812\n",
      "====> Epoch: 319 Average loss: 3.1379  reconstruction loss:  3.13771072384514 contractive_loss:  1.5434416849199837\n",
      "====> Epoch: 320 Average loss: 3.9339  reconstruction loss:  3.9336863073511097 contractive_loss:  2.1890334549810517\n",
      "====> Epoch: 321 Average loss: 5.1704  reconstruction loss:  5.170044358147147 contractive_loss:  3.481464383942321\n",
      "====> Epoch: 322 Average loss: 5.0645  reconstruction loss:  5.064336368369736 contractive_loss:  2.040019808379934\n",
      "====> Epoch: 323 Average loss: 3.5880  reconstruction loss:  3.587788054471418 contractive_loss:  1.8460773992758523\n",
      "====> Epoch: 324 Average loss: 3.8704  reconstruction loss:  3.8701684504686256 contractive_loss:  2.034085043027258\n",
      "====> Epoch: 325 Average loss: 5.3785  reconstruction loss:  5.378136312607508 contractive_loss:  3.9482356752573464\n",
      "====> Epoch: 326 Average loss: 6.8709  reconstruction loss:  6.870477183663071 contractive_loss:  4.009663665669582\n",
      "====> Epoch: 327 Average loss: 4.5386  reconstruction loss:  4.5381572919186866 contractive_loss:  4.600757801104418\n",
      "====> Epoch: 328 Average loss: 5.1498  reconstruction loss:  5.148933797463992 contractive_loss:  8.813401840708648\n",
      "====> Epoch: 329 Average loss: 6.0530  reconstruction loss:  6.052078794921496 contractive_loss:  9.001753736792553\n",
      "====> Epoch: 330 Average loss: 6.2140  reconstruction loss:  6.213244801127685 contractive_loss:  7.169262421294281\n",
      "====> Epoch: 331 Average loss: 5.8293  reconstruction loss:  5.828382052243509 contractive_loss:  8.737249046004605\n",
      "====> Epoch: 332 Average loss: 4.1895  reconstruction loss:  4.1886324113119855 contractive_loss:  8.852098356697365\n",
      "====> Epoch: 333 Average loss: 4.6472  reconstruction loss:  4.64625318303563 contractive_loss:  9.601485002875144\n",
      "====> Epoch: 334 Average loss: 4.4474  reconstruction loss:  4.446383586474121 contractive_loss:  10.054084768470162\n",
      "====> Epoch: 335 Average loss: 4.2830  reconstruction loss:  4.282072473673653 contractive_loss:  9.611591229011664\n",
      "====> Epoch: 336 Average loss: 3.9162  reconstruction loss:  3.9153969130106425 contractive_loss:  8.018832075481079\n",
      "====> Epoch: 337 Average loss: 3.5769  reconstruction loss:  3.576616167345679 contractive_loss:  3.240359033220057\n",
      "====> Epoch: 338 Average loss: 4.0506  reconstruction loss:  4.050291529954261 contractive_loss:  3.0784594358528836\n",
      "====> Epoch: 339 Average loss: 4.0714  reconstruction loss:  4.071035175123172 contractive_loss:  3.786002277944628\n",
      "====> Epoch: 340 Average loss: 3.8689  reconstruction loss:  3.868337807664999 contractive_loss:  5.2673520923918655\n",
      "====> Epoch: 341 Average loss: 3.7585  reconstruction loss:  3.75785750094974 contractive_loss:  5.9834964857945305\n",
      "====> Epoch: 342 Average loss: 3.1749  reconstruction loss:  3.1745380758280763 contractive_loss:  3.801295736045178\n",
      "====> Epoch: 343 Average loss: 4.1135  reconstruction loss:  4.113042023362584 contractive_loss:  4.432526831614755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 344 Average loss: 3.5441  reconstruction loss:  3.5435494114391632 contractive_loss:  5.338153529855853\n",
      "====> Epoch: 345 Average loss: 3.1951  reconstruction loss:  3.1946628921903244 contractive_loss:  4.2113914767867575\n",
      "====> Epoch: 346 Average loss: 4.0606  reconstruction loss:  4.060162563563321 contractive_loss:  4.718670130322061\n",
      "====> Epoch: 347 Average loss: 3.9379  reconstruction loss:  3.9372346572394497 contractive_loss:  6.168063571488899\n",
      "====> Epoch: 348 Average loss: 7.1301  reconstruction loss:  7.129516874580654 contractive_loss:  5.507833166610857\n",
      "====> Epoch: 349 Average loss: 7.2330  reconstruction loss:  7.2325051102024975 contractive_loss:  4.566902255280355\n",
      "====> Epoch: 350 Average loss: 5.8228  reconstruction loss:  5.822227043383712 contractive_loss:  5.94606600609216\n",
      "====> Epoch: 351 Average loss: 4.8865  reconstruction loss:  4.885604581047772 contractive_loss:  9.41991993291551\n",
      "====> Epoch: 352 Average loss: 6.7703  reconstruction loss:  6.769628832644266 contractive_loss:  6.530235314450534\n",
      "====> Epoch: 353 Average loss: 5.6684  reconstruction loss:  5.667648679921792 contractive_loss:  7.4838287640195045\n",
      "====> Epoch: 354 Average loss: 5.1972  reconstruction loss:  5.196342285147782 contractive_loss:  8.480909779658564\n",
      "====> Epoch: 355 Average loss: 4.8924  reconstruction loss:  4.891481052613058 contractive_loss:  8.902198849874809\n",
      "====> Epoch: 356 Average loss: 4.2647  reconstruction loss:  4.2639091796482385 contractive_loss:  7.62180793681296\n",
      "====> Epoch: 357 Average loss: 3.8222  reconstruction loss:  3.8216846565259215 contractive_loss:  4.669539250392009\n",
      "====> Epoch: 358 Average loss: 3.9631  reconstruction loss:  3.9625952079591356 contractive_loss:  4.677955979697026\n",
      "====> Epoch: 359 Average loss: 4.1642  reconstruction loss:  4.16345532922559 contractive_loss:  7.70468915089224\n",
      "====> Epoch: 360 Average loss: 3.7121  reconstruction loss:  3.71145582236716 contractive_loss:  6.593572850827888\n",
      "====> Epoch: 361 Average loss: 3.5146  reconstruction loss:  3.5140653469081333 contractive_loss:  5.691614005054931\n",
      "====> Epoch: 362 Average loss: 4.7481  reconstruction loss:  4.747911933236568 contractive_loss:  2.014774874051973\n",
      "====> Epoch: 363 Average loss: 3.9555  reconstruction loss:  3.955379152169893 contractive_loss:  0.7449626002812979\n",
      "====> Epoch: 364 Average loss: 3.3414  reconstruction loss:  3.3412635090138507 contractive_loss:  0.9569603372354687\n",
      "====> Epoch: 365 Average loss: 3.2504  reconstruction loss:  3.250301286028859 contractive_loss:  1.214456170451775\n",
      "====> Epoch: 366 Average loss: 3.0220  reconstruction loss:  3.0218137097275943 contractive_loss:  2.0453099561222094\n",
      "====> Epoch: 367 Average loss: 3.4112  reconstruction loss:  3.4110360175893146 contractive_loss:  1.9385245310719539\n",
      "====> Epoch: 368 Average loss: 4.3362  reconstruction loss:  4.3361237426152135 contractive_loss:  1.1765147448904545\n",
      "====> Epoch: 369 Average loss: 3.2714  reconstruction loss:  3.2712732540771765 contractive_loss:  0.8241686317200067\n",
      "====> Epoch: 370 Average loss: 3.3581  reconstruction loss:  3.3579914546378906 contractive_loss:  0.8910590462140016\n",
      "====> Epoch: 371 Average loss: 3.4710  reconstruction loss:  3.4708678313359314 contractive_loss:  1.2437552391986522\n",
      "====> Epoch: 372 Average loss: 3.2070  reconstruction loss:  3.206833366832545 contractive_loss:  1.6278747063846237\n",
      "====> Epoch: 373 Average loss: 3.4323  reconstruction loss:  3.4322532912785495 contractive_loss:  0.6906771472584\n",
      "====> Epoch: 374 Average loss: 3.3774  reconstruction loss:  3.3773805263899193 contractive_loss:  0.5807239040074716\n",
      "====> Epoch: 375 Average loss: 3.0685  reconstruction loss:  3.068447595119323 contractive_loss:  0.6535623300795519\n",
      "====> Epoch: 376 Average loss: 3.1893  reconstruction loss:  3.189214888347581 contractive_loss:  1.1510806291089644\n",
      "====> Epoch: 377 Average loss: 4.5611  reconstruction loss:  4.560891233523948 contractive_loss:  2.5107884509950193\n",
      "====> Epoch: 378 Average loss: 3.6195  reconstruction loss:  3.6192401480869414 contractive_loss:  2.889351608715026\n",
      "====> Epoch: 379 Average loss: 3.4449  reconstruction loss:  3.444732215869658 contractive_loss:  1.9973369781562131\n",
      "====> Epoch: 380 Average loss: 5.1229  reconstruction loss:  5.122643856183151 contractive_loss:  2.1679443061560035\n",
      "====> Epoch: 381 Average loss: 4.7737  reconstruction loss:  4.7736064055337115 contractive_loss:  1.4121181468292006\n",
      "====> Epoch: 382 Average loss: 4.0243  reconstruction loss:  4.024137463677209 contractive_loss:  2.0513119275257132\n",
      "====> Epoch: 383 Average loss: 4.1855  reconstruction loss:  4.185308237750402 contractive_loss:  1.90024052221896\n",
      "====> Epoch: 384 Average loss: 4.0298  reconstruction loss:  4.0292490990664716 contractive_loss:  5.81246856226095\n",
      "====> Epoch: 385 Average loss: 3.7018  reconstruction loss:  3.701213424768072 contractive_loss:  5.832717020390182\n",
      "====> Epoch: 386 Average loss: 4.1303  reconstruction loss:  4.130090729996515 contractive_loss:  1.6304188116608926\n",
      "====> Epoch: 387 Average loss: 3.7012  reconstruction loss:  3.701067578201594 contractive_loss:  1.6990798222073995\n",
      "====> Epoch: 388 Average loss: 3.6424  reconstruction loss:  3.6418178274528374 contractive_loss:  5.408697886527181\n",
      "====> Epoch: 389 Average loss: 4.3690  reconstruction loss:  4.368745147680316 contractive_loss:  2.17369043858481\n",
      "====> Epoch: 390 Average loss: 4.1778  reconstruction loss:  4.1774425096095715 contractive_loss:  3.149844769424508\n",
      "====> Epoch: 391 Average loss: 3.5199  reconstruction loss:  3.5196684629225565 contractive_loss:  2.2461706604110807\n",
      "====> Epoch: 392 Average loss: 3.6430  reconstruction loss:  3.642649346766898 contractive_loss:  3.424499075590675\n",
      "====> Epoch: 393 Average loss: 2.8587  reconstruction loss:  2.85851926776576 contractive_loss:  1.619494638183711\n",
      "====> Epoch: 394 Average loss: 3.4993  reconstruction loss:  3.4989956393061936 contractive_loss:  2.8356736569360623\n",
      "====> Epoch: 395 Average loss: 5.0829  reconstruction loss:  5.082633765083926 contractive_loss:  3.0164345404358173\n",
      "====> Epoch: 396 Average loss: 3.9125  reconstruction loss:  3.9123968921196046 contractive_loss:  1.0611274448734307\n",
      "====> Epoch: 397 Average loss: 3.9081  reconstruction loss:  3.907786147049537 contractive_loss:  2.728462008035684\n",
      "====> Epoch: 398 Average loss: 3.6361  reconstruction loss:  3.635606820505815 contractive_loss:  4.498459047228153\n",
      "====> Epoch: 399 Average loss: 3.4827  reconstruction loss:  3.482211043292531 contractive_loss:  4.671210807715507\n",
      "====> Epoch: 400 Average loss: 3.6386  reconstruction loss:  3.6381312993449773 contractive_loss:  4.571766615583617\n",
      "====> Epoch: 401 Average loss: 4.1309  reconstruction loss:  4.130436733017741 contractive_loss:  4.180582147751587\n",
      "====> Epoch: 402 Average loss: 3.8868  reconstruction loss:  3.8864170170645185 contractive_loss:  4.156290809894258\n",
      "====> Epoch: 403 Average loss: 3.3924  reconstruction loss:  3.391979685134107 contractive_loss:  4.48876557817459\n",
      "====> Epoch: 404 Average loss: 3.0440  reconstruction loss:  3.0436238406116143 contractive_loss:  3.4618565708888127\n",
      "====> Epoch: 405 Average loss: 4.0550  reconstruction loss:  4.054567460594772 contractive_loss:  3.8515806144723506\n",
      "====> Epoch: 406 Average loss: 4.4271  reconstruction loss:  4.4268595405221545 contractive_loss:  2.570641592796748\n",
      "====> Epoch: 407 Average loss: 3.3234  reconstruction loss:  3.3229653068297718 contractive_loss:  3.913442922452012\n",
      "====> Epoch: 408 Average loss: 3.3358  reconstruction loss:  3.3355057321995343 contractive_loss:  2.5159078964147685\n",
      "====> Epoch: 409 Average loss: 2.9801  reconstruction loss:  2.979925719401491 contractive_loss:  2.0432625599928076\n",
      "====> Epoch: 410 Average loss: 3.0392  reconstruction loss:  3.039028044801359 contractive_loss:  1.3921544914038595\n",
      "====> Epoch: 411 Average loss: 3.0408  reconstruction loss:  3.040638742414781 contractive_loss:  1.440432880763429\n",
      "====> Epoch: 412 Average loss: 3.0248  reconstruction loss:  3.024645190743668 contractive_loss:  1.239255136084622\n",
      "====> Epoch: 413 Average loss: 3.3322  reconstruction loss:  3.332103687783318 contractive_loss:  0.8230344724422677\n",
      "====> Epoch: 414 Average loss: 3.4082  reconstruction loss:  3.4081166110873315 contractive_loss:  0.7497191199492725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 415 Average loss: 3.3137  reconstruction loss:  3.313661035258585 contractive_loss:  0.6087710662057795\n",
      "====> Epoch: 416 Average loss: 3.7903  reconstruction loss:  3.7902083322016136 contractive_loss:  0.8397592985089634\n",
      "====> Epoch: 417 Average loss: 3.1118  reconstruction loss:  3.111750538629136 contractive_loss:  0.5558777586594256\n",
      "====> Epoch: 418 Average loss: 3.1866  reconstruction loss:  3.1865282773836294 contractive_loss:  0.8829130165087535\n",
      "====> Epoch: 419 Average loss: 2.8924  reconstruction loss:  2.892291427973565 contractive_loss:  1.1946869460830662\n",
      "====> Epoch: 420 Average loss: 3.0833  reconstruction loss:  3.083189966757202 contractive_loss:  1.1600693498820462\n",
      "====> Epoch: 421 Average loss: 4.1436  reconstruction loss:  4.14346015545988 contractive_loss:  1.0365351248480152\n",
      "====> Epoch: 422 Average loss: 4.0488  reconstruction loss:  4.048708038522397 contractive_loss:  0.47090058223979286\n",
      "====> Epoch: 423 Average loss: 3.6001  reconstruction loss:  3.600052295163804 contractive_loss:  0.8042749605988522\n",
      "====> Epoch: 424 Average loss: 3.7540  reconstruction loss:  3.7539305377681638 contractive_loss:  1.0533848834959398\n",
      "====> Epoch: 425 Average loss: 3.2579  reconstruction loss:  3.2578372626868504 contractive_loss:  1.0204375580280478\n",
      "model saved!\n",
      "====> Epoch: 426 Average loss: 2.6246  reconstruction loss:  2.6244176078183887 contractive_loss:  1.4815246448248425\n",
      "====> Epoch: 427 Average loss: 3.0572  reconstruction loss:  3.0569153676353795 contractive_loss:  2.454933438729599\n",
      "====> Epoch: 428 Average loss: 2.6481  reconstruction loss:  2.6480038790903553 contractive_loss:  1.3083723459584071\n",
      "model saved!\n",
      "====> Epoch: 429 Average loss: 2.5742  reconstruction loss:  2.574122554080184 contractive_loss:  1.1592004074942968\n",
      "====> Epoch: 430 Average loss: 3.7518  reconstruction loss:  3.751567734894942 contractive_loss:  2.6595801754179726\n",
      "====> Epoch: 431 Average loss: 4.4598  reconstruction loss:  4.459466288441636 contractive_loss:  2.859287268731442\n",
      "====> Epoch: 432 Average loss: 4.0871  reconstruction loss:  4.086931844410632 contractive_loss:  2.156655095771312\n",
      "====> Epoch: 433 Average loss: 3.6763  reconstruction loss:  3.6759779737923344 contractive_loss:  2.879003849612341\n",
      "====> Epoch: 434 Average loss: 3.2446  reconstruction loss:  3.244385967314961 contractive_loss:  1.8774033265282082\n",
      "====> Epoch: 435 Average loss: 2.8225  reconstruction loss:  2.822305662378119 contractive_loss:  1.9726072159984767\n",
      "====> Epoch: 436 Average loss: 3.4692  reconstruction loss:  3.469059195648632 contractive_loss:  1.202291438144851\n",
      "====> Epoch: 437 Average loss: 3.0572  reconstruction loss:  3.057099016642768 contractive_loss:  0.9296800480478213\n",
      "====> Epoch: 438 Average loss: 2.8228  reconstruction loss:  2.8227185783778026 contractive_loss:  1.0379621943383817\n",
      "====> Epoch: 439 Average loss: 4.4579  reconstruction loss:  4.457811218478588 contractive_loss:  0.9083797663988884\n",
      "====> Epoch: 440 Average loss: 3.0563  reconstruction loss:  3.0562085136207173 contractive_loss:  0.9047999426902399\n",
      "====> Epoch: 441 Average loss: 2.7075  reconstruction loss:  2.7073615358778254 contractive_loss:  1.1077299503132485\n",
      "====> Epoch: 442 Average loss: 2.7412  reconstruction loss:  2.7411260450063155 contractive_loss:  1.1869293780110344\n",
      "====> Epoch: 443 Average loss: 3.0084  reconstruction loss:  3.008327752371445 contractive_loss:  0.6409408575196893\n",
      "====> Epoch: 444 Average loss: 2.8260  reconstruction loss:  2.825991437639072 contractive_loss:  0.49321865445180585\n",
      "====> Epoch: 445 Average loss: 2.7808  reconstruction loss:  2.7807010805337713 contractive_loss:  0.6187920444513739\n",
      "====> Epoch: 446 Average loss: 4.2238  reconstruction loss:  4.223713239255245 contractive_loss:  0.7311016417883818\n",
      "====> Epoch: 447 Average loss: 3.2384  reconstruction loss:  3.2383887660755124 contractive_loss:  0.5752352720805192\n",
      "====> Epoch: 448 Average loss: 3.0379  reconstruction loss:  3.037889839515985 contractive_loss:  0.48677753431607645\n",
      "====> Epoch: 449 Average loss: 2.8205  reconstruction loss:  2.820468434941773 contractive_loss:  0.6852820189292984\n",
      "====> Epoch: 450 Average loss: 3.1861  reconstruction loss:  3.186030817961139 contractive_loss:  0.5356393859849415\n",
      "====> Epoch: 451 Average loss: 3.1331  reconstruction loss:  3.133019999069519 contractive_loss:  0.6820237892108241\n",
      "====> Epoch: 452 Average loss: 2.8913  reconstruction loss:  2.8912421027552027 contractive_loss:  0.5973629041266523\n",
      "====> Epoch: 453 Average loss: 2.7106  reconstruction loss:  2.710508153784959 contractive_loss:  0.7949858986827365\n",
      "====> Epoch: 454 Average loss: 3.4307  reconstruction loss:  3.4305077247236433 contractive_loss:  2.084671789483283\n",
      "====> Epoch: 455 Average loss: 2.8642  reconstruction loss:  2.864022448033607 contractive_loss:  1.7072086910763355\n",
      "====> Epoch: 456 Average loss: 3.5176  reconstruction loss:  3.517424827110583 contractive_loss:  2.1643562183580993\n",
      "====> Epoch: 457 Average loss: 3.3657  reconstruction loss:  3.3655197705924436 contractive_loss:  2.2464977633681795\n",
      "====> Epoch: 458 Average loss: 3.2600  reconstruction loss:  3.259738312265762 contractive_loss:  2.4060435949328314\n",
      "====> Epoch: 459 Average loss: 2.7636  reconstruction loss:  2.763524011643073 contractive_loss:  0.9875467135784293\n",
      "====> Epoch: 460 Average loss: 2.9739  reconstruction loss:  2.9737715240221143 contractive_loss:  1.088861030245395\n",
      "====> Epoch: 461 Average loss: 2.7234  reconstruction loss:  2.7232441701831567 contractive_loss:  1.124931786953897\n",
      "====> Epoch: 462 Average loss: 3.2382  reconstruction loss:  3.238126781588724 contractive_loss:  0.6334284265826003\n",
      "====> Epoch: 463 Average loss: 3.7240  reconstruction loss:  3.723972816321128 contractive_loss:  0.7212642490334547\n",
      "====> Epoch: 464 Average loss: 3.7499  reconstruction loss:  3.7498690725223685 contractive_loss:  0.619887932450157\n",
      "====> Epoch: 465 Average loss: 3.0231  reconstruction loss:  3.023045390921504 contractive_loss:  0.5110105680518799\n",
      "====> Epoch: 466 Average loss: 3.1731  reconstruction loss:  3.1729934144186194 contractive_loss:  0.6970584226796597\n",
      "====> Epoch: 467 Average loss: 3.4572  reconstruction loss:  3.457026177902069 contractive_loss:  1.9018264839674885\n",
      "====> Epoch: 468 Average loss: 4.3166  reconstruction loss:  4.316385362215065 contractive_loss:  1.8134238631378865\n",
      "====> Epoch: 469 Average loss: 3.1249  reconstruction loss:  3.1247890115131245 contractive_loss:  1.5208753536252033\n",
      "====> Epoch: 470 Average loss: 3.2873  reconstruction loss:  3.287213836318041 contractive_loss:  1.0589685549095602\n",
      "====> Epoch: 471 Average loss: 2.7898  reconstruction loss:  2.789689377242047 contractive_loss:  1.0696194711586398\n",
      "====> Epoch: 472 Average loss: 2.9060  reconstruction loss:  2.9058799452673334 contractive_loss:  0.7063964171568659\n",
      "====> Epoch: 473 Average loss: 2.9948  reconstruction loss:  2.9947103709094454 contractive_loss:  0.6452867277713937\n",
      "====> Epoch: 474 Average loss: 3.1677  reconstruction loss:  3.1676177895638413 contractive_loss:  0.6288041858343107\n",
      "====> Epoch: 475 Average loss: 2.9994  reconstruction loss:  2.999314684362649 contractive_loss:  0.5506458885824128\n",
      "====> Epoch: 476 Average loss: 3.2498  reconstruction loss:  3.2497396617873893 contractive_loss:  0.517601303814822\n",
      "====> Epoch: 477 Average loss: 4.3655  reconstruction loss:  4.365451155782926 contractive_loss:  0.4279555787267138\n",
      "====> Epoch: 478 Average loss: 4.8797  reconstruction loss:  4.879620184364428 contractive_loss:  0.5335209998001107\n",
      "====> Epoch: 479 Average loss: 4.0666  reconstruction loss:  4.0664594437825246 contractive_loss:  1.0682417734466867\n",
      "====> Epoch: 480 Average loss: 3.9527  reconstruction loss:  3.9525436695931035 contractive_loss:  1.3895107145094046\n",
      "====> Epoch: 481 Average loss: 3.5622  reconstruction loss:  3.5620236990124043 contractive_loss:  1.4349723954194835\n",
      "====> Epoch: 482 Average loss: 3.3341  reconstruction loss:  3.333775674342188 contractive_loss:  2.866797422776205\n",
      "====> Epoch: 483 Average loss: 3.9065  reconstruction loss:  3.906236387766997 contractive_loss:  2.6844744862480914\n",
      "====> Epoch: 484 Average loss: 4.3761  reconstruction loss:  4.3758239840376065 contractive_loss:  3.209928433867522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 485 Average loss: 3.8788  reconstruction loss:  3.8784508552076677 contractive_loss:  3.2837130320147767\n",
      "====> Epoch: 486 Average loss: 3.0459  reconstruction loss:  3.045666555749498 contractive_loss:  2.2752576882942197\n",
      "====> Epoch: 487 Average loss: 3.3541  reconstruction loss:  3.353842711300887 contractive_loss:  2.915111052558736\n",
      "model saved!\n",
      "====> Epoch: 488 Average loss: 2.5474  reconstruction loss:  2.5472846670134204 contractive_loss:  1.324181119297353\n",
      "====> Epoch: 489 Average loss: 3.0192  reconstruction loss:  3.0190851897462276 contractive_loss:  1.392524106746981\n",
      "====> Epoch: 490 Average loss: 3.4020  reconstruction loss:  3.401882257401729 contractive_loss:  0.8104365962182044\n",
      "====> Epoch: 491 Average loss: 2.8725  reconstruction loss:  2.8724489740095356 contractive_loss:  0.6946139335214363\n",
      "====> Epoch: 492 Average loss: 3.0931  reconstruction loss:  3.0929679607911793 contractive_loss:  0.9835283207645185\n",
      "====> Epoch: 493 Average loss: 2.7191  reconstruction loss:  2.719037247407189 contractive_loss:  1.0834105758659331\n",
      "====> Epoch: 494 Average loss: 2.9114  reconstruction loss:  2.9113577990170536 contractive_loss:  0.6530323990666127\n",
      "====> Epoch: 495 Average loss: 3.1089  reconstruction loss:  3.108800007321438 contractive_loss:  0.6822355682460095\n",
      "====> Epoch: 496 Average loss: 3.1419  reconstruction loss:  3.1418357080957366 contractive_loss:  0.42545734079856123\n",
      "====> Epoch: 497 Average loss: 2.8897  reconstruction loss:  2.889668792588261 contractive_loss:  0.575510139427224\n",
      "====> Epoch: 498 Average loss: 3.0154  reconstruction loss:  3.0153308788163677 contractive_loss:  0.6844282259447038\n",
      "====> Epoch: 499 Average loss: 2.9590  reconstruction loss:  2.95891240248395 contractive_loss:  0.8248035252634864\n",
      "====> Epoch: 500 Average loss: 3.0574  reconstruction loss:  3.057239635462285 contractive_loss:  1.840573626445172\n",
      "====> Epoch: 501 Average loss: 3.1616  reconstruction loss:  3.1614030633404435 contractive_loss:  1.5221240044040978\n",
      "====> Epoch: 502 Average loss: 3.2887  reconstruction loss:  3.2886081484257064 contractive_loss:  0.9576151323781851\n",
      "====> Epoch: 503 Average loss: 2.7971  reconstruction loss:  2.7969336470675676 contractive_loss:  1.6254602986102957\n",
      "====> Epoch: 504 Average loss: 3.0372  reconstruction loss:  3.0369143761373976 contractive_loss:  2.816442992849095\n",
      "====> Epoch: 505 Average loss: 3.7507  reconstruction loss:  3.7503852989055493 contractive_loss:  3.223292386331744\n",
      "====> Epoch: 506 Average loss: 3.6242  reconstruction loss:  3.6238660784328456 contractive_loss:  3.286791322894675\n",
      "====> Epoch: 507 Average loss: 3.4301  reconstruction loss:  3.4298005754972407 contractive_loss:  3.4142512051557343\n",
      "====> Epoch: 508 Average loss: 2.7010  reconstruction loss:  2.700748832935235 contractive_loss:  2.2580615593185187\n",
      "====> Epoch: 509 Average loss: 2.9195  reconstruction loss:  2.9193300905765303 contractive_loss:  1.3361765388609292\n",
      "====> Epoch: 510 Average loss: 4.4605  reconstruction loss:  4.460388959441581 contractive_loss:  1.1741102272441433\n",
      "====> Epoch: 511 Average loss: 3.0958  reconstruction loss:  3.095758256533159 contractive_loss:  0.5175398094680672\n",
      "====> Epoch: 512 Average loss: 2.9523  reconstruction loss:  2.952209127369965 contractive_loss:  0.7742085037796308\n",
      "====> Epoch: 513 Average loss: 2.9797  reconstruction loss:  2.979619103367995 contractive_loss:  1.066897649000426\n",
      "====> Epoch: 514 Average loss: 2.6509  reconstruction loss:  2.6508392787336197 contractive_loss:  0.9202726449174791\n",
      "====> Epoch: 515 Average loss: 2.9457  reconstruction loss:  2.9455289667454876 contractive_loss:  2.047550823156121\n",
      "====> Epoch: 516 Average loss: 2.5754  reconstruction loss:  2.5752694710551616 contractive_loss:  1.3152073287615118\n",
      "====> Epoch: 517 Average loss: 3.1235  reconstruction loss:  3.1234191165452274 contractive_loss:  0.8420161625144953\n",
      "====> Epoch: 518 Average loss: 2.8354  reconstruction loss:  2.8353084889241593 contractive_loss:  0.8731206344701983\n",
      "====> Epoch: 519 Average loss: 4.7505  reconstruction loss:  4.750343379502064 contractive_loss:  1.9622548618295848\n",
      "====> Epoch: 520 Average loss: 3.2388  reconstruction loss:  3.238580091407712 contractive_loss:  2.0503022986797697\n",
      "====> Epoch: 521 Average loss: 3.0631  reconstruction loss:  3.0628575505207247 contractive_loss:  2.2443077461299508\n",
      "====> Epoch: 522 Average loss: 3.2488  reconstruction loss:  3.2485579271192546 contractive_loss:  2.8478319200410107\n",
      "====> Epoch: 523 Average loss: 3.7303  reconstruction loss:  3.7299555651387224 contractive_loss:  2.9511621654339955\n",
      "====> Epoch: 524 Average loss: 3.0293  reconstruction loss:  3.0290243935761265 contractive_loss:  2.2840096824139446\n",
      "====> Epoch: 525 Average loss: 2.7726  reconstruction loss:  2.77247824574283 contractive_loss:  1.3280140455505465\n",
      "====> Epoch: 526 Average loss: 2.8794  reconstruction loss:  2.8793176126628817 contractive_loss:  1.105287016865216\n",
      "====> Epoch: 527 Average loss: 2.9610  reconstruction loss:  2.9609227844723525 contractive_loss:  0.7275391600616108\n",
      "====> Epoch: 528 Average loss: 2.7924  reconstruction loss:  2.7922501042074996 contractive_loss:  1.1900166402547274\n",
      "====> Epoch: 529 Average loss: 2.8085  reconstruction loss:  2.808352878861782 contractive_loss:  1.1953758389070785\n",
      "====> Epoch: 530 Average loss: 2.7507  reconstruction loss:  2.7505343219975464 contractive_loss:  1.1960475083346171\n",
      "model saved!\n",
      "====> Epoch: 531 Average loss: 2.4635  reconstruction loss:  2.463418078993999 contractive_loss:  1.0000578673343046\n",
      "====> Epoch: 532 Average loss: 2.7567  reconstruction loss:  2.7564809608682834 contractive_loss:  1.8784651377296935\n",
      "====> Epoch: 533 Average loss: 3.1940  reconstruction loss:  3.1937708341702615 contractive_loss:  2.578570701592877\n",
      "====> Epoch: 534 Average loss: 2.8940  reconstruction loss:  2.8937949509911234 contractive_loss:  2.1940965609481227\n",
      "====> Epoch: 535 Average loss: 2.7861  reconstruction loss:  2.7859034441491395 contractive_loss:  2.211501364976894\n",
      "====> Epoch: 536 Average loss: 2.9159  reconstruction loss:  2.9157434774859583 contractive_loss:  1.7613167557551679\n",
      "====> Epoch: 537 Average loss: 3.1036  reconstruction loss:  3.103389040127232 contractive_loss:  2.5933412041378374\n",
      "====> Epoch: 538 Average loss: 2.8852  reconstruction loss:  2.884992319446189 contractive_loss:  2.117190693456973\n",
      "====> Epoch: 539 Average loss: 2.6726  reconstruction loss:  2.6724811542920515 contractive_loss:  1.6728736066176133\n",
      "====> Epoch: 540 Average loss: 3.4046  reconstruction loss:  3.4045255832602384 contractive_loss:  0.9488056953203574\n",
      "====> Epoch: 541 Average loss: 2.9983  reconstruction loss:  2.99826691269756 contractive_loss:  0.7660766714368861\n",
      "====> Epoch: 542 Average loss: 2.6664  reconstruction loss:  2.666281753960631 contractive_loss:  1.0856928728970616\n",
      "====> Epoch: 543 Average loss: 2.9029  reconstruction loss:  2.9027938243049722 contractive_loss:  1.091187470515119\n",
      "====> Epoch: 544 Average loss: 3.0109  reconstruction loss:  3.010800785237399 contractive_loss:  0.6094264337642766\n",
      "====> Epoch: 545 Average loss: 2.9391  reconstruction loss:  2.9390260101924697 contractive_loss:  0.5884412440259059\n",
      "====> Epoch: 546 Average loss: 3.0771  reconstruction loss:  3.0770126116133896 contractive_loss:  0.7634397221065526\n",
      "====> Epoch: 547 Average loss: 2.7815  reconstruction loss:  2.7814474545947676 contractive_loss:  0.6621292039885003\n",
      "====> Epoch: 548 Average loss: 2.7431  reconstruction loss:  2.7430126424945307 contractive_loss:  0.6040937018259839\n",
      "====> Epoch: 549 Average loss: 2.9786  reconstruction loss:  2.9785288926543427 contractive_loss:  0.5174786062990467\n",
      "====> Epoch: 550 Average loss: 2.6894  reconstruction loss:  2.689305079687646 contractive_loss:  0.4682603575526586\n",
      "====> Epoch: 551 Average loss: 2.8490  reconstruction loss:  2.8489364414213547 contractive_loss:  0.6269184042716748\n",
      "====> Epoch: 552 Average loss: 3.6375  reconstruction loss:  3.637468536060719 contractive_loss:  0.7804042864008431\n",
      "====> Epoch: 553 Average loss: 3.0286  reconstruction loss:  3.0285491799994317 contractive_loss:  0.5253200713419579\n",
      "====> Epoch: 554 Average loss: 3.0493  reconstruction loss:  3.0492677818111606 contractive_loss:  0.702443737602519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 555 Average loss: 2.7012  reconstruction loss:  2.701076320463089 contractive_loss:  0.856807828946873\n",
      "====> Epoch: 556 Average loss: 2.9919  reconstruction loss:  2.9917909775927956 contractive_loss:  1.448965358488121\n",
      "====> Epoch: 557 Average loss: 2.7561  reconstruction loss:  2.7559872863456154 contractive_loss:  1.213931867761534\n",
      "====> Epoch: 558 Average loss: 3.1043  reconstruction loss:  3.1041924368401714 contractive_loss:  1.001675429841034\n",
      "====> Epoch: 559 Average loss: 2.8345  reconstruction loss:  2.83436432887665 contractive_loss:  0.8691882730427563\n",
      "====> Epoch: 560 Average loss: 2.5243  reconstruction loss:  2.5242140814631973 contractive_loss:  0.9752213849574729\n",
      "====> Epoch: 561 Average loss: 3.2085  reconstruction loss:  3.208306611233303 contractive_loss:  2.2838439826983157\n",
      "====> Epoch: 562 Average loss: 2.8500  reconstruction loss:  2.8497456711481526 contractive_loss:  2.2854432918448078\n",
      "====> Epoch: 563 Average loss: 2.7400  reconstruction loss:  2.7398524827462243 contractive_loss:  1.292270374281937\n",
      "====> Epoch: 564 Average loss: 2.7955  reconstruction loss:  2.795385004027791 contractive_loss:  1.1315744346864673\n",
      "====> Epoch: 565 Average loss: 4.7654  reconstruction loss:  4.765327652007916 contractive_loss:  0.6718523217151051\n",
      "====> Epoch: 566 Average loss: 4.2744  reconstruction loss:  4.2742900476256604 contractive_loss:  0.6098283277957179\n",
      "====> Epoch: 567 Average loss: 3.6018  reconstruction loss:  3.601695321042075 contractive_loss:  0.7638341628837694\n",
      "====> Epoch: 568 Average loss: 3.3043  reconstruction loss:  3.304218937415348 contractive_loss:  0.6933849147444491\n",
      "====> Epoch: 569 Average loss: 5.0036  reconstruction loss:  5.0034482585377065 contractive_loss:  2.0117177238029598\n",
      "====> Epoch: 570 Average loss: 4.2658  reconstruction loss:  4.265550896844567 contractive_loss:  2.1411044855521326\n",
      "====> Epoch: 571 Average loss: 2.7165  reconstruction loss:  2.7163989058193487 contractive_loss:  0.9796361637383294\n",
      "====> Epoch: 572 Average loss: 3.5934  reconstruction loss:  3.593321067900159 contractive_loss:  0.7433790091466906\n",
      "====> Epoch: 573 Average loss: 2.7711  reconstruction loss:  2.7710146750543445 contractive_loss:  0.9373252379883823\n",
      "====> Epoch: 574 Average loss: 2.5878  reconstruction loss:  2.5877361418573037 contractive_loss:  0.709549310537613\n",
      "====> Epoch: 575 Average loss: 2.7992  reconstruction loss:  2.7991210746778665 contractive_loss:  0.9276046988303571\n",
      "====> Epoch: 576 Average loss: 2.5898  reconstruction loss:  2.5897347673965068 contractive_loss:  0.7021161292189297\n",
      "====> Epoch: 577 Average loss: 2.5840  reconstruction loss:  2.583918594309333 contractive_loss:  0.828875920520846\n",
      "====> Epoch: 578 Average loss: 2.7627  reconstruction loss:  2.7625572630623556 contractive_loss:  1.0903785027275634\n",
      "====> Epoch: 579 Average loss: 2.6799  reconstruction loss:  2.6797066625582575 contractive_loss:  1.7120445168173841\n",
      "====> Epoch: 580 Average loss: 4.2635  reconstruction loss:  4.263260641975381 contractive_loss:  2.321318821464033\n",
      "====> Epoch: 581 Average loss: 4.3051  reconstruction loss:  4.304812593234052 contractive_loss:  2.5192733127968494\n",
      "====> Epoch: 582 Average loss: 3.0113  reconstruction loss:  3.0111102777025684 contractive_loss:  2.230292860451481\n",
      "====> Epoch: 583 Average loss: 3.3713  reconstruction loss:  3.371141885561047 contractive_loss:  1.3634027968919302\n",
      "====> Epoch: 584 Average loss: 2.6555  reconstruction loss:  2.6554283098483955 contractive_loss:  0.9678140584105728\n",
      "====> Epoch: 585 Average loss: 3.1167  reconstruction loss:  3.11657004214143 contractive_loss:  1.3096980844205635\n",
      "====> Epoch: 586 Average loss: 4.0578  reconstruction loss:  4.057679909273016 contractive_loss:  1.4894463432002136\n",
      "====> Epoch: 587 Average loss: 3.2786  reconstruction loss:  3.2784609367705584 contractive_loss:  1.7862367863419049\n",
      "====> Epoch: 588 Average loss: 3.3145  reconstruction loss:  3.3142959192118138 contractive_loss:  2.456204569097016\n",
      "====> Epoch: 589 Average loss: 3.3339  reconstruction loss:  3.3335823808047604 contractive_loss:  2.6811286540167374\n",
      "====> Epoch: 590 Average loss: 2.5648  reconstruction loss:  2.564610839585981 contractive_loss:  1.7917980530582949\n",
      "====> Epoch: 591 Average loss: 2.7939  reconstruction loss:  2.793735222525738 contractive_loss:  1.8859210762116543\n",
      "====> Epoch: 592 Average loss: 3.0691  reconstruction loss:  3.0688254698391138 contractive_loss:  2.683747213299244\n",
      "====> Epoch: 593 Average loss: 2.5930  reconstruction loss:  2.592775815596152 contractive_loss:  1.8567712927992108\n",
      "====> Epoch: 594 Average loss: 2.7416  reconstruction loss:  2.7413479792675326 contractive_loss:  2.87618401071599\n",
      "====> Epoch: 595 Average loss: 2.7950  reconstruction loss:  2.7948103012199463 contractive_loss:  1.861504440648994\n",
      "====> Epoch: 596 Average loss: 2.4661  reconstruction loss:  2.465953997036061 contractive_loss:  1.361892719386716\n",
      "====> Epoch: 597 Average loss: 3.3857  reconstruction loss:  3.385450282814422 contractive_loss:  2.5725947528230226\n",
      "====> Epoch: 598 Average loss: 3.7976  reconstruction loss:  3.7973102812911033 contractive_loss:  2.4764441984610017\n",
      "====> Epoch: 599 Average loss: 3.3114  reconstruction loss:  3.3111295808501175 contractive_loss:  2.7919420006296174\n",
      "====> Epoch: 600 Average loss: 3.1931  reconstruction loss:  3.192768045116493 contractive_loss:  3.0615584315624975\n",
      "====> Epoch: 601 Average loss: 2.4785  reconstruction loss:  2.478326672844 contractive_loss:  1.2384981704064042\n",
      "====> Epoch: 602 Average loss: 2.7669  reconstruction loss:  2.7667656688723157 contractive_loss:  1.513747589483062\n",
      "====> Epoch: 603 Average loss: 2.8359  reconstruction loss:  2.8357580437924956 contractive_loss:  1.0850505778203114\n",
      "====> Epoch: 604 Average loss: 2.5891  reconstruction loss:  2.5889670837414154 contractive_loss:  1.3078856896264532\n",
      "model saved!\n",
      "====> Epoch: 605 Average loss: 2.4619  reconstruction loss:  2.4617526565718895 contractive_loss:  1.130825803465523\n",
      "model saved!\n",
      "====> Epoch: 606 Average loss: 2.3780  reconstruction loss:  2.377931776164729 contractive_loss:  1.046635650936721\n",
      "====> Epoch: 607 Average loss: 2.7682  reconstruction loss:  2.768114324229769 contractive_loss:  1.246048179300018\n",
      "====> Epoch: 608 Average loss: 2.8035  reconstruction loss:  2.8033433980519487 contractive_loss:  1.398702601777858\n",
      "====> Epoch: 609 Average loss: 2.8638  reconstruction loss:  2.8635984126261267 contractive_loss:  2.40961942319916\n",
      "====> Epoch: 610 Average loss: 2.8340  reconstruction loss:  2.833786418950774 contractive_loss:  1.6906862830079104\n",
      "====> Epoch: 611 Average loss: 3.1862  reconstruction loss:  3.1859263732147896 contractive_loss:  3.0898925850575414\n",
      "====> Epoch: 612 Average loss: 3.6274  reconstruction loss:  3.6270909455225455 contractive_loss:  3.0990877180548164\n",
      "====> Epoch: 613 Average loss: 3.4667  reconstruction loss:  3.4663674272392915 contractive_loss:  3.5341786619171467\n",
      "====> Epoch: 614 Average loss: 3.4178  reconstruction loss:  3.4174223259526637 contractive_loss:  3.3508570041673615\n",
      "====> Epoch: 615 Average loss: 4.0696  reconstruction loss:  4.0692437626434295 contractive_loss:  3.6886813323280156\n",
      "====> Epoch: 616 Average loss: 3.4450  reconstruction loss:  3.444712434332814 contractive_loss:  3.2643260959092535\n",
      "====> Epoch: 617 Average loss: 3.1414  reconstruction loss:  3.1409998559750942 contractive_loss:  3.54727870946015\n",
      "====> Epoch: 618 Average loss: 2.7876  reconstruction loss:  2.787257579832919 contractive_loss:  3.301513063103853\n",
      "====> Epoch: 619 Average loss: 2.8114  reconstruction loss:  2.8111246369049674 contractive_loss:  2.399978753222773\n",
      "====> Epoch: 620 Average loss: 2.7438  reconstruction loss:  2.743707198125555 contractive_loss:  1.1316639905458936\n",
      "====> Epoch: 621 Average loss: 2.6898  reconstruction loss:  2.689611376400888 contractive_loss:  1.4825916503101468\n",
      "====> Epoch: 622 Average loss: 2.8179  reconstruction loss:  2.817808862758302 contractive_loss:  0.8116541341575522\n",
      "====> Epoch: 623 Average loss: 3.0215  reconstruction loss:  3.0213680079995555 contractive_loss:  1.3027635834803144\n",
      "====> Epoch: 624 Average loss: 2.6911  reconstruction loss:  2.69097270441869 contractive_loss:  1.1289402743087031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 625 Average loss: 2.7230  reconstruction loss:  2.7229437042743827 contractive_loss:  0.776622094436516\n",
      "====> Epoch: 626 Average loss: 2.6932  reconstruction loss:  2.6930694472555046 contractive_loss:  0.828392553021397\n",
      "====> Epoch: 627 Average loss: 3.3218  reconstruction loss:  3.3216784617781445 contractive_loss:  1.6553190119355523\n",
      "====> Epoch: 628 Average loss: 2.8822  reconstruction loss:  2.882046631237113 contractive_loss:  1.9373031304716728\n",
      "====> Epoch: 629 Average loss: 2.5259  reconstruction loss:  2.5257283256523637 contractive_loss:  1.276429004561436\n",
      "====> Epoch: 630 Average loss: 3.9301  reconstruction loss:  3.929711122578563 contractive_loss:  3.4598532056454676\n",
      "====> Epoch: 631 Average loss: 3.5325  reconstruction loss:  3.5321826610201583 contractive_loss:  3.212598151046209\n",
      "====> Epoch: 632 Average loss: 3.8458  reconstruction loss:  3.845444856616142 contractive_loss:  3.542287341669842\n",
      "====> Epoch: 633 Average loss: 3.5009  reconstruction loss:  3.5006142194713243 contractive_loss:  3.3312582499522017\n",
      "====> Epoch: 634 Average loss: 3.3424  reconstruction loss:  3.3420645446565334 contractive_loss:  3.3713234388378575\n",
      "====> Epoch: 635 Average loss: 2.8294  reconstruction loss:  2.8291284547905686 contractive_loss:  2.8780567927800433\n",
      "====> Epoch: 636 Average loss: 2.5207  reconstruction loss:  2.5204585977955607 contractive_loss:  2.498759767611462\n",
      "====> Epoch: 637 Average loss: 2.6911  reconstruction loss:  2.69080951464532 contractive_loss:  3.1123076223162616\n",
      "====> Epoch: 638 Average loss: 5.3754  reconstruction loss:  5.375184006127642 contractive_loss:  2.358668721105576\n",
      "====> Epoch: 639 Average loss: 4.7789  reconstruction loss:  4.778732393139083 contractive_loss:  1.4195788367375493\n",
      "====> Epoch: 640 Average loss: 3.7593  reconstruction loss:  3.759131784590677 contractive_loss:  1.1991303435332137\n",
      "====> Epoch: 641 Average loss: 3.4880  reconstruction loss:  3.487829368011381 contractive_loss:  1.4338035287882567\n",
      "====> Epoch: 642 Average loss: 3.5317  reconstruction loss:  3.531573818970661 contractive_loss:  1.6605668685652117\n",
      "====> Epoch: 643 Average loss: 3.7595  reconstruction loss:  3.759285449000907 contractive_loss:  1.9472933724220003\n",
      "====> Epoch: 644 Average loss: 3.5091  reconstruction loss:  3.5090092090173903 contractive_loss:  1.1220256151682924\n",
      "====> Epoch: 645 Average loss: 3.2945  reconstruction loss:  3.294100734419191 contractive_loss:  4.428195158759174\n",
      "====> Epoch: 646 Average loss: 3.2364  reconstruction loss:  3.2361350470878993 contractive_loss:  3.0028925385425826\n",
      "====> Epoch: 647 Average loss: 3.7690  reconstruction loss:  3.7687665990082126 contractive_loss:  2.5286587673445973\n",
      "====> Epoch: 648 Average loss: 3.9906  reconstruction loss:  3.9904467999559743 contractive_loss:  1.7295328902547102\n",
      "====> Epoch: 649 Average loss: 3.9412  reconstruction loss:  3.9410007212612617 contractive_loss:  2.1315364999578024\n",
      "====> Epoch: 650 Average loss: 3.9073  reconstruction loss:  3.9072134970555714 contractive_loss:  0.9704123213355658\n",
      "====> Epoch: 651 Average loss: 4.1083  reconstruction loss:  4.108170493327036 contractive_loss:  0.9866182910499887\n",
      "====> Epoch: 652 Average loss: 4.5917  reconstruction loss:  4.591488162291684 contractive_loss:  2.439570252437361\n",
      "====> Epoch: 653 Average loss: 4.0483  reconstruction loss:  4.048074730933931 contractive_loss:  2.453412716294893\n",
      "====> Epoch: 654 Average loss: 3.7440  reconstruction loss:  3.7437759505404973 contractive_loss:  2.084906092500125\n",
      "====> Epoch: 655 Average loss: 4.5146  reconstruction loss:  4.514321151153072 contractive_loss:  3.1157275201288375\n",
      "====> Epoch: 656 Average loss: 3.8442  reconstruction loss:  3.8439344980458374 contractive_loss:  2.4556367432909805\n",
      "====> Epoch: 657 Average loss: 3.5608  reconstruction loss:  3.560645035375182 contractive_loss:  1.702276672699027\n",
      "====> Epoch: 658 Average loss: 3.7163  reconstruction loss:  3.7158137810543637 contractive_loss:  4.549797543419617\n",
      "====> Epoch: 659 Average loss: 3.7798  reconstruction loss:  3.779387647056103 contractive_loss:  4.617632318230237\n",
      "====> Epoch: 660 Average loss: 3.9632  reconstruction loss:  3.962954253493666 contractive_loss:  2.125156341436407\n",
      "====> Epoch: 661 Average loss: 3.6964  reconstruction loss:  3.695907425092865 contractive_loss:  4.704508152850908\n",
      "====> Epoch: 662 Average loss: 3.5907  reconstruction loss:  3.590191744254314 contractive_loss:  4.86073321334748\n",
      "====> Epoch: 663 Average loss: 3.6956  reconstruction loss:  3.695279618953044 contractive_loss:  3.579485179189735\n",
      "====> Epoch: 664 Average loss: 3.6946  reconstruction loss:  3.6942131449440967 contractive_loss:  4.08084154398553\n",
      "====> Epoch: 665 Average loss: 3.4574  reconstruction loss:  3.4569448797214193 contractive_loss:  4.779676152559398\n",
      "====> Epoch: 666 Average loss: 3.7857  reconstruction loss:  3.7852937865079297 contractive_loss:  4.205512047744314\n",
      "====> Epoch: 667 Average loss: 3.9650  reconstruction loss:  3.9646772700066184 contractive_loss:  3.003460835822767\n",
      "====> Epoch: 668 Average loss: 3.7063  reconstruction loss:  3.7058837598964387 contractive_loss:  3.87134891306346\n",
      "====> Epoch: 669 Average loss: 3.8447  reconstruction loss:  3.844459532773628 contractive_loss:  2.837281566803939\n",
      "====> Epoch: 670 Average loss: 4.2025  reconstruction loss:  4.202300458091763 contractive_loss:  2.4016242532536114\n",
      "====> Epoch: 671 Average loss: 3.2064  reconstruction loss:  3.205971413178734 contractive_loss:  4.494574703023252\n",
      "====> Epoch: 672 Average loss: 3.3329  reconstruction loss:  3.332457542389666 contractive_loss:  4.894498528470889\n",
      "====> Epoch: 673 Average loss: 3.7733  reconstruction loss:  3.7731781059971348 contractive_loss:  1.1902849875602077\n",
      "====> Epoch: 674 Average loss: 3.9858  reconstruction loss:  3.985478245695599 contractive_loss:  2.891294196777955\n",
      "====> Epoch: 675 Average loss: 3.6185  reconstruction loss:  3.618057961573983 contractive_loss:  3.9636463328034206\n",
      "====> Epoch: 676 Average loss: 3.3433  reconstruction loss:  3.342877059562056 contractive_loss:  4.34601363940734\n",
      "====> Epoch: 677 Average loss: 2.9843  reconstruction loss:  2.983843216113384 contractive_loss:  4.371082278392394\n",
      "====> Epoch: 678 Average loss: 2.7737  reconstruction loss:  2.7733699394047817 contractive_loss:  3.098589458399279\n",
      "====> Epoch: 679 Average loss: 2.9503  reconstruction loss:  2.949915474797255 contractive_loss:  3.851632207187218\n",
      "====> Epoch: 680 Average loss: 2.6772  reconstruction loss:  2.676830040968605 contractive_loss:  3.814877432333296\n",
      "====> Epoch: 681 Average loss: 3.3144  reconstruction loss:  3.314021687147409 contractive_loss:  3.7619764980495227\n",
      "====> Epoch: 682 Average loss: 3.1885  reconstruction loss:  3.188183022849119 contractive_loss:  3.5489577822719682\n",
      "====> Epoch: 683 Average loss: 3.8382  reconstruction loss:  3.8378184766283163 contractive_loss:  3.8461305490862654\n",
      "====> Epoch: 684 Average loss: 3.2637  reconstruction loss:  3.2633123816534306 contractive_loss:  3.8774620693150332\n",
      "====> Epoch: 685 Average loss: 3.1428  reconstruction loss:  3.1423611829981177 contractive_loss:  4.138592431458738\n",
      "====> Epoch: 686 Average loss: 2.9447  reconstruction loss:  2.94435741593875 contractive_loss:  2.972752044916714\n",
      "====> Epoch: 687 Average loss: 3.2748  reconstruction loss:  3.2744678262763722 contractive_loss:  3.2268592625980173\n",
      "====> Epoch: 688 Average loss: 3.5448  reconstruction loss:  3.544428465631906 contractive_loss:  3.7642924421922115\n",
      "====> Epoch: 689 Average loss: 3.4996  reconstruction loss:  3.499237197773549 contractive_loss:  3.621754689171768\n",
      "====> Epoch: 690 Average loss: 3.4564  reconstruction loss:  3.4560145838989285 contractive_loss:  3.6472329650902715\n",
      "====> Epoch: 691 Average loss: 2.7925  reconstruction loss:  2.7921033476489834 contractive_loss:  3.5944504151382812\n",
      "====> Epoch: 692 Average loss: 2.6971  reconstruction loss:  2.696835163999775 contractive_loss:  2.599354881637864\n",
      "====> Epoch: 693 Average loss: 3.5030  reconstruction loss:  3.5026426140298086 contractive_loss:  3.7492709817072347\n",
      "====> Epoch: 694 Average loss: 3.4542  reconstruction loss:  3.4537244248411123 contractive_loss:  4.3474387544285\n",
      "====> Epoch: 695 Average loss: 3.2622  reconstruction loss:  3.261810636556942 contractive_loss:  3.5227176811148637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 696 Average loss: 3.2151  reconstruction loss:  3.2147362232121424 contractive_loss:  3.2695571262729435\n",
      "====> Epoch: 697 Average loss: 3.3753  reconstruction loss:  3.3749810124203985 contractive_loss:  3.4189501481430202\n",
      "====> Epoch: 698 Average loss: 3.2196  reconstruction loss:  3.2194734702048904 contractive_loss:  1.6639403179606755\n",
      "====> Epoch: 699 Average loss: 2.9060  reconstruction loss:  2.905936081369802 contractive_loss:  1.014288728349956\n",
      "====> Epoch: 700 Average loss: 2.7119  reconstruction loss:  2.711724295118705 contractive_loss:  1.3926634526815236\n",
      "====> Epoch: 701 Average loss: 2.8611  reconstruction loss:  2.860951585996362 contractive_loss:  1.7072289021259452\n",
      "model saved!\n",
      "====> Epoch: 702 Average loss: 2.3744  reconstruction loss:  2.374224953877841 contractive_loss:  1.3379330476904916\n",
      "====> Epoch: 703 Average loss: 2.7297  reconstruction loss:  2.729397652080898 contractive_loss:  3.062532983842871\n",
      "====> Epoch: 704 Average loss: 4.0989  reconstruction loss:  4.098526969116921 contractive_loss:  3.3867130963266834\n",
      "====> Epoch: 705 Average loss: 4.2775  reconstruction loss:  4.277272245173947 contractive_loss:  2.2629024202745383\n",
      "====> Epoch: 706 Average loss: 3.2806  reconstruction loss:  3.2802460270806106 contractive_loss:  3.65174800611031\n",
      "====> Epoch: 707 Average loss: 3.2493  reconstruction loss:  3.2489763425564115 contractive_loss:  3.5824157376743604\n",
      "====> Epoch: 708 Average loss: 2.9439  reconstruction loss:  2.943537628433583 contractive_loss:  3.9132406199099643\n",
      "====> Epoch: 709 Average loss: 3.4001  reconstruction loss:  3.3997043348431957 contractive_loss:  4.1669428803391595\n",
      "====> Epoch: 710 Average loss: 2.4423  reconstruction loss:  2.4421440244351986 contractive_loss:  1.6041463614995604\n",
      "====> Epoch: 711 Average loss: 2.7610  reconstruction loss:  2.7606347541238994 contractive_loss:  3.2180805600745925\n",
      "====> Epoch: 712 Average loss: 2.6011  reconstruction loss:  2.6007372167581524 contractive_loss:  3.6355128025029604\n",
      "====> Epoch: 713 Average loss: 3.2757  reconstruction loss:  3.2753375537879488 contractive_loss:  3.4378252080276908\n",
      "====> Epoch: 714 Average loss: 3.5740  reconstruction loss:  3.573600039591438 contractive_loss:  3.9192695803355377\n",
      "====> Epoch: 715 Average loss: 3.2175  reconstruction loss:  3.2171496345341115 contractive_loss:  3.498040481274641\n",
      "====> Epoch: 716 Average loss: 2.8780  reconstruction loss:  2.8776679115430674 contractive_loss:  3.5625976719457504\n",
      "====> Epoch: 717 Average loss: 2.9882  reconstruction loss:  2.987824198873807 contractive_loss:  3.4628235891227104\n",
      "====> Epoch: 718 Average loss: 3.0178  reconstruction loss:  3.0174210428160793 contractive_loss:  3.7252496371232637\n",
      "====> Epoch: 719 Average loss: 2.9971  reconstruction loss:  2.9967483818130294 contractive_loss:  3.799064705762507\n",
      "====> Epoch: 720 Average loss: 2.6676  reconstruction loss:  2.6672430105044542 contractive_loss:  3.1032195367807462\n",
      "====> Epoch: 721 Average loss: 2.6033  reconstruction loss:  2.6029889753589153 contractive_loss:  3.1830397086245017\n",
      "====> Epoch: 722 Average loss: 2.9207  reconstruction loss:  2.920370520764954 contractive_loss:  3.639654681432107\n",
      "====> Epoch: 723 Average loss: 2.9215  reconstruction loss:  2.921111425729656 contractive_loss:  3.635504570637235\n",
      "====> Epoch: 724 Average loss: 3.3245  reconstruction loss:  3.3241089018102183 contractive_loss:  3.426846059633704\n",
      "====> Epoch: 725 Average loss: 2.5481  reconstruction loss:  2.5478601293047456 contractive_loss:  2.134385872524182\n",
      "====> Epoch: 726 Average loss: 2.8464  reconstruction loss:  2.846228319434226 contractive_loss:  1.7624140145209402\n",
      "====> Epoch: 727 Average loss: 2.7318  reconstruction loss:  2.7317112931169523 contractive_loss:  1.2668522915257694\n",
      "====> Epoch: 728 Average loss: 2.8346  reconstruction loss:  2.834520267459808 contractive_loss:  1.063383861799205\n",
      "====> Epoch: 729 Average loss: 2.6118  reconstruction loss:  2.6116352866294674 contractive_loss:  1.3579006153923017\n",
      "====> Epoch: 730 Average loss: 3.5970  reconstruction loss:  3.5968727019918547 contractive_loss:  1.1412602298223915\n",
      "====> Epoch: 731 Average loss: 2.9754  reconstruction loss:  2.9753119392190057 contractive_loss:  0.5683037781193554\n",
      "====> Epoch: 732 Average loss: 2.7748  reconstruction loss:  2.774701572291838 contractive_loss:  0.6221888956100607\n",
      "====> Epoch: 733 Average loss: 2.6756  reconstruction loss:  2.6754885764192124 contractive_loss:  0.6261349914511214\n",
      "====> Epoch: 734 Average loss: 2.4276  reconstruction loss:  2.4275663688313154 contractive_loss:  0.8290913296939872\n",
      "====> Epoch: 735 Average loss: 3.0566  reconstruction loss:  3.0565488669439054 contractive_loss:  0.7356803378940914\n",
      "====> Epoch: 736 Average loss: 2.4808  reconstruction loss:  2.4806971702135003 contractive_loss:  1.2264926934815996\n",
      "====> Epoch: 737 Average loss: 2.6719  reconstruction loss:  2.6717605687825845 contractive_loss:  1.4106172875347684\n",
      "====> Epoch: 738 Average loss: 2.6671  reconstruction loss:  2.6670459632867165 contractive_loss:  1.0015370446675715\n",
      "====> Epoch: 739 Average loss: 2.8995  reconstruction loss:  2.8994048344470404 contractive_loss:  0.7123399595808573\n",
      "====> Epoch: 740 Average loss: 2.7224  reconstruction loss:  2.7223665925134783 contractive_loss:  0.6156667459556894\n",
      "====> Epoch: 741 Average loss: 2.6981  reconstruction loss:  2.698043950362297 contractive_loss:  0.628180105020097\n",
      "====> Epoch: 742 Average loss: 2.7102  reconstruction loss:  2.7101420066385202 contractive_loss:  0.6632737297503142\n",
      "====> Epoch: 743 Average loss: 2.4862  reconstruction loss:  2.486104359245877 contractive_loss:  0.9810691832242066\n",
      "====> Epoch: 744 Average loss: 2.4649  reconstruction loss:  2.464772815096819 contractive_loss:  1.0467073126362973\n",
      "====> Epoch: 745 Average loss: 2.4847  reconstruction loss:  2.484645697787398 contractive_loss:  0.8955488300647466\n",
      "====> Epoch: 746 Average loss: 2.5765  reconstruction loss:  2.576380582561909 contractive_loss:  1.1436975455960556\n",
      "====> Epoch: 747 Average loss: 3.0874  reconstruction loss:  3.087175944048482 contractive_loss:  2.6662347608953763\n",
      "====> Epoch: 748 Average loss: 3.0399  reconstruction loss:  3.0396540561523566 contractive_loss:  2.6366471051958817\n",
      "====> Epoch: 749 Average loss: 2.8176  reconstruction loss:  2.8172954231091807 contractive_loss:  2.5740562107349803\n",
      "model saved!\n",
      "====> Epoch: 750 Average loss: 2.3161  reconstruction loss:  2.315959528660544 contractive_loss:  1.1702812120155641\n",
      "====> Epoch: 751 Average loss: 2.4689  reconstruction loss:  2.4687656701055505 contractive_loss:  0.9661437637583543\n",
      "====> Epoch: 752 Average loss: 2.8285  reconstruction loss:  2.8283767533511663 contractive_loss:  0.9676153564973625\n",
      "====> Epoch: 753 Average loss: 2.4736  reconstruction loss:  2.4734500704162805 contractive_loss:  1.1299072704806012\n",
      "====> Epoch: 754 Average loss: 2.4422  reconstruction loss:  2.442089407106336 contractive_loss:  1.1789684074680378\n",
      "====> Epoch: 755 Average loss: 2.6406  reconstruction loss:  2.6404690877542616 contractive_loss:  1.265870300174162\n",
      "model saved!\n",
      "====> Epoch: 756 Average loss: 2.2753  reconstruction loss:  2.275151013671586 contractive_loss:  1.0398083859159726\n",
      "====> Epoch: 757 Average loss: 2.2771  reconstruction loss:  2.2770089382489576 contractive_loss:  1.1738038386455125\n",
      "====> Epoch: 758 Average loss: 2.6053  reconstruction loss:  2.605121184232782 contractive_loss:  2.0306453415053074\n",
      "model saved!\n",
      "====> Epoch: 759 Average loss: 2.2688  reconstruction loss:  2.268706270549637 contractive_loss:  1.0341098476380417\n",
      "model saved!\n",
      "====> Epoch: 760 Average loss: 2.2060  reconstruction loss:  2.2058148234064703 contractive_loss:  1.5634800717125612\n",
      "====> Epoch: 761 Average loss: 2.5693  reconstruction loss:  2.569102188923813 contractive_loss:  2.4015619548832476\n",
      "====> Epoch: 762 Average loss: 2.5298  reconstruction loss:  2.5296347692008925 contractive_loss:  2.025615618537521\n",
      "====> Epoch: 763 Average loss: 2.5739  reconstruction loss:  2.573762539946403 contractive_loss:  1.0938582978977125\n",
      "====> Epoch: 764 Average loss: 2.9260  reconstruction loss:  2.9259640036624845 contractive_loss:  0.7926290109878057\n",
      "====> Epoch: 765 Average loss: 2.7442  reconstruction loss:  2.7441411539098244 contractive_loss:  0.7269279258019392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 766 Average loss: 2.7772  reconstruction loss:  2.7771115675471867 contractive_loss:  0.7725903179436899\n",
      "====> Epoch: 767 Average loss: 2.8009  reconstruction loss:  2.800825983686367 contractive_loss:  1.0970683962897916\n",
      "====> Epoch: 768 Average loss: 3.1353  reconstruction loss:  3.135166873168488 contractive_loss:  1.179934026341423\n",
      "====> Epoch: 769 Average loss: 2.8351  reconstruction loss:  2.834946736321094 contractive_loss:  1.6486011947500674\n",
      "====> Epoch: 770 Average loss: 3.2836  reconstruction loss:  3.2833437456413246 contractive_loss:  2.5376101717639323\n",
      "====> Epoch: 771 Average loss: 5.9135  reconstruction loss:  5.913420001636974 contractive_loss:  1.1680654065434974\n",
      "====> Epoch: 772 Average loss: 7.4378  reconstruction loss:  7.437404510538783 contractive_loss:  4.20767254054165\n",
      "====> Epoch: 773 Average loss: 6.1638  reconstruction loss:  6.163451602113021 contractive_loss:  3.9789900938328624\n",
      "====> Epoch: 774 Average loss: 5.1589  reconstruction loss:  5.158508956965541 contractive_loss:  3.686842334753722\n",
      "====> Epoch: 775 Average loss: 3.9691  reconstruction loss:  3.968956316026499 contractive_loss:  1.6816314018407743\n",
      "====> Epoch: 776 Average loss: 3.5897  reconstruction loss:  3.5895231222284805 contractive_loss:  1.8477433087025896\n",
      "====> Epoch: 777 Average loss: 3.5479  reconstruction loss:  3.547784820761888 contractive_loss:  0.7280486305206642\n",
      "====> Epoch: 778 Average loss: 3.7923  reconstruction loss:  3.7922658003573617 contractive_loss:  0.7701475325684042\n",
      "====> Epoch: 779 Average loss: 3.6729  reconstruction loss:  3.6727167746031655 contractive_loss:  1.6508113705123855\n",
      "====> Epoch: 780 Average loss: 4.0466  reconstruction loss:  4.046315171716873 contractive_loss:  2.8094093937430107\n",
      "====> Epoch: 781 Average loss: 3.9212  reconstruction loss:  3.9209945175766223 contractive_loss:  2.533571527834186\n",
      "====> Epoch: 782 Average loss: 3.8531  reconstruction loss:  3.852949842349415 contractive_loss:  1.0939442681012077\n",
      "====> Epoch: 783 Average loss: 3.9190  reconstruction loss:  3.9188317196208824 contractive_loss:  2.160784598490619\n",
      "====> Epoch: 784 Average loss: 5.3857  reconstruction loss:  5.385319921319217 contractive_loss:  3.8496578125029517\n",
      "====> Epoch: 785 Average loss: 4.8765  reconstruction loss:  4.87606559282845 contractive_loss:  3.9940616702717575\n",
      "====> Epoch: 786 Average loss: 5.1667  reconstruction loss:  5.166238511070571 contractive_loss:  4.136278272594005\n",
      "====> Epoch: 787 Average loss: 3.8375  reconstruction loss:  3.8371879690376214 contractive_loss:  3.1528310335454552\n",
      "====> Epoch: 788 Average loss: 3.4308  reconstruction loss:  3.430690911161049 contractive_loss:  1.103387569877456\n",
      "====> Epoch: 789 Average loss: 3.7078  reconstruction loss:  3.7076812048326606 contractive_loss:  1.489268211333977\n",
      "====> Epoch: 790 Average loss: 3.6759  reconstruction loss:  3.6756827679269612 contractive_loss:  1.7154344275954825\n",
      "====> Epoch: 791 Average loss: 3.7204  reconstruction loss:  3.72019825082941 contractive_loss:  2.480299345515589\n",
      "====> Epoch: 792 Average loss: 3.6912  reconstruction loss:  3.6910934716377626 contractive_loss:  1.331431386771443\n",
      "====> Epoch: 793 Average loss: 3.4419  reconstruction loss:  3.4418385754432608 contractive_loss:  0.9339822788323553\n",
      "====> Epoch: 794 Average loss: 3.5729  reconstruction loss:  3.5727393621822294 contractive_loss:  1.2654966799375273\n",
      "====> Epoch: 795 Average loss: 3.4925  reconstruction loss:  3.492362714063047 contractive_loss:  1.2613382532152677\n",
      "====> Epoch: 796 Average loss: 3.5045  reconstruction loss:  3.504384086373499 contractive_loss:  1.246023200263441\n",
      "====> Epoch: 797 Average loss: 3.9446  reconstruction loss:  3.9443317781643055 contractive_loss:  2.5973112581175437\n",
      "====> Epoch: 798 Average loss: 4.6348  reconstruction loss:  4.6346241823005725 contractive_loss:  1.607523061203492\n",
      "====> Epoch: 799 Average loss: 3.3479  reconstruction loss:  3.347755298105285 contractive_loss:  1.1533121514757299\n",
      "====> Epoch: 800 Average loss: 3.0764  reconstruction loss:  3.0761070619429853 contractive_loss:  2.8586652990143864\n",
      "====> Epoch: 801 Average loss: 2.8585  reconstruction loss:  2.858282159887849 contractive_loss:  2.2593590128984173\n",
      "====> Epoch: 802 Average loss: 2.7811  reconstruction loss:  2.7808282444596752 contractive_loss:  2.9794569275402276\n",
      "====> Epoch: 803 Average loss: 3.0571  reconstruction loss:  3.056870219740063 contractive_loss:  1.9696643322524028\n",
      "====> Epoch: 804 Average loss: 3.2949  reconstruction loss:  3.29448703582181 contractive_loss:  4.496098392829841\n",
      "====> Epoch: 805 Average loss: 3.1188  reconstruction loss:  3.1183312183536542 contractive_loss:  4.49959402646962\n",
      "====> Epoch: 806 Average loss: 2.9390  reconstruction loss:  2.9386166635503823 contractive_loss:  4.263335978728113\n",
      "====> Epoch: 807 Average loss: 2.6277  reconstruction loss:  2.6273121306602043 contractive_loss:  4.265519859658417\n",
      "====> Epoch: 808 Average loss: 2.4276  reconstruction loss:  2.4272452739823818 contractive_loss:  3.4146357547519157\n",
      "model saved!\n",
      "====> Epoch: 809 Average loss: 2.2026  reconstruction loss:  2.2024999634136857 contractive_loss:  1.385608880069416\n",
      "====> Epoch: 810 Average loss: 2.2573  reconstruction loss:  2.2571848680437747 contractive_loss:  1.184522650457668\n",
      "====> Epoch: 811 Average loss: 2.9889  reconstruction loss:  2.9888077455189115 contractive_loss:  1.2536603355081688\n",
      "====> Epoch: 812 Average loss: 4.0002  reconstruction loss:  4.000108668266932 contractive_loss:  1.034611032693945\n",
      "====> Epoch: 813 Average loss: 3.5840  reconstruction loss:  3.58389736764046 contractive_loss:  1.211879954115872\n",
      "====> Epoch: 814 Average loss: 3.4820  reconstruction loss:  3.4819179789288346 contractive_loss:  1.1091043242399672\n",
      "====> Epoch: 815 Average loss: 3.2530  reconstruction loss:  3.2527803397343003 contractive_loss:  2.436495146739614\n",
      "====> Epoch: 816 Average loss: 2.9639  reconstruction loss:  2.9634613593133317 contractive_loss:  3.9318109696748915\n",
      "====> Epoch: 817 Average loss: 3.0430  reconstruction loss:  3.0426072094199985 contractive_loss:  4.361283749799255\n",
      "====> Epoch: 818 Average loss: 3.1820  reconstruction loss:  3.1815669735576604 contractive_loss:  4.422178623507098\n",
      "====> Epoch: 819 Average loss: 2.6509  reconstruction loss:  2.6505269764592243 contractive_loss:  3.887986833858572\n",
      "====> Epoch: 820 Average loss: 2.7918  reconstruction loss:  2.7914542012428694 contractive_loss:  3.5674051629889134\n",
      "====> Epoch: 821 Average loss: 2.3688  reconstruction loss:  2.3686835615631225 contractive_loss:  1.3947297283126538\n",
      "====> Epoch: 822 Average loss: 3.0512  reconstruction loss:  3.0510733363372164 contractive_loss:  1.3966229506439334\n",
      "====> Epoch: 823 Average loss: 3.1459  reconstruction loss:  3.145706803933312 contractive_loss:  1.6680934179394038\n",
      "====> Epoch: 824 Average loss: 3.2353  reconstruction loss:  3.2350934217648852 contractive_loss:  1.6716693551058424\n",
      "====> Epoch: 825 Average loss: 3.3803  reconstruction loss:  3.3801434467649742 contractive_loss:  1.5547351861423004\n",
      "====> Epoch: 826 Average loss: 3.3954  reconstruction loss:  3.3952146267355805 contractive_loss:  1.896774183749779\n",
      "====> Epoch: 827 Average loss: 3.1048  reconstruction loss:  3.104524803660128 contractive_loss:  2.4169148958555002\n",
      "====> Epoch: 828 Average loss: 2.8632  reconstruction loss:  2.8629430625934265 contractive_loss:  2.7875941572818848\n",
      "====> Epoch: 829 Average loss: 2.8696  reconstruction loss:  2.8691653179060315 contractive_loss:  3.981295279380802\n",
      "====> Epoch: 830 Average loss: 2.3048  reconstruction loss:  2.304560716104802 contractive_loss:  2.465332507699618\n",
      "====> Epoch: 831 Average loss: 2.2951  reconstruction loss:  2.2950005773074236 contractive_loss:  1.343329698076788\n",
      "====> Epoch: 832 Average loss: 2.4573  reconstruction loss:  2.4571820519626373 contractive_loss:  1.535862939023746\n",
      "====> Epoch: 833 Average loss: 2.7033  reconstruction loss:  2.703231704634758 contractive_loss:  1.0518831292649398\n",
      "====> Epoch: 834 Average loss: 2.6931  reconstruction loss:  2.692964815787277 contractive_loss:  1.683139790644962\n",
      "====> Epoch: 835 Average loss: 3.8201  reconstruction loss:  3.81980440051586 contractive_loss:  2.5261836387211063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 836 Average loss: 2.7961  reconstruction loss:  2.79593438144531 contractive_loss:  2.0958441255560576\n",
      "====> Epoch: 837 Average loss: 2.7506  reconstruction loss:  2.7503651527718858 contractive_loss:  2.2961600092748293\n",
      "====> Epoch: 838 Average loss: 2.4481  reconstruction loss:  2.447890594568589 contractive_loss:  2.558382925592638\n",
      "====> Epoch: 839 Average loss: 2.2110  reconstruction loss:  2.2107580967620697 contractive_loss:  2.312855386701671\n",
      "====> Epoch: 840 Average loss: 2.7067  reconstruction loss:  2.706459353731913 contractive_loss:  2.7594645609840867\n",
      "====> Epoch: 841 Average loss: 2.6135  reconstruction loss:  2.613193467078654 contractive_loss:  2.9367954214126097\n",
      "====> Epoch: 842 Average loss: 2.5759  reconstruction loss:  2.5755888793519985 contractive_loss:  2.6693077039297437\n",
      "====> Epoch: 843 Average loss: 2.6833  reconstruction loss:  2.682861422444567 contractive_loss:  4.082348675461855\n",
      "====> Epoch: 844 Average loss: 2.8447  reconstruction loss:  2.84421670324323 contractive_loss:  5.059069148599749\n",
      "====> Epoch: 845 Average loss: 2.9006  reconstruction loss:  2.900101433476971 contractive_loss:  4.837327447736626\n",
      "====> Epoch: 846 Average loss: 3.0433  reconstruction loss:  3.0427998290068627 contractive_loss:  4.891291339449653\n",
      "====> Epoch: 847 Average loss: 3.6735  reconstruction loss:  3.672991005128848 contractive_loss:  5.365937098675283\n",
      "====> Epoch: 848 Average loss: 3.4708  reconstruction loss:  3.470228208913994 contractive_loss:  5.245135919310621\n",
      "====> Epoch: 849 Average loss: 2.9101  reconstruction loss:  2.9095597352198608 contractive_loss:  5.208090544533038\n",
      "====> Epoch: 850 Average loss: 2.9406  reconstruction loss:  2.9400871333563567 contractive_loss:  5.157077681581513\n",
      "====> Epoch: 851 Average loss: 2.5145  reconstruction loss:  2.5140269792523147 contractive_loss:  4.4636169085302315\n",
      "model saved!\n",
      "====> Epoch: 852 Average loss: 2.0096  reconstruction loss:  2.0093816391024912 contractive_loss:  2.3659051030010305\n",
      "====> Epoch: 853 Average loss: 2.5459  reconstruction loss:  2.5456651299735524 contractive_loss:  2.031716962830086\n",
      "====> Epoch: 854 Average loss: 2.9655  reconstruction loss:  2.9653310561696857 contractive_loss:  1.7314357933982736\n",
      "====> Epoch: 855 Average loss: 3.7227  reconstruction loss:  3.7226081382045386 contractive_loss:  1.355658335698187\n",
      "====> Epoch: 856 Average loss: 3.0320  reconstruction loss:  3.031891224465308 contractive_loss:  1.110567907810152\n",
      "====> Epoch: 857 Average loss: 2.8874  reconstruction loss:  2.8872201840284193 contractive_loss:  1.3974356147013736\n",
      "====> Epoch: 858 Average loss: 2.5622  reconstruction loss:  2.562033937219117 contractive_loss:  1.2634581044254554\n",
      "====> Epoch: 859 Average loss: 2.5698  reconstruction loss:  2.5697418505027 contractive_loss:  0.7655369999829458\n",
      "====> Epoch: 860 Average loss: 2.7200  reconstruction loss:  2.7198297756520473 contractive_loss:  1.73412768371615\n",
      "====> Epoch: 861 Average loss: 2.5690  reconstruction loss:  2.568802380519007 contractive_loss:  1.896032385064052\n",
      "====> Epoch: 862 Average loss: 3.1337  reconstruction loss:  3.1335485127217253 contractive_loss:  1.7163180893928143\n",
      "====> Epoch: 863 Average loss: 2.7552  reconstruction loss:  2.7549564245896017 contractive_loss:  2.0699908661974464\n",
      "====> Epoch: 864 Average loss: 2.2653  reconstruction loss:  2.2650920822867184 contractive_loss:  1.797638612676254\n",
      "====> Epoch: 865 Average loss: 2.2394  reconstruction loss:  2.239157823791067 contractive_loss:  2.7964478175564893\n",
      "====> Epoch: 866 Average loss: 2.9786  reconstruction loss:  2.9781098211641996 contractive_loss:  4.42431161786931\n",
      "====> Epoch: 867 Average loss: 3.8723  reconstruction loss:  3.8718321014264756 contractive_loss:  4.737178360710072\n",
      "====> Epoch: 868 Average loss: 3.5729  reconstruction loss:  3.5724582534377634 contractive_loss:  3.9820903635437293\n",
      "====> Epoch: 869 Average loss: 3.6565  reconstruction loss:  3.656314853698581 contractive_loss:  1.6437827551790574\n",
      "====> Epoch: 870 Average loss: 3.6552  reconstruction loss:  3.655034493442609 contractive_loss:  2.1533630775309014\n",
      "====> Epoch: 871 Average loss: 3.8975  reconstruction loss:  3.8971837776142286 contractive_loss:  3.1792278734798236\n",
      "====> Epoch: 872 Average loss: 3.8800  reconstruction loss:  3.879525632949411 contractive_loss:  4.496218653590964\n",
      "====> Epoch: 873 Average loss: 3.5685  reconstruction loss:  3.5681210937508556 contractive_loss:  3.6578992870023685\n",
      "====> Epoch: 874 Average loss: 3.9472  reconstruction loss:  3.9468230560234123 contractive_loss:  3.9717474650479057\n",
      "====> Epoch: 875 Average loss: 5.2880  reconstruction loss:  5.287394200498924 contractive_loss:  6.186467647023037\n",
      "====> Epoch: 876 Average loss: 5.0353  reconstruction loss:  5.03459472139837 contractive_loss:  6.7348973058275154\n",
      "====> Epoch: 877 Average loss: 5.0754  reconstruction loss:  5.074746491037117 contractive_loss:  6.919899156546582\n",
      "====> Epoch: 878 Average loss: 4.8488  reconstruction loss:  4.848112028024061 contractive_loss:  6.677918655094789\n",
      "====> Epoch: 879 Average loss: 3.8306  reconstruction loss:  3.8298450612850137 contractive_loss:  7.097137642978437\n",
      "====> Epoch: 880 Average loss: 5.0046  reconstruction loss:  5.00398013138073 contractive_loss:  6.5791042266029\n",
      "====> Epoch: 881 Average loss: 6.2821  reconstruction loss:  6.281523905700655 contractive_loss:  5.541439171208718\n",
      "====> Epoch: 882 Average loss: 6.1813  reconstruction loss:  6.180713189749362 contractive_loss:  5.806641089944685\n",
      "====> Epoch: 883 Average loss: 5.2948  reconstruction loss:  5.294136503731215 contractive_loss:  6.4252979488165725\n",
      "====> Epoch: 884 Average loss: 4.7687  reconstruction loss:  4.767873146401059 contractive_loss:  8.31500194936196\n",
      "====> Epoch: 885 Average loss: 4.1057  reconstruction loss:  4.104637082142006 contractive_loss:  10.296118326741933\n",
      "====> Epoch: 886 Average loss: 3.7312  reconstruction loss:  3.7306502842773273 contractive_loss:  5.330940812226464\n",
      "====> Epoch: 887 Average loss: 3.5102  reconstruction loss:  3.509828786992242 contractive_loss:  3.6006777226854987\n",
      "====> Epoch: 888 Average loss: 3.7537  reconstruction loss:  3.753280450192954 contractive_loss:  4.484017600913971\n",
      "====> Epoch: 889 Average loss: 3.2089  reconstruction loss:  3.2086889499848676 contractive_loss:  2.3559326287383224\n",
      "====> Epoch: 890 Average loss: 3.1146  reconstruction loss:  3.113978351164027 contractive_loss:  6.270320572989131\n",
      "====> Epoch: 891 Average loss: 2.4798  reconstruction loss:  2.479424422222827 contractive_loss:  3.8936770226374495\n",
      "model saved!\n",
      "====> Epoch: 892 Average loss: 1.9579  reconstruction loss:  1.9577453307306447 contractive_loss:  1.804747899946068\n",
      "====> Epoch: 893 Average loss: 4.4960  reconstruction loss:  4.495609455370268 contractive_loss:  3.6815232270396816\n",
      "====> Epoch: 894 Average loss: 2.2804  reconstruction loss:  2.280150201563757 contractive_loss:  2.5700151707468315\n",
      "====> Epoch: 895 Average loss: 2.4133  reconstruction loss:  2.413060397102446 contractive_loss:  2.6699132010396887\n",
      "====> Epoch: 896 Average loss: 3.2409  reconstruction loss:  3.240538020282636 contractive_loss:  3.831205367928353\n",
      "====> Epoch: 897 Average loss: 2.2779  reconstruction loss:  2.2776263821826173 contractive_loss:  2.4756908923135557\n",
      "====> Epoch: 898 Average loss: 2.9638  reconstruction loss:  2.9634887383950588 contractive_loss:  3.552248277461754\n",
      "====> Epoch: 899 Average loss: 3.7294  reconstruction loss:  3.7292548310514393 contractive_loss:  1.1865538044597792\n",
      "====> Epoch: 900 Average loss: 3.3575  reconstruction loss:  3.3573814405070737 contractive_loss:  0.8772815114354793\n",
      "====> Epoch: 901 Average loss: 2.4157  reconstruction loss:  2.41560273351629 contractive_loss:  1.3346902337655902\n",
      "====> Epoch: 902 Average loss: 2.6710  reconstruction loss:  2.670769891703831 contractive_loss:  2.4905631021482115\n",
      "====> Epoch: 903 Average loss: 2.4512  reconstruction loss:  2.450861569266912 contractive_loss:  3.6452753366025115\n",
      "====> Epoch: 904 Average loss: 2.3193  reconstruction loss:  2.3189483342178 contractive_loss:  3.7999936367379523\n",
      "====> Epoch: 905 Average loss: 2.3158  reconstruction loss:  2.3156190435383768 contractive_loss:  1.8609655861600742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 906 Average loss: 2.3638  reconstruction loss:  2.3636263112020837 contractive_loss:  1.2824525791393884\n",
      "====> Epoch: 907 Average loss: 2.3712  reconstruction loss:  2.371020775960304 contractive_loss:  1.4726227661729545\n",
      "====> Epoch: 908 Average loss: 2.1152  reconstruction loss:  2.1150520849890686 contractive_loss:  1.413765305957155\n",
      "====> Epoch: 909 Average loss: 2.3441  reconstruction loss:  2.343778994153006 contractive_loss:  2.7578359022042145\n",
      "====> Epoch: 910 Average loss: 2.4684  reconstruction loss:  2.4680909175150045 contractive_loss:  3.0468736257076836\n",
      "====> Epoch: 911 Average loss: 2.2155  reconstruction loss:  2.2152150969984863 contractive_loss:  2.509361098164951\n",
      "====> Epoch: 912 Average loss: 2.3265  reconstruction loss:  2.3262776098246563 contractive_loss:  2.6309655403328853\n",
      "====> Epoch: 913 Average loss: 2.2248  reconstruction loss:  2.22459204144892 contractive_loss:  1.7159823667359875\n",
      "====> Epoch: 914 Average loss: 2.1805  reconstruction loss:  2.1803655966798443 contractive_loss:  1.8091972685447253\n",
      "====> Epoch: 915 Average loss: 2.2028  reconstruction loss:  2.2025470002273693 contractive_loss:  2.2906964423961935\n",
      "====> Epoch: 916 Average loss: 2.2801  reconstruction loss:  2.2798755437400295 contractive_loss:  2.144541972099282\n",
      "====> Epoch: 917 Average loss: 2.2913  reconstruction loss:  2.2910736767663407 contractive_loss:  2.195498582675918\n",
      "====> Epoch: 918 Average loss: 2.1292  reconstruction loss:  2.1289726307560652 contractive_loss:  2.237064860521711\n",
      "====> Epoch: 919 Average loss: 2.1338  reconstruction loss:  2.133550918843788 contractive_loss:  2.1834058733255173\n",
      "====> Epoch: 920 Average loss: 2.3475  reconstruction loss:  2.3473303169920205 contractive_loss:  1.886791154643798\n",
      "====> Epoch: 921 Average loss: 2.0856  reconstruction loss:  2.0854705307949444 contractive_loss:  1.7112691449083706\n",
      "====> Epoch: 922 Average loss: 2.4561  reconstruction loss:  2.455843001760525 contractive_loss:  2.568581447691864\n",
      "====> Epoch: 923 Average loss: 2.2073  reconstruction loss:  2.207027968013897 contractive_loss:  2.6027294416968743\n",
      "====> Epoch: 924 Average loss: 2.2280  reconstruction loss:  2.2277011098527266 contractive_loss:  3.4431939361915216\n",
      "====> Epoch: 925 Average loss: 3.4537  reconstruction loss:  3.453257622642374 contractive_loss:  4.417357382031284\n",
      "====> Epoch: 926 Average loss: 3.4739  reconstruction loss:  3.4737155006562386 contractive_loss:  2.126857296285699\n",
      "====> Epoch: 927 Average loss: 3.1810  reconstruction loss:  3.180877558770734 contractive_loss:  1.304675630631032\n",
      "====> Epoch: 928 Average loss: 3.7301  reconstruction loss:  3.729899786447578 contractive_loss:  2.362427271184943\n",
      "====> Epoch: 929 Average loss: 2.7059  reconstruction loss:  2.705526285929092 contractive_loss:  3.287638470575154\n",
      "====> Epoch: 930 Average loss: 2.7878  reconstruction loss:  2.787469378597517 contractive_loss:  2.825859636285396\n",
      "====> Epoch: 931 Average loss: 3.3409  reconstruction loss:  3.340621511173686 contractive_loss:  2.4992786589840787\n",
      "====> Epoch: 932 Average loss: 3.5667  reconstruction loss:  3.5665277244056184 contractive_loss:  1.404743466720949\n",
      "====> Epoch: 933 Average loss: 3.0706  reconstruction loss:  3.0703382657134783 contractive_loss:  3.0741965875680424\n",
      "====> Epoch: 934 Average loss: 2.8213  reconstruction loss:  2.8209672403006856 contractive_loss:  3.621954524593758\n",
      "====> Epoch: 935 Average loss: 2.8606  reconstruction loss:  2.8602654786832256 contractive_loss:  3.706007390609027\n",
      "====> Epoch: 936 Average loss: 2.7521  reconstruction loss:  2.751772154533071 contractive_loss:  3.2396391232972173\n",
      "====> Epoch: 937 Average loss: 2.1262  reconstruction loss:  2.125944495269096 contractive_loss:  2.688819976020451\n",
      "====> Epoch: 938 Average loss: 3.5947  reconstruction loss:  3.594466960392999 contractive_loss:  2.5333388488409687\n",
      "====> Epoch: 939 Average loss: 3.4554  reconstruction loss:  3.4552735021421683 contractive_loss:  0.9297271860219384\n",
      "====> Epoch: 940 Average loss: 3.4828  reconstruction loss:  3.482686535347774 contractive_loss:  1.599862871003907\n",
      "====> Epoch: 941 Average loss: 2.7415  reconstruction loss:  2.7411623283832576 contractive_loss:  3.251430524680234\n",
      "====> Epoch: 942 Average loss: 2.4189  reconstruction loss:  2.4185065477856127 contractive_loss:  3.7193957625826255\n",
      "model saved!\n",
      "====> Epoch: 943 Average loss: 1.8671  reconstruction loss:  1.8669739481126768 contractive_loss:  1.5125388701863278\n",
      "====> Epoch: 944 Average loss: 2.2697  reconstruction loss:  2.2696096915061634 contractive_loss:  1.3490295013958593\n",
      "====> Epoch: 945 Average loss: 1.9221  reconstruction loss:  1.9220161538963925 contractive_loss:  1.1392673773521085\n",
      "====> Epoch: 946 Average loss: 2.0404  reconstruction loss:  2.040242249228376 contractive_loss:  1.4472423398175738\n",
      "====> Epoch: 947 Average loss: 2.2678  reconstruction loss:  2.2676113369397415 contractive_loss:  2.229084669951711\n",
      "====> Epoch: 948 Average loss: 2.3586  reconstruction loss:  2.3583358396991003 contractive_loss:  2.342702104038608\n",
      "====> Epoch: 949 Average loss: 2.5146  reconstruction loss:  2.5144629042178 contractive_loss:  1.8055892222317955\n",
      "====> Epoch: 950 Average loss: 3.3466  reconstruction loss:  3.3464351214282226 contractive_loss:  1.4367762497091199\n",
      "====> Epoch: 951 Average loss: 3.2906  reconstruction loss:  3.290434802258293 contractive_loss:  1.3268085774628775\n",
      "====> Epoch: 952 Average loss: 3.9578  reconstruction loss:  3.9576819058183887 contractive_loss:  1.302701047160477\n",
      "====> Epoch: 953 Average loss: 3.7651  reconstruction loss:  3.7650427681670413 contractive_loss:  1.0326837127949147\n",
      "====> Epoch: 954 Average loss: 3.7415  reconstruction loss:  3.7414167035290364 contractive_loss:  0.908042461675555\n",
      "====> Epoch: 955 Average loss: 3.6658  reconstruction loss:  3.66574245228569 contractive_loss:  0.8356047710794858\n",
      "====> Epoch: 956 Average loss: 3.6819  reconstruction loss:  3.6818284833439043 contractive_loss:  0.9058717545453406\n",
      "====> Epoch: 957 Average loss: 3.4183  reconstruction loss:  3.4181775101341016 contractive_loss:  0.9396177820393872\n",
      "====> Epoch: 958 Average loss: 3.6141  reconstruction loss:  3.613999083317415 contractive_loss:  0.8781086226274368\n",
      "====> Epoch: 959 Average loss: 3.0910  reconstruction loss:  3.090891656196959 contractive_loss:  0.8549406869324008\n",
      "====> Epoch: 960 Average loss: 3.1107  reconstruction loss:  3.1106023528452607 contractive_loss:  1.2338830887551517\n",
      "====> Epoch: 961 Average loss: 3.1066  reconstruction loss:  3.106563604429827 contractive_loss:  0.5083801343231155\n",
      "====> Epoch: 962 Average loss: 3.1030  reconstruction loss:  3.1029670473896216 contractive_loss:  0.5212037635499339\n",
      "====> Epoch: 963 Average loss: 3.2808  reconstruction loss:  3.2806896190576724 contractive_loss:  0.747059888467207\n",
      "====> Epoch: 964 Average loss: 3.1513  reconstruction loss:  3.151195253125183 contractive_loss:  0.6342337371207251\n",
      "====> Epoch: 965 Average loss: 2.6989  reconstruction loss:  2.6988148150732814 contractive_loss:  0.6967723094810475\n",
      "====> Epoch: 966 Average loss: 2.7128  reconstruction loss:  2.712768083168835 contractive_loss:  0.7430694711441406\n",
      "====> Epoch: 967 Average loss: 2.6757  reconstruction loss:  2.6756548614104054 contractive_loss:  0.6282216176730588\n",
      "====> Epoch: 968 Average loss: 2.6259  reconstruction loss:  2.625857263667486 contractive_loss:  0.6859553503180812\n",
      "====> Epoch: 969 Average loss: 3.0947  reconstruction loss:  3.0946124497551226 contractive_loss:  0.8999966190605361\n",
      "====> Epoch: 970 Average loss: 2.6953  reconstruction loss:  2.6952042632930473 contractive_loss:  0.6916310269140384\n",
      "====> Epoch: 971 Average loss: 3.0163  reconstruction loss:  3.0162238503075183 contractive_loss:  0.7840768240809376\n",
      "====> Epoch: 972 Average loss: 2.7243  reconstruction loss:  2.724220519100223 contractive_loss:  0.7466813613064859\n",
      "====> Epoch: 973 Average loss: 2.7408  reconstruction loss:  2.740761310630408 contractive_loss:  0.749114390543551\n",
      "====> Epoch: 974 Average loss: 2.5934  reconstruction loss:  2.5933200164146997 contractive_loss:  0.5365698964642807\n",
      "====> Epoch: 975 Average loss: 2.9537  reconstruction loss:  2.95360098507113 contractive_loss:  0.8070534766095236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 976 Average loss: 2.7359  reconstruction loss:  2.7358000473413666 contractive_loss:  0.7029260269262397\n",
      "====> Epoch: 977 Average loss: 2.5446  reconstruction loss:  2.5445754369006277 contractive_loss:  0.5107796556221559\n",
      "====> Epoch: 978 Average loss: 3.0217  reconstruction loss:  3.021664994796959 contractive_loss:  0.8465120672284095\n",
      "====> Epoch: 979 Average loss: 2.5657  reconstruction loss:  2.565690736854783 contractive_loss:  0.44219394809838597\n",
      "====> Epoch: 980 Average loss: 2.7008  reconstruction loss:  2.7008057249589137 contractive_loss:  0.39297193358729293\n",
      "====> Epoch: 981 Average loss: 3.2807  reconstruction loss:  3.2806213752889573 contractive_loss:  0.8337080218891368\n",
      "====> Epoch: 982 Average loss: 3.1658  reconstruction loss:  3.165715876562223 contractive_loss:  0.8164493909925346\n",
      "====> Epoch: 983 Average loss: 3.5162  reconstruction loss:  3.5161667525176563 contractive_loss:  0.43544034911197\n",
      "====> Epoch: 984 Average loss: 3.1638  reconstruction loss:  3.163736614283099 contractive_loss:  0.5993398385663073\n",
      "====> Epoch: 985 Average loss: 3.1658  reconstruction loss:  3.16577605689688 contractive_loss:  0.6355180636073455\n",
      "====> Epoch: 986 Average loss: 3.1728  reconstruction loss:  3.1726897349152363 contractive_loss:  0.6908746324312038\n",
      "====> Epoch: 987 Average loss: 3.1431  reconstruction loss:  3.142988146216961 contractive_loss:  0.7308602012119269\n",
      "====> Epoch: 988 Average loss: 3.0756  reconstruction loss:  3.0754682829139965 contractive_loss:  0.8241579968495456\n",
      "====> Epoch: 989 Average loss: 3.0780  reconstruction loss:  3.0779702650696885 contractive_loss:  0.5900116500170983\n",
      "====> Epoch: 990 Average loss: 3.1061  reconstruction loss:  3.106027567281684 contractive_loss:  0.7465413356960924\n",
      "====> Epoch: 991 Average loss: 2.5146  reconstruction loss:  2.5145196659137703 contractive_loss:  0.3624886321747017\n",
      "====> Epoch: 992 Average loss: 3.2091  reconstruction loss:  3.2090601133640155 contractive_loss:  0.7640010777823995\n",
      "====> Epoch: 993 Average loss: 2.9231  reconstruction loss:  2.9230230793562413 contractive_loss:  0.4327760038440723\n",
      "====> Epoch: 994 Average loss: 2.8135  reconstruction loss:  2.8133997214330493 contractive_loss:  0.6816822692814697\n",
      "====> Epoch: 995 Average loss: 2.8085  reconstruction loss:  2.808463995255514 contractive_loss:  0.6318381320799739\n",
      "====> Epoch: 996 Average loss: 2.6788  reconstruction loss:  2.678740470047507 contractive_loss:  0.6603780721715664\n",
      "====> Epoch: 997 Average loss: 2.4160  reconstruction loss:  2.4159984660470633 contractive_loss:  0.3909071088911311\n",
      "====> Epoch: 998 Average loss: 3.0824  reconstruction loss:  3.082289058132003 contractive_loss:  0.727326398117117\n",
      "====> Epoch: 999 Average loss: 2.4815  reconstruction loss:  2.481407009708541 contractive_loss:  0.5633794954560393\n",
      "====> Epoch: 1000 Average loss: 2.9517  reconstruction loss:  2.9516165005045956 contractive_loss:  0.3567240664188718\n",
      "====> Epoch: 1001 Average loss: 2.9885  reconstruction loss:  2.988486187608788 contractive_loss:  0.6046915205810446\n",
      "====> Epoch: 1002 Average loss: 2.5696  reconstruction loss:  2.5694826928026004 contractive_loss:  0.827200666178455\n",
      "====> Epoch: 1003 Average loss: 2.8342  reconstruction loss:  2.8341263873700053 contractive_loss:  0.6366923663135271\n",
      "====> Epoch: 1004 Average loss: 2.5018  reconstruction loss:  2.5017449147497293 contractive_loss:  0.6235843223070511\n",
      "====> Epoch: 1005 Average loss: 2.7333  reconstruction loss:  2.7331988548948902 contractive_loss:  0.7100488058087345\n",
      "====> Epoch: 1006 Average loss: 2.5553  reconstruction loss:  2.5551878601587985 contractive_loss:  0.6522300838320607\n",
      "====> Epoch: 1007 Average loss: 2.3715  reconstruction loss:  2.371417606127312 contractive_loss:  0.40645104078504407\n",
      "====> Epoch: 1008 Average loss: 2.4192  reconstruction loss:  2.4190965875264423 contractive_loss:  0.572125845021493\n",
      "====> Epoch: 1009 Average loss: 2.4168  reconstruction loss:  2.416784527604449 contractive_loss:  0.5149501784271965\n",
      "====> Epoch: 1010 Average loss: 3.2534  reconstruction loss:  3.253369922914779 contractive_loss:  0.5474553079090401\n",
      "====> Epoch: 1011 Average loss: 2.2828  reconstruction loss:  2.282812766515799 contractive_loss:  0.36709167726722675\n",
      "====> Epoch: 1012 Average loss: 2.2878  reconstruction loss:  2.2877576473910812 contractive_loss:  0.3978903593092876\n",
      "====> Epoch: 1013 Average loss: 2.3505  reconstruction loss:  2.3504277891379046 contractive_loss:  0.39248908327368903\n",
      "====> Epoch: 1014 Average loss: 3.3180  reconstruction loss:  3.317929234421261 contractive_loss:  0.7601357365826236\n",
      "====> Epoch: 1015 Average loss: 2.9801  reconstruction loss:  2.980016139716562 contractive_loss:  0.7945854274494643\n",
      "====> Epoch: 1016 Average loss: 2.7219  reconstruction loss:  2.7218162848860543 contractive_loss:  0.8729650041447576\n",
      "====> Epoch: 1017 Average loss: 2.4493  reconstruction loss:  2.4492376633128194 contractive_loss:  0.9337536071307657\n",
      "====> Epoch: 1018 Average loss: 2.6542  reconstruction loss:  2.6540702151668403 contractive_loss:  0.8559574685254404\n",
      "====> Epoch: 1019 Average loss: 2.4330  reconstruction loss:  2.4328446628253038 contractive_loss:  1.072906336185369\n",
      "====> Epoch: 1020 Average loss: 3.1541  reconstruction loss:  3.1540538388393715 contractive_loss:  0.7669308137059978\n",
      "====> Epoch: 1021 Average loss: 3.5578  reconstruction loss:  3.5577284821436432 contractive_loss:  0.59024683569492\n",
      "====> Epoch: 1022 Average loss: 3.0725  reconstruction loss:  3.0724293476150972 contractive_loss:  0.6826427111254626\n",
      "====> Epoch: 1023 Average loss: 2.6420  reconstruction loss:  2.641933098492075 contractive_loss:  0.7090185536557143\n",
      "====> Epoch: 1024 Average loss: 2.9103  reconstruction loss:  2.910169067969717 contractive_loss:  0.8785604490814585\n",
      "====> Epoch: 1025 Average loss: 3.2558  reconstruction loss:  3.255726598082165 contractive_loss:  0.9584687768923305\n",
      "====> Epoch: 1026 Average loss: 2.8099  reconstruction loss:  2.8097242664122732 contractive_loss:  1.273151260337196\n",
      "====> Epoch: 1027 Average loss: 2.7764  reconstruction loss:  2.7762590269268186 contractive_loss:  1.275963967437004\n",
      "====> Epoch: 1028 Average loss: 2.3751  reconstruction loss:  2.375017929066682 contractive_loss:  0.4137375584369346\n",
      "====> Epoch: 1029 Average loss: 2.8585  reconstruction loss:  2.858421745252674 contractive_loss:  1.074455308682785\n",
      "====> Epoch: 1030 Average loss: 2.4402  reconstruction loss:  2.44011193085976 contractive_loss:  0.8045517694897828\n",
      "====> Epoch: 1031 Average loss: 2.5437  reconstruction loss:  2.543609034363093 contractive_loss:  0.8812010383476561\n",
      "====> Epoch: 1032 Average loss: 2.3219  reconstruction loss:  2.3218137248277437 contractive_loss:  0.7490276668522459\n",
      "====> Epoch: 1033 Average loss: 2.6638  reconstruction loss:  2.663649548405908 contractive_loss:  1.1412770462140707\n",
      "====> Epoch: 1034 Average loss: 3.1336  reconstruction loss:  3.133508218610392 contractive_loss:  0.5575614952030327\n",
      "====> Epoch: 1035 Average loss: 2.3343  reconstruction loss:  2.334202820101398 contractive_loss:  0.7639486925433114\n",
      "====> Epoch: 1036 Average loss: 2.2213  reconstruction loss:  2.221195355517153 contractive_loss:  0.8420448465222272\n",
      "====> Epoch: 1037 Average loss: 3.0706  reconstruction loss:  3.070544308540743 contractive_loss:  0.8659416063086224\n",
      "====> Epoch: 1038 Average loss: 2.7120  reconstruction loss:  2.711935068403656 contractive_loss:  0.5217961482579209\n",
      "====> Epoch: 1039 Average loss: 3.1019  reconstruction loss:  3.1018611476981084 contractive_loss:  0.4815357989320006\n",
      "====> Epoch: 1040 Average loss: 2.8563  reconstruction loss:  2.8562022925445607 contractive_loss:  0.9600181737447723\n",
      "====> Epoch: 1041 Average loss: 2.8774  reconstruction loss:  2.877335873003856 contractive_loss:  0.8218768175413004\n",
      "====> Epoch: 1042 Average loss: 2.2608  reconstruction loss:  2.260760061240476 contractive_loss:  0.5612143353825456\n",
      "====> Epoch: 1043 Average loss: 2.9703  reconstruction loss:  2.9701808297214995 contractive_loss:  0.9297100843839146\n",
      "====> Epoch: 1044 Average loss: 2.7145  reconstruction loss:  2.714458340214484 contractive_loss:  0.7641645563986228\n",
      "====> Epoch: 1045 Average loss: 3.1062  reconstruction loss:  3.1060919208980353 contractive_loss:  0.8645481800055088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1046 Average loss: 2.7646  reconstruction loss:  2.7645397824500213 contractive_loss:  1.0448454700443373\n",
      "====> Epoch: 1047 Average loss: 3.4181  reconstruction loss:  3.4179973826866172 contractive_loss:  0.8532841199475893\n",
      "====> Epoch: 1048 Average loss: 4.0710  reconstruction loss:  4.070992896315292 contractive_loss:  0.5360198323645217\n",
      "====> Epoch: 1049 Average loss: 3.8364  reconstruction loss:  3.8363331845592255 contractive_loss:  0.5553382415468527\n",
      "====> Epoch: 1050 Average loss: 3.3456  reconstruction loss:  3.34547574056013 contractive_loss:  0.8041902360046981\n",
      "====> Epoch: 1051 Average loss: 3.6903  reconstruction loss:  3.6902475261966483 contractive_loss:  0.6207630275923659\n",
      "====> Epoch: 1052 Average loss: 3.3633  reconstruction loss:  3.3632296738509546 contractive_loss:  0.7363829055418121\n",
      "====> Epoch: 1053 Average loss: 3.5914  reconstruction loss:  3.591363970618295 contractive_loss:  0.6886887606943501\n",
      "====> Epoch: 1054 Average loss: 3.3252  reconstruction loss:  3.325124753700081 contractive_loss:  0.6740721763687522\n",
      "====> Epoch: 1055 Average loss: 3.4254  reconstruction loss:  3.42538587621368 contractive_loss:  0.6347329352621245\n",
      "====> Epoch: 1056 Average loss: 3.0398  reconstruction loss:  3.03967925233973 contractive_loss:  0.794191258017566\n",
      "====> Epoch: 1057 Average loss: 2.8276  reconstruction loss:  2.8275056764272195 contractive_loss:  0.6867774283990292\n",
      "====> Epoch: 1058 Average loss: 2.4660  reconstruction loss:  2.465988959746778 contractive_loss:  0.3103900829786099\n",
      "====> Epoch: 1059 Average loss: 2.8106  reconstruction loss:  2.8105381890166656 contractive_loss:  0.7207461477592736\n",
      "====> Epoch: 1060 Average loss: 2.2532  reconstruction loss:  2.2531788607357965 contractive_loss:  0.3786120503910304\n",
      "====> Epoch: 1061 Average loss: 2.3523  reconstruction loss:  2.3522588923628454 contractive_loss:  0.3152494533507916\n",
      "====> Epoch: 1062 Average loss: 2.2775  reconstruction loss:  2.2774684666034064 contractive_loss:  0.2956410917425704\n",
      "====> Epoch: 1063 Average loss: 2.8772  reconstruction loss:  2.877158490212151 contractive_loss:  0.5929050820035743\n",
      "====> Epoch: 1064 Average loss: 2.7489  reconstruction loss:  2.748783999269096 contractive_loss:  0.6827978279324369\n",
      "====> Epoch: 1065 Average loss: 3.0260  reconstruction loss:  3.0258971817246567 contractive_loss:  0.7743829269604781\n",
      "====> Epoch: 1066 Average loss: 2.2542  reconstruction loss:  2.2541735247743286 contractive_loss:  0.3031198527582736\n",
      "====> Epoch: 1067 Average loss: 2.7606  reconstruction loss:  2.760526969406931 contractive_loss:  0.706197423948165\n",
      "====> Epoch: 1068 Average loss: 2.3960  reconstruction loss:  2.395927386516578 contractive_loss:  0.3022990697340808\n",
      "====> Epoch: 1069 Average loss: 2.4100  reconstruction loss:  2.409857876070946 contractive_loss:  1.0225941885308505\n",
      "====> Epoch: 1070 Average loss: 2.2803  reconstruction loss:  2.2802394846074936 contractive_loss:  0.3372314077658588\n",
      "====> Epoch: 1071 Average loss: 2.3147  reconstruction loss:  2.3147082819616607 contractive_loss:  0.39724876530099806\n",
      "====> Epoch: 1072 Average loss: 2.5070  reconstruction loss:  2.506931584091168 contractive_loss:  1.0395478982777502\n",
      "====> Epoch: 1073 Average loss: 2.9087  reconstruction loss:  2.908570500582307 contractive_loss:  0.9626701167680091\n",
      "====> Epoch: 1074 Average loss: 3.0906  reconstruction loss:  3.0904656838339064 contractive_loss:  1.0718455576163544\n",
      "====> Epoch: 1075 Average loss: 2.2588  reconstruction loss:  2.258698929311121 contractive_loss:  1.342567620786592\n",
      "====> Epoch: 1076 Average loss: 2.0392  reconstruction loss:  2.03910523232407 contractive_loss:  1.3013740217307905\n",
      "====> Epoch: 1077 Average loss: 2.2767  reconstruction loss:  2.2765319055811686 contractive_loss:  1.5054153646868353\n",
      "====> Epoch: 1078 Average loss: 2.8992  reconstruction loss:  2.8989354932644784 contractive_loss:  3.0164251411242877\n",
      "====> Epoch: 1079 Average loss: 2.7221  reconstruction loss:  2.7218074931154472 contractive_loss:  2.7652923202118878\n",
      "====> Epoch: 1080 Average loss: 3.2177  reconstruction loss:  3.217403609491555 contractive_loss:  2.9845157713198756\n",
      "====> Epoch: 1081 Average loss: 3.1118  reconstruction loss:  3.1115876950028123 contractive_loss:  2.0713731467612977\n",
      "====> Epoch: 1082 Average loss: 2.8864  reconstruction loss:  2.8861479272606854 contractive_loss:  2.9488308861625474\n",
      "====> Epoch: 1083 Average loss: 3.3601  reconstruction loss:  3.359829488613441 contractive_loss:  3.0037651437082507\n",
      "====> Epoch: 1084 Average loss: 3.1316  reconstruction loss:  3.1313034916133495 contractive_loss:  2.8575989082924225\n",
      "====> Epoch: 1085 Average loss: 2.2863  reconstruction loss:  2.2861682172562636 contractive_loss:  1.4370094060045082\n",
      "====> Epoch: 1086 Average loss: 2.4820  reconstruction loss:  2.481903420604476 contractive_loss:  1.242073952636227\n",
      "====> Epoch: 1087 Average loss: 2.1600  reconstruction loss:  2.1598565033487365 contractive_loss:  1.255228321543245\n",
      "====> Epoch: 1088 Average loss: 2.3791  reconstruction loss:  2.378979722545532 contractive_loss:  1.1379636697003452\n",
      "model saved!\n",
      "====> Epoch: 1089 Average loss: 1.7723  reconstruction loss:  1.7721741326406686 contractive_loss:  1.4251481938703376\n",
      "====> Epoch: 1090 Average loss: 2.2953  reconstruction loss:  2.295188134036281 contractive_loss:  1.1419722914929562\n",
      "====> Epoch: 1091 Average loss: 2.8122  reconstruction loss:  2.8119850569764058 contractive_loss:  1.6611924776330724\n",
      "====> Epoch: 1092 Average loss: 1.8558  reconstruction loss:  1.8556084225698368 contractive_loss:  1.5950865874566318\n",
      "====> Epoch: 1093 Average loss: 2.2267  reconstruction loss:  2.226542637715128 contractive_loss:  1.320178738980593\n",
      "====> Epoch: 1094 Average loss: 2.5929  reconstruction loss:  2.592742548598482 contractive_loss:  2.033983715463344\n",
      "====> Epoch: 1095 Average loss: 2.0477  reconstruction loss:  2.047526129057408 contractive_loss:  1.3434478384245552\n",
      "model saved!\n",
      "====> Epoch: 1096 Average loss: 1.7477  reconstruction loss:  1.747582293858988 contractive_loss:  1.3796320596971183\n",
      "====> Epoch: 1097 Average loss: 2.5672  reconstruction loss:  2.5671157032032035 contractive_loss:  1.2167074820227306\n",
      "====> Epoch: 1098 Average loss: 1.9137  reconstruction loss:  1.91359434881134 contractive_loss:  1.1076347798412969\n",
      "model saved!\n",
      "====> Epoch: 1099 Average loss: 1.6832  reconstruction loss:  1.6830053662956916 contractive_loss:  1.469081169912009\n",
      "====> Epoch: 1100 Average loss: 1.8686  reconstruction loss:  1.8684653927301111 contractive_loss:  1.111883277337106\n",
      "====> Epoch: 1101 Average loss: 2.2438  reconstruction loss:  2.2436781368858565 contractive_loss:  1.1973109503170896\n",
      "====> Epoch: 1102 Average loss: 1.8652  reconstruction loss:  1.8650069496304484 contractive_loss:  1.7238751399173353\n",
      "====> Epoch: 1103 Average loss: 2.8260  reconstruction loss:  2.8258815174868617 contractive_loss:  1.5979469871283098\n",
      "====> Epoch: 1104 Average loss: 2.8860  reconstruction loss:  2.8858682049224935 contractive_loss:  1.5052989924566773\n",
      "====> Epoch: 1105 Average loss: 2.9404  reconstruction loss:  2.940309843044579 contractive_loss:  1.129924536960735\n",
      "====> Epoch: 1106 Average loss: 2.6988  reconstruction loss:  2.698598279244499 contractive_loss:  1.8321954497452755\n",
      "====> Epoch: 1107 Average loss: 2.5314  reconstruction loss:  2.5313417477854685 contractive_loss:  0.567134777582271\n",
      "====> Epoch: 1108 Average loss: 2.9210  reconstruction loss:  2.9208554308987162 contractive_loss:  1.4148298612148331\n",
      "====> Epoch: 1109 Average loss: 2.4604  reconstruction loss:  2.4602368105881083 contractive_loss:  1.655308495153528\n",
      "====> Epoch: 1110 Average loss: 1.8694  reconstruction loss:  1.8692227079686998 contractive_loss:  1.6694688301808063\n",
      "====> Epoch: 1111 Average loss: 1.8201  reconstruction loss:  1.8200059003262967 contractive_loss:  1.3811234131120713\n",
      "====> Epoch: 1112 Average loss: 2.0064  reconstruction loss:  2.0062436798394754 contractive_loss:  1.0695179327973314\n",
      "====> Epoch: 1113 Average loss: 2.7891  reconstruction loss:  2.7889565818402375 contractive_loss:  1.4398838726201704\n",
      "====> Epoch: 1114 Average loss: 2.5668  reconstruction loss:  2.566778211841328 contractive_loss:  0.5244226820919388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1115 Average loss: 3.2199  reconstruction loss:  3.219823732833885 contractive_loss:  1.059001100147825\n",
      "====> Epoch: 1116 Average loss: 2.5087  reconstruction loss:  2.5085248545867094 contractive_loss:  1.6693386576227052\n",
      "====> Epoch: 1117 Average loss: 2.2873  reconstruction loss:  2.287190285463846 contractive_loss:  0.7885928961061814\n",
      "====> Epoch: 1118 Average loss: 2.4880  reconstruction loss:  2.487916147773109 contractive_loss:  1.1244559420273743\n",
      "====> Epoch: 1119 Average loss: 2.8694  reconstruction loss:  2.8692892016905205 contractive_loss:  1.3084223811310611\n",
      "====> Epoch: 1120 Average loss: 3.4466  reconstruction loss:  3.446462108491337 contractive_loss:  1.375688562879402\n",
      "====> Epoch: 1121 Average loss: 2.4947  reconstruction loss:  2.4945924099397185 contractive_loss:  1.0271434361324012\n",
      "====> Epoch: 1122 Average loss: 2.6374  reconstruction loss:  2.637303924247984 contractive_loss:  0.8481185113623715\n",
      "====> Epoch: 1123 Average loss: 2.4841  reconstruction loss:  2.484005000032523 contractive_loss:  1.1083721882300004\n",
      "====> Epoch: 1124 Average loss: 2.5073  reconstruction loss:  2.507285022217309 contractive_loss:  0.46337016411921395\n",
      "====> Epoch: 1125 Average loss: 1.9960  reconstruction loss:  1.9958212057743037 contractive_loss:  1.490076482991993\n",
      "====> Epoch: 1126 Average loss: 2.1826  reconstruction loss:  2.182455399916517 contractive_loss:  1.2557410879158857\n",
      "====> Epoch: 1127 Average loss: 1.8406  reconstruction loss:  1.8404859972559797 contractive_loss:  1.2389472263451158\n",
      "====> Epoch: 1128 Average loss: 2.2455  reconstruction loss:  2.245380937064055 contractive_loss:  1.2105990198040089\n",
      "====> Epoch: 1129 Average loss: 2.3463  reconstruction loss:  2.346148296120027 contractive_loss:  1.1219812397753677\n",
      "====> Epoch: 1130 Average loss: 2.0906  reconstruction loss:  2.090478820129411 contractive_loss:  1.3693477797033957\n",
      "====> Epoch: 1131 Average loss: 2.3705  reconstruction loss:  2.3703797637355795 contractive_loss:  1.2034123939883186\n",
      "====> Epoch: 1132 Average loss: 2.4276  reconstruction loss:  2.427469083746175 contractive_loss:  1.1358949438885904\n",
      "====> Epoch: 1133 Average loss: 2.2141  reconstruction loss:  2.2139825764095904 contractive_loss:  1.1823660680549466\n",
      "====> Epoch: 1134 Average loss: 2.7478  reconstruction loss:  2.747581927352207 contractive_loss:  1.940664301377923\n",
      "====> Epoch: 1135 Average loss: 1.8930  reconstruction loss:  1.8928088100334508 contractive_loss:  1.5995508678206765\n",
      "====> Epoch: 1136 Average loss: 2.1937  reconstruction loss:  2.1935382217237565 contractive_loss:  1.2402350770145718\n",
      "====> Epoch: 1137 Average loss: 2.8799  reconstruction loss:  2.879628865102731 contractive_loss:  2.2502035786385446\n",
      "====> Epoch: 1138 Average loss: 1.9291  reconstruction loss:  1.9289195121778522 contractive_loss:  1.7420132088615399\n",
      "====> Epoch: 1139 Average loss: 2.9339  reconstruction loss:  2.933639035588075 contractive_loss:  2.861987216687203\n",
      "====> Epoch: 1140 Average loss: 2.5768  reconstruction loss:  2.576335138183716 contractive_loss:  4.383671418049588\n",
      "====> Epoch: 1141 Average loss: 2.2291  reconstruction loss:  2.2286180426746363 contractive_loss:  4.401715099886214\n",
      "====> Epoch: 1142 Average loss: 2.0926  reconstruction loss:  2.092100830592359 contractive_loss:  4.621147587255824\n",
      "====> Epoch: 1143 Average loss: 1.8520  reconstruction loss:  1.851493340789398 contractive_loss:  4.777770346179167\n",
      "====> Epoch: 1144 Average loss: 2.1213  reconstruction loss:  2.120853469540334 contractive_loss:  4.708800188768248\n",
      "====> Epoch: 1145 Average loss: 2.4626  reconstruction loss:  2.4621129551607037 contractive_loss:  4.40167509321513\n",
      "====> Epoch: 1146 Average loss: 2.1351  reconstruction loss:  2.134617253721476 contractive_loss:  4.387945805111475\n",
      "====> Epoch: 1147 Average loss: 1.8659  reconstruction loss:  1.865414532379528 contractive_loss:  4.945859001805011\n",
      "====> Epoch: 1148 Average loss: 1.8555  reconstruction loss:  1.8549790905802275 contractive_loss:  4.876942834029129\n",
      "====> Epoch: 1149 Average loss: 2.2299  reconstruction loss:  2.2294350087077586 contractive_loss:  4.590729162427878\n",
      "====> Epoch: 1150 Average loss: 2.4162  reconstruction loss:  2.4158635698687094 contractive_loss:  3.64137435314316\n",
      "====> Epoch: 1151 Average loss: 2.2473  reconstruction loss:  2.2470098104832372 contractive_loss:  3.2914997808226047\n",
      "model saved!\n",
      "====> Epoch: 1152 Average loss: 1.2507  reconstruction loss:  1.250335086038714 contractive_loss:  3.8727416732471474\n",
      "====> Epoch: 1153 Average loss: 2.0626  reconstruction loss:  2.0620715758978134 contractive_loss:  5.033221141781965\n",
      "====> Epoch: 1154 Average loss: 1.6779  reconstruction loss:  1.6776936603166135 contractive_loss:  2.403648807010299\n",
      "====> Epoch: 1155 Average loss: 2.2986  reconstruction loss:  2.298363994056406 contractive_loss:  2.117588251908774\n",
      "====> Epoch: 1156 Average loss: 1.9164  reconstruction loss:  1.9161801059719665 contractive_loss:  1.8398607414599264\n",
      "====> Epoch: 1157 Average loss: 5.4423  reconstruction loss:  5.442126546892419 contractive_loss:  2.1889548390387805\n",
      "====> Epoch: 1158 Average loss: 4.6471  reconstruction loss:  4.646916577523651 contractive_loss:  1.5655205692646352\n",
      "====> Epoch: 1159 Average loss: 3.5104  reconstruction loss:  3.5101542122881244 contractive_loss:  2.275146590861778\n",
      "====> Epoch: 1160 Average loss: 3.2844  reconstruction loss:  3.2841172218542067 contractive_loss:  2.5488992270848367\n",
      "====> Epoch: 1161 Average loss: 2.9261  reconstruction loss:  2.925893289707334 contractive_loss:  2.1468877805625146\n",
      "====> Epoch: 1162 Average loss: 2.8138  reconstruction loss:  2.813607430463261 contractive_loss:  1.9941315197312257\n",
      "====> Epoch: 1163 Average loss: 2.6401  reconstruction loss:  2.6400446128891826 contractive_loss:  0.7991651732037646\n",
      "====> Epoch: 1164 Average loss: 2.6561  reconstruction loss:  2.6558428824637588 contractive_loss:  2.9506988253537183\n",
      "====> Epoch: 1165 Average loss: 2.5452  reconstruction loss:  2.5449301058494127 contractive_loss:  3.0611114525317293\n",
      "====> Epoch: 1166 Average loss: 2.3587  reconstruction loss:  2.3586310073192975 contractive_loss:  0.8208677875623075\n",
      "====> Epoch: 1167 Average loss: 2.8455  reconstruction loss:  2.845291937594311 contractive_loss:  2.176736313296893\n",
      "====> Epoch: 1168 Average loss: 2.4599  reconstruction loss:  2.4596890230820567 contractive_loss:  1.829752083839532\n",
      "====> Epoch: 1169 Average loss: 2.0938  reconstruction loss:  2.0933948467343213 contractive_loss:  3.615901289840595\n",
      "====> Epoch: 1170 Average loss: 2.2750  reconstruction loss:  2.2745990464276966 contractive_loss:  3.857219355303638\n",
      "====> Epoch: 1171 Average loss: 2.7342  reconstruction loss:  2.733926453227874 contractive_loss:  3.208737944208267\n",
      "====> Epoch: 1172 Average loss: 1.8470  reconstruction loss:  1.8467954187645625 contractive_loss:  1.9166186364859121\n",
      "====> Epoch: 1173 Average loss: 1.6342  reconstruction loss:  1.6340333198563064 contractive_loss:  1.8180034485521994\n",
      "====> Epoch: 1174 Average loss: 2.0932  reconstruction loss:  2.0929641929815763 contractive_loss:  2.007689196603831\n",
      "====> Epoch: 1175 Average loss: 1.9704  reconstruction loss:  1.9702635945220104 contractive_loss:  1.507303510030389\n",
      "====> Epoch: 1176 Average loss: 1.8621  reconstruction loss:  1.861899709236889 contractive_loss:  1.5591789653723753\n",
      "====> Epoch: 1177 Average loss: 1.7442  reconstruction loss:  1.7439565498050387 contractive_loss:  2.159144893226745\n",
      "====> Epoch: 1178 Average loss: 2.6076  reconstruction loss:  2.607507791148263 contractive_loss:  0.850395040926815\n",
      "====> Epoch: 1179 Average loss: 3.9097  reconstruction loss:  3.909512377716602 contractive_loss:  2.0833964397681792\n",
      "====> Epoch: 1180 Average loss: 3.8274  reconstruction loss:  3.8272699161189196 contractive_loss:  1.3352714961331447\n",
      "====> Epoch: 1181 Average loss: 3.2512  reconstruction loss:  3.2509777774157294 contractive_loss:  1.8999917659340888\n",
      "====> Epoch: 1182 Average loss: 2.9576  reconstruction loss:  2.9574399116479917 contractive_loss:  1.9273546315912553\n",
      "====> Epoch: 1183 Average loss: 2.8441  reconstruction loss:  2.844064082532051 contractive_loss:  0.7131573034948276\n",
      "====> Epoch: 1184 Average loss: 2.8610  reconstruction loss:  2.8609534068636413 contractive_loss:  0.7141142451813248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1185 Average loss: 2.6129  reconstruction loss:  2.6127875261065587 contractive_loss:  1.4093981620168277\n",
      "====> Epoch: 1186 Average loss: 3.2062  reconstruction loss:  3.2059615376979607 contractive_loss:  2.1421448156755907\n",
      "====> Epoch: 1187 Average loss: 3.4533  reconstruction loss:  3.4531039954078193 contractive_loss:  2.0576204870721853\n",
      "====> Epoch: 1188 Average loss: 3.5216  reconstruction loss:  3.5215376535644225 contractive_loss:  0.9380572112697289\n",
      "====> Epoch: 1189 Average loss: 2.6191  reconstruction loss:  2.6189827875674676 contractive_loss:  1.6335114377171236\n",
      "====> Epoch: 1190 Average loss: 3.2810  reconstruction loss:  3.2808297984737265 contractive_loss:  2.1509863814436123\n",
      "====> Epoch: 1191 Average loss: 3.0008  reconstruction loss:  3.0006276382494708 contractive_loss:  1.8412693313991848\n",
      "====> Epoch: 1192 Average loss: 2.8413  reconstruction loss:  2.841240001585553 contractive_loss:  0.8061421821866311\n",
      "====> Epoch: 1193 Average loss: 3.0759  reconstruction loss:  3.07569371293921 contractive_loss:  1.9061599455889167\n",
      "====> Epoch: 1194 Average loss: 3.4041  reconstruction loss:  3.403879411487225 contractive_loss:  2.0837049256791005\n",
      "====> Epoch: 1195 Average loss: 3.3034  reconstruction loss:  3.3031838674022715 contractive_loss:  1.7179147192054491\n",
      "====> Epoch: 1196 Average loss: 3.2722  reconstruction loss:  3.272028811759211 contractive_loss:  1.6251025397160526\n",
      "====> Epoch: 1197 Average loss: 3.8696  reconstruction loss:  3.869449980490586 contractive_loss:  1.2281003227871465\n",
      "====> Epoch: 1198 Average loss: 3.2469  reconstruction loss:  3.24673876600856 contractive_loss:  1.5762304219888115\n",
      "====> Epoch: 1199 Average loss: 3.5251  reconstruction loss:  3.5248894935561914 contractive_loss:  1.605786591809437\n",
      "====> Epoch: 1200 Average loss: 3.7862  reconstruction loss:  3.7860472917223205 contractive_loss:  1.299628638230595\n",
      "====> Epoch: 1201 Average loss: 4.5013  reconstruction loss:  4.501150098222718 contractive_loss:  1.1452811317692968\n",
      "====> Epoch: 1202 Average loss: 3.8911  reconstruction loss:  3.890975848736519 contractive_loss:  1.503041455177642\n",
      "====> Epoch: 1203 Average loss: 3.5070  reconstruction loss:  3.506868612288423 contractive_loss:  1.338265044241188\n",
      "====> Epoch: 1204 Average loss: 3.3995  reconstruction loss:  3.39933734728901 contractive_loss:  1.6472840027875892\n",
      "====> Epoch: 1205 Average loss: 2.8769  reconstruction loss:  2.876710994368935 contractive_loss:  1.6852500266818358\n",
      "====> Epoch: 1206 Average loss: 3.3268  reconstruction loss:  3.3266777347278196 contractive_loss:  1.2436658641626626\n",
      "====> Epoch: 1207 Average loss: 3.6401  reconstruction loss:  3.6399635015328817 contractive_loss:  1.7239665649478733\n",
      "====> Epoch: 1208 Average loss: 2.9217  reconstruction loss:  2.921539126687309 contractive_loss:  1.5034754134793065\n",
      "====> Epoch: 1209 Average loss: 3.7193  reconstruction loss:  3.7191176300766786 contractive_loss:  1.7709745862310982\n",
      "====> Epoch: 1210 Average loss: 3.2798  reconstruction loss:  3.279640356248818 contractive_loss:  1.7146833278457463\n",
      "====> Epoch: 1211 Average loss: 3.0299  reconstruction loss:  3.02975891101814 contractive_loss:  1.5285837055309202\n",
      "====> Epoch: 1212 Average loss: 2.3133  reconstruction loss:  2.3131917388922862 contractive_loss:  1.5744740136462585\n",
      "====> Epoch: 1213 Average loss: 2.2845  reconstruction loss:  2.2843512555014818 contractive_loss:  1.5226753593858589\n",
      "====> Epoch: 1214 Average loss: 2.3818  reconstruction loss:  2.3816840494625553 contractive_loss:  1.5329008012506036\n",
      "====> Epoch: 1215 Average loss: 2.0167  reconstruction loss:  2.0165654821277768 contractive_loss:  1.2343366568971506\n",
      "====> Epoch: 1216 Average loss: 1.8795  reconstruction loss:  1.8793472636489228 contractive_loss:  1.8200981950949326\n",
      "====> Epoch: 1217 Average loss: 2.0372  reconstruction loss:  2.0370653566165267 contractive_loss:  1.6183919237854099\n",
      "====> Epoch: 1218 Average loss: 2.7791  reconstruction loss:  2.7787074256939364 contractive_loss:  3.5448555132813153\n",
      "====> Epoch: 1219 Average loss: 1.8455  reconstruction loss:  1.8453000086746665 contractive_loss:  1.8674107132636937\n",
      "====> Epoch: 1220 Average loss: 1.8050  reconstruction loss:  1.804855852691419 contractive_loss:  1.9131420114284525\n",
      "====> Epoch: 1221 Average loss: 2.4401  reconstruction loss:  2.4398662375146247 contractive_loss:  1.8686557970050548\n",
      "====> Epoch: 1222 Average loss: 2.0342  reconstruction loss:  2.034030736217093 contractive_loss:  1.8841964072877182\n",
      "====> Epoch: 1223 Average loss: 2.2889  reconstruction loss:  2.288660025375805 contractive_loss:  1.9535220478562034\n",
      "====> Epoch: 1224 Average loss: 2.7329  reconstruction loss:  2.732431758060819 contractive_loss:  4.661601970700907\n",
      "====> Epoch: 1225 Average loss: 2.4072  reconstruction loss:  2.4068090934056006 contractive_loss:  4.404684747651754\n",
      "====> Epoch: 1226 Average loss: 1.8484  reconstruction loss:  1.8479499704494071 contractive_loss:  4.75059196772647\n",
      "====> Epoch: 1227 Average loss: 1.8128  reconstruction loss:  1.81240796722648 contractive_loss:  4.223829733312213\n",
      "====> Epoch: 1228 Average loss: 2.7505  reconstruction loss:  2.7500325154566885 contractive_loss:  4.884443314886855\n",
      "====> Epoch: 1229 Average loss: 1.9340  reconstruction loss:  1.9335301964964324 contractive_loss:  5.0711215980944475\n",
      "====> Epoch: 1230 Average loss: 3.1446  reconstruction loss:  3.1444008945901527 contractive_loss:  2.100050674882783\n",
      "====> Epoch: 1231 Average loss: 2.1258  reconstruction loss:  2.125566698247507 contractive_loss:  1.9179021407577668\n",
      "====> Epoch: 1232 Average loss: 1.7483  reconstruction loss:  1.7480476980009707 contractive_loss:  2.1664566789374637\n",
      "====> Epoch: 1233 Average loss: 2.6830  reconstruction loss:  2.682669773228865 contractive_loss:  3.2829291127710567\n",
      "====> Epoch: 1234 Average loss: 2.7669  reconstruction loss:  2.7668118151580954 contractive_loss:  1.2010170455001397\n",
      "====> Epoch: 1235 Average loss: 3.5192  reconstruction loss:  3.518922069818192 contractive_loss:  2.7079644515592456\n",
      "====> Epoch: 1236 Average loss: 4.3366  reconstruction loss:  4.336417477087681 contractive_loss:  1.4902313326973282\n",
      "====> Epoch: 1237 Average loss: 3.7700  reconstruction loss:  3.7698419299106174 contractive_loss:  1.4050322512044064\n",
      "====> Epoch: 1238 Average loss: 3.5669  reconstruction loss:  3.5666494156895254 contractive_loss:  2.050863883989843\n",
      "====> Epoch: 1239 Average loss: 3.0798  reconstruction loss:  3.0796308676770336 contractive_loss:  1.7406971312306754\n",
      "====> Epoch: 1240 Average loss: 3.4780  reconstruction loss:  3.4779811383259642 contractive_loss:  0.6574064626996672\n",
      "====> Epoch: 1241 Average loss: 2.8149  reconstruction loss:  2.814637027478923 contractive_loss:  2.339811356193806\n",
      "====> Epoch: 1242 Average loss: 2.9733  reconstruction loss:  2.9730328248683207 contractive_loss:  3.1562886293864425\n",
      "====> Epoch: 1243 Average loss: 2.9165  reconstruction loss:  2.9163870810567327 contractive_loss:  0.7310784330912597\n",
      "====> Epoch: 1244 Average loss: 3.2561  reconstruction loss:  3.2559429727039157 contractive_loss:  1.806462785091078\n",
      "====> Epoch: 1245 Average loss: 2.7437  reconstruction loss:  2.7434674857189214 contractive_loss:  2.1001001468070126\n",
      "====> Epoch: 1246 Average loss: 2.4064  reconstruction loss:  2.406302055142794 contractive_loss:  0.9664312763092502\n",
      "====> Epoch: 1247 Average loss: 3.0771  reconstruction loss:  3.076926801812424 contractive_loss:  2.1123972432272735\n",
      "====> Epoch: 1248 Average loss: 2.9851  reconstruction loss:  2.9848685201485603 contractive_loss:  2.476061390229297\n",
      "====> Epoch: 1249 Average loss: 2.7925  reconstruction loss:  2.792391495551487 contractive_loss:  1.0500858246459497\n",
      "====> Epoch: 1250 Average loss: 2.7070  reconstruction loss:  2.706750083428347 contractive_loss:  2.442005629202671\n",
      "====> Epoch: 1251 Average loss: 2.7435  reconstruction loss:  2.743241891003834 contractive_loss:  2.2746239828374577\n",
      "====> Epoch: 1252 Average loss: 2.1192  reconstruction loss:  2.119033555558739 contractive_loss:  2.1363865304444998\n",
      "====> Epoch: 1253 Average loss: 2.4127  reconstruction loss:  2.4124964911294167 contractive_loss:  1.6105202719746725\n",
      "====> Epoch: 1254 Average loss: 1.7882  reconstruction loss:  1.7880376356028491 contractive_loss:  2.0593666797609367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1255 Average loss: 2.0670  reconstruction loss:  2.0668267893322296 contractive_loss:  2.065824101812014\n",
      "====> Epoch: 1256 Average loss: 2.2647  reconstruction loss:  2.264570554211438 contractive_loss:  1.7428615673270516\n",
      "====> Epoch: 1257 Average loss: 1.6655  reconstruction loss:  1.6653042022696056 contractive_loss:  2.183556262974639\n",
      "====> Epoch: 1258 Average loss: 2.5335  reconstruction loss:  2.5333065569204773 contractive_loss:  1.809548676949081\n",
      "====> Epoch: 1259 Average loss: 1.8921  reconstruction loss:  1.8919451994410559 contractive_loss:  1.850787585551707\n",
      "====> Epoch: 1260 Average loss: 2.0403  reconstruction loss:  2.040110525758653 contractive_loss:  1.9134893788724303\n",
      "====> Epoch: 1261 Average loss: 2.7605  reconstruction loss:  2.7601223074181847 contractive_loss:  3.5207017118909025\n",
      "====> Epoch: 1262 Average loss: 2.4366  reconstruction loss:  2.43613337589583 contractive_loss:  4.545734855655561\n",
      "====> Epoch: 1263 Average loss: 2.5333  reconstruction loss:  2.5328889490139828 contractive_loss:  4.501259882048726\n",
      "====> Epoch: 1264 Average loss: 2.5959  reconstruction loss:  2.5953530282806425 contractive_loss:  5.067116692621778\n",
      "====> Epoch: 1265 Average loss: 2.3838  reconstruction loss:  2.3832867064717465 contractive_loss:  5.11823571403292\n",
      "====> Epoch: 1266 Average loss: 1.8066  reconstruction loss:  1.8060286295872408 contractive_loss:  5.602666464687271\n",
      "====> Epoch: 1267 Average loss: 2.5339  reconstruction loss:  2.533449211706032 contractive_loss:  4.20513752390952\n",
      "====> Epoch: 1268 Average loss: 1.7197  reconstruction loss:  1.71948154203014 contractive_loss:  2.3043476092729636\n",
      "====> Epoch: 1269 Average loss: 2.1454  reconstruction loss:  2.145144317310671 contractive_loss:  2.22749629806898\n",
      "====> Epoch: 1270 Average loss: 1.9804  reconstruction loss:  1.9801335507819267 contractive_loss:  3.1532188002213695\n",
      "====> Epoch: 1271 Average loss: 1.5762  reconstruction loss:  1.575578608881022 contractive_loss:  5.805201152974155\n",
      "====> Epoch: 1272 Average loss: 1.8134  reconstruction loss:  1.812851503601307 contractive_loss:  5.580235647893959\n",
      "====> Epoch: 1273 Average loss: 1.7808  reconstruction loss:  1.7803345399581159 contractive_loss:  5.0110288223864154\n",
      "====> Epoch: 1274 Average loss: 2.1515  reconstruction loss:  2.1510054733244344 contractive_loss:  5.2505037981746\n",
      "====> Epoch: 1275 Average loss: 1.4838  reconstruction loss:  1.4832071812387755 contractive_loss:  6.0568344736294195\n",
      "====> Epoch: 1276 Average loss: 2.4337  reconstruction loss:  2.4331236571500727 contractive_loss:  5.953662677293865\n",
      "====> Epoch: 1277 Average loss: 1.8644  reconstruction loss:  1.8640253739432284 contractive_loss:  3.4160366253502805\n",
      "====> Epoch: 1278 Average loss: 1.9141  reconstruction loss:  1.9138031594566496 contractive_loss:  2.6997874767524923\n",
      "====> Epoch: 1279 Average loss: 2.0212  reconstruction loss:  2.0209369320943176 contractive_loss:  2.452225357882181\n",
      "====> Epoch: 1280 Average loss: 2.0405  reconstruction loss:  2.0402091401170126 contractive_loss:  2.736362940240145\n",
      "====> Epoch: 1281 Average loss: 1.6515  reconstruction loss:  1.650899561882656 contractive_loss:  6.224450367554383\n",
      "====> Epoch: 1282 Average loss: 2.0161  reconstruction loss:  2.015466712200819 contractive_loss:  5.903404651890139\n",
      "====> Epoch: 1283 Average loss: 1.4835  reconstruction loss:  1.4828866740150115 contractive_loss:  6.111145848586698\n",
      "====> Epoch: 1284 Average loss: 1.4805  reconstruction loss:  1.4799702258021301 contractive_loss:  5.0228366694338\n",
      "====> Epoch: 1285 Average loss: 1.9429  reconstruction loss:  1.942284108011813 contractive_loss:  5.89571600236727\n",
      "====> Epoch: 1286 Average loss: 1.6583  reconstruction loss:  1.6577112328554617 contractive_loss:  6.380151729486766\n",
      "====> Epoch: 1287 Average loss: 2.5122  reconstruction loss:  2.5115874685538966 contractive_loss:  5.816933883202086\n",
      "====> Epoch: 1288 Average loss: 2.1651  reconstruction loss:  2.1646708424124914 contractive_loss:  4.481521707228678\n",
      "====> Epoch: 1289 Average loss: 1.8661  reconstruction loss:  1.865848786041486 contractive_loss:  2.842769774035612\n",
      "====> Epoch: 1290 Average loss: 1.8453  reconstruction loss:  1.8450099241295754 contractive_loss:  3.2715187629438165\n",
      "====> Epoch: 1291 Average loss: 2.0405  reconstruction loss:  2.0402508937914563 contractive_loss:  2.9165094559931086\n",
      "====> Epoch: 1292 Average loss: 1.7398  reconstruction loss:  1.7391282436752542 contractive_loss:  6.4277785807277485\n",
      "====> Epoch: 1293 Average loss: 1.6280  reconstruction loss:  1.6274138348516283 contractive_loss:  5.454279175366784\n",
      "====> Epoch: 1294 Average loss: 1.5550  reconstruction loss:  1.554293931755831 contractive_loss:  6.724682649607654\n",
      "====> Epoch: 1295 Average loss: 2.2713  reconstruction loss:  2.2706694488703305 contractive_loss:  6.269298869254515\n",
      "====> Epoch: 1296 Average loss: 2.6076  reconstruction loss:  2.6071887072120354 contractive_loss:  4.492594854326161\n",
      "====> Epoch: 1297 Average loss: 1.4948  reconstruction loss:  1.4944950103451147 contractive_loss:  2.768301262840219\n",
      "====> Epoch: 1298 Average loss: 1.3681  reconstruction loss:  1.3677676611818563 contractive_loss:  2.9779506053358022\n",
      "====> Epoch: 1299 Average loss: 1.8810  reconstruction loss:  1.8806897709263461 contractive_loss:  2.738428560228189\n",
      "====> Epoch: 1300 Average loss: 1.5562  reconstruction loss:  1.5559062722254766 contractive_loss:  2.795269553299085\n",
      "====> Epoch: 1301 Average loss: 1.9220  reconstruction loss:  1.9213276877358267 contractive_loss:  6.72840530173813\n",
      "====> Epoch: 1302 Average loss: 2.0871  reconstruction loss:  2.0864408174467495 contractive_loss:  6.331709498314563\n",
      "====> Epoch: 1303 Average loss: 1.5225  reconstruction loss:  1.521845895006929 contractive_loss:  6.9089066035809745\n",
      "====> Epoch: 1304 Average loss: 1.5713  reconstruction loss:  1.5706236774909894 contractive_loss:  7.032793766272977\n",
      "====> Epoch: 1305 Average loss: 1.4931  reconstruction loss:  1.4925716231821462 contractive_loss:  5.606531696008612\n",
      "====> Epoch: 1306 Average loss: 1.6441  reconstruction loss:  1.6436903222164552 contractive_loss:  4.501778454238646\n",
      "====> Epoch: 1307 Average loss: 1.3597  reconstruction loss:  1.359082545254748 contractive_loss:  5.813047211936398\n",
      "====> Epoch: 1308 Average loss: 2.1712  reconstruction loss:  2.170626760975279 contractive_loss:  5.306514319092361\n",
      "====> Epoch: 1309 Average loss: 2.3059  reconstruction loss:  2.30519410068736 contractive_loss:  6.665144030211238\n",
      "====> Epoch: 1310 Average loss: 2.2030  reconstruction loss:  2.202530846546482 contractive_loss:  4.37263728466392\n",
      "====> Epoch: 1311 Average loss: 2.0905  reconstruction loss:  2.0901857121467406 contractive_loss:  2.8793472226832826\n",
      "====> Epoch: 1312 Average loss: 1.4811  reconstruction loss:  1.480339331586868 contractive_loss:  7.218063627201291\n",
      "====> Epoch: 1313 Average loss: 2.0035  reconstruction loss:  2.0027997479498603 contractive_loss:  6.585042346001527\n",
      "====> Epoch: 1314 Average loss: 1.9650  reconstruction loss:  1.9643155794827059 contractive_loss:  6.8080878470669575\n",
      "model saved!\n",
      "====> Epoch: 1315 Average loss: 1.1060  reconstruction loss:  1.1056339479136335 contractive_loss:  3.4192926480306336\n",
      "====> Epoch: 1316 Average loss: 1.1240  reconstruction loss:  1.1238924980480727 contractive_loss:  1.5449026645017636\n",
      "model saved!\n",
      "====> Epoch: 1317 Average loss: 0.9428  reconstruction loss:  0.9424287604849062 contractive_loss:  3.747569127555528\n",
      "====> Epoch: 1318 Average loss: 1.5937  reconstruction loss:  1.593029461267185 contractive_loss:  6.6097246205649665\n",
      "====> Epoch: 1319 Average loss: 1.9575  reconstruction loss:  1.9568125272204728 contractive_loss:  6.7770986046494\n",
      "====> Epoch: 1320 Average loss: 1.9616  reconstruction loss:  1.9612456951336354 contractive_loss:  3.0504703193463887\n",
      "====> Epoch: 1321 Average loss: 1.5575  reconstruction loss:  1.5572002788076147 contractive_loss:  3.1060356745197732\n",
      "====> Epoch: 1322 Average loss: 2.1027  reconstruction loss:  2.102165170633555 contractive_loss:  5.232531429740919\n",
      "====> Epoch: 1323 Average loss: 1.9589  reconstruction loss:  1.9586137067451437 contractive_loss:  2.9992050081386457\n",
      "====> Epoch: 1324 Average loss: 1.7488  reconstruction loss:  1.748527855059966 contractive_loss:  2.951340278714932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1325 Average loss: 2.2530  reconstruction loss:  2.2526844997242783 contractive_loss:  2.8985393121751493\n",
      "====> Epoch: 1326 Average loss: 2.4340  reconstruction loss:  2.4334655112207897 contractive_loss:  5.646995420943512\n",
      "====> Epoch: 1327 Average loss: 2.2128  reconstruction loss:  2.212285964334572 contractive_loss:  5.32222937936526\n",
      "====> Epoch: 1328 Average loss: 2.5095  reconstruction loss:  2.508978367452743 contractive_loss:  4.721074005089989\n",
      "====> Epoch: 1329 Average loss: 2.5891  reconstruction loss:  2.5885987658597456 contractive_loss:  5.195703689457172\n",
      "====> Epoch: 1330 Average loss: 1.5963  reconstruction loss:  1.5960009929989332 contractive_loss:  3.100487527503102\n",
      "====> Epoch: 1331 Average loss: 1.8348  reconstruction loss:  1.8344426795171205 contractive_loss:  3.1320427375493316\n",
      "====> Epoch: 1332 Average loss: 2.1099  reconstruction loss:  2.1095112931190667 contractive_loss:  4.225736704931016\n",
      "====> Epoch: 1333 Average loss: 1.8973  reconstruction loss:  1.8968866030943323 contractive_loss:  4.101966712184207\n",
      "====> Epoch: 1334 Average loss: 1.7546  reconstruction loss:  1.7542536906400215 contractive_loss:  3.723186940844364\n",
      "====> Epoch: 1335 Average loss: 2.0041  reconstruction loss:  2.0035252542281032 contractive_loss:  5.69639399942482\n",
      "====> Epoch: 1336 Average loss: 1.3866  reconstruction loss:  1.3858050912818813 contractive_loss:  7.609182265276365\n",
      "====> Epoch: 1337 Average loss: 1.2019  reconstruction loss:  1.2016503020560803 contractive_loss:  2.4209446443800906\n",
      "====> Epoch: 1338 Average loss: 1.1998  reconstruction loss:  1.1996337910126575 contractive_loss:  1.5448049575618352\n",
      "====> Epoch: 1339 Average loss: 1.2549  reconstruction loss:  1.2547506221437585 contractive_loss:  1.8062449972747776\n",
      "====> Epoch: 1340 Average loss: 1.0787  reconstruction loss:  1.0786034448508364 contractive_loss:  1.4573983007267375\n",
      "====> Epoch: 1341 Average loss: 1.0796  reconstruction loss:  1.0794051388047339 contractive_loss:  1.9759224271665372\n",
      "====> Epoch: 1342 Average loss: 1.2338  reconstruction loss:  1.2336466783286506 contractive_loss:  1.6634966190739136\n",
      "====> Epoch: 1343 Average loss: 1.0490  reconstruction loss:  1.04856147756448 contractive_loss:  4.212640876667168\n",
      "====> Epoch: 1344 Average loss: 2.6945  reconstruction loss:  2.694348624943767 contractive_loss:  1.8413810342651893\n",
      "====> Epoch: 1345 Average loss: 2.3523  reconstruction loss:  2.3518698610218314 contractive_loss:  4.104224163546976\n",
      "====> Epoch: 1346 Average loss: 2.3676  reconstruction loss:  2.367213254387141 contractive_loss:  3.9415710379340494\n",
      "====> Epoch: 1347 Average loss: 1.8240  reconstruction loss:  1.823669132855378 contractive_loss:  3.477238824016488\n",
      "====> Epoch: 1348 Average loss: 1.1864  reconstruction loss:  1.1857871006617564 contractive_loss:  6.548487189274234\n",
      "====> Epoch: 1349 Average loss: 1.3103  reconstruction loss:  1.3100645682100005 contractive_loss:  2.3493808789561186\n",
      "====> Epoch: 1350 Average loss: 1.0932  reconstruction loss:  1.0930532617189919 contractive_loss:  1.1275825177240302\n",
      "====> Epoch: 1351 Average loss: 2.3672  reconstruction loss:  2.3667647936622136 contractive_loss:  4.326179921009684\n",
      "====> Epoch: 1352 Average loss: 1.4670  reconstruction loss:  1.4667904405400274 contractive_loss:  1.713223933868106\n",
      "====> Epoch: 1353 Average loss: 1.7146  reconstruction loss:  1.7136414693139665 contractive_loss:  10.019904577404136\n",
      "====> Epoch: 1354 Average loss: 2.2962  reconstruction loss:  2.295651931641564 contractive_loss:  5.564690498988195\n",
      "====> Epoch: 1355 Average loss: 1.9682  reconstruction loss:  1.9677808358660804 contractive_loss:  4.404987372208996\n",
      "====> Epoch: 1356 Average loss: 1.6191  reconstruction loss:  1.6186616448704398 contractive_loss:  4.770566560730524\n",
      "====> Epoch: 1357 Average loss: 2.3104  reconstruction loss:  2.30991308751843 contractive_loss:  4.951826406924093\n",
      "====> Epoch: 1358 Average loss: 1.7072  reconstruction loss:  1.7067220193397303 contractive_loss:  4.354472810381724\n",
      "====> Epoch: 1359 Average loss: 1.8582  reconstruction loss:  1.8577827985221318 contractive_loss:  4.340918678794199\n",
      "====> Epoch: 1360 Average loss: 2.3520  reconstruction loss:  2.3512841857753575 contractive_loss:  6.892189955580226\n",
      "====> Epoch: 1361 Average loss: 2.0576  reconstruction loss:  2.0571171817285228 contractive_loss:  5.085330470623549\n",
      "====> Epoch: 1362 Average loss: 1.5159  reconstruction loss:  1.515411144386048 contractive_loss:  4.540010472224327\n",
      "====> Epoch: 1363 Average loss: 2.2426  reconstruction loss:  2.2417188502283567 contractive_loss:  9.225633740562762\n",
      "====> Epoch: 1364 Average loss: 1.6684  reconstruction loss:  1.6673687860777593 contractive_loss:  10.58650416383915\n",
      "====> Epoch: 1365 Average loss: 1.9641  reconstruction loss:  1.9632014855074578 contractive_loss:  9.4089250287866\n",
      "====> Epoch: 1366 Average loss: 1.1547  reconstruction loss:  1.1544910210763588 contractive_loss:  2.13948346472676\n",
      "====> Epoch: 1367 Average loss: 1.1280  reconstruction loss:  1.1276494083893656 contractive_loss:  3.540485942636359\n",
      "====> Epoch: 1368 Average loss: 1.7136  reconstruction loss:  1.7133083043106427 contractive_loss:  2.9487319349283303\n",
      "model saved!\n",
      "====> Epoch: 1369 Average loss: 0.8856  reconstruction loss:  0.8851612718271139 contractive_loss:  4.663464501266802\n",
      "====> Epoch: 1370 Average loss: 1.1793  reconstruction loss:  1.1790627231959254 contractive_loss:  2.0640021621113545\n",
      "====> Epoch: 1371 Average loss: 1.2220  reconstruction loss:  1.221505382618039 contractive_loss:  4.940052268838438\n",
      "====> Epoch: 1372 Average loss: 2.0334  reconstruction loss:  2.0324109328861075 contractive_loss:  9.592945190728189\n",
      "====> Epoch: 1373 Average loss: 1.9396  reconstruction loss:  1.9385508961613034 contractive_loss:  10.038729157920779\n",
      "model saved!\n",
      "====> Epoch: 1374 Average loss: 0.8823  reconstruction loss:  0.8815675808965644 contractive_loss:  7.748568498676239\n",
      "====> Epoch: 1375 Average loss: 0.9354  reconstruction loss:  0.9346410450448291 contractive_loss:  7.347685019889843\n",
      "====> Epoch: 1376 Average loss: 1.5267  reconstruction loss:  1.525675755114004 contractive_loss:  10.194274711689413\n",
      "====> Epoch: 1377 Average loss: 1.4202  reconstruction loss:  1.419107012835117 contractive_loss:  10.533348838328005\n",
      "====> Epoch: 1378 Average loss: 1.8676  reconstruction loss:  1.8666320656692414 contractive_loss:  9.500431550033847\n",
      "====> Epoch: 1379 Average loss: 2.3110  reconstruction loss:  2.310043587308547 contractive_loss:  9.92701625673317\n",
      "====> Epoch: 1380 Average loss: 2.0117  reconstruction loss:  2.01078459168581 contractive_loss:  9.157399754099451\n",
      "====> Epoch: 1381 Average loss: 1.3368  reconstruction loss:  1.3362291709363492 contractive_loss:  5.680471908493914\n",
      "====> Epoch: 1382 Average loss: 1.6558  reconstruction loss:  1.6556221572678824 contractive_loss:  1.8069530223368702\n",
      "====> Epoch: 1383 Average loss: 1.1889  reconstruction loss:  1.1885512555347395 contractive_loss:  3.6512568314742238\n",
      "====> Epoch: 1384 Average loss: 1.4098  reconstruction loss:  1.4095655181867102 contractive_loss:  2.0268583410429137\n",
      "====> Epoch: 1385 Average loss: 1.9292  reconstruction loss:  1.9287964277170073 contractive_loss:  3.7622621130893923\n",
      "====> Epoch: 1386 Average loss: 0.8902  reconstruction loss:  0.889740141813729 contractive_loss:  4.6222261182834306\n",
      "====> Epoch: 1387 Average loss: 0.9686  reconstruction loss:  0.9682915223392513 contractive_loss:  2.6487691514767415\n",
      "====> Epoch: 1388 Average loss: 0.9353  reconstruction loss:  0.9350342434471551 contractive_loss:  2.5500272924504466\n",
      "====> Epoch: 1389 Average loss: 0.9279  reconstruction loss:  0.9274346819230199 contractive_loss:  4.351192915872014\n",
      "====> Epoch: 1390 Average loss: 1.0428  reconstruction loss:  1.042292152054422 contractive_loss:  4.9698650520441365\n",
      "====> Epoch: 1391 Average loss: 2.0081  reconstruction loss:  2.0076753321650713 contractive_loss:  4.230490699711957\n",
      "====> Epoch: 1392 Average loss: 1.2550  reconstruction loss:  1.2547563230795247 contractive_loss:  2.760551818635374\n",
      "model saved!\n",
      "====> Epoch: 1393 Average loss: 0.8295  reconstruction loss:  0.8289805149757542 contractive_loss:  5.228316310426802\n",
      "====> Epoch: 1394 Average loss: 1.0849  reconstruction loss:  1.0847574287379473 contractive_loss:  1.7642705772212326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1395 Average loss: 1.5183  reconstruction loss:  1.5180873351449293 contractive_loss:  1.7967297149121018\n",
      "====> Epoch: 1396 Average loss: 1.1373  reconstruction loss:  1.1370813282749186 contractive_loss:  1.8879157182387656\n",
      "====> Epoch: 1397 Average loss: 1.2878  reconstruction loss:  1.2869231648284067 contractive_loss:  8.81828516515218\n",
      "====> Epoch: 1398 Average loss: 2.2026  reconstruction loss:  2.201550112638432 contractive_loss:  10.536573153808554\n",
      "====> Epoch: 1399 Average loss: 1.8918  reconstruction loss:  1.8906753019892395 contractive_loss:  11.225226316488394\n",
      "====> Epoch: 1400 Average loss: 1.3164  reconstruction loss:  1.3162541185778573 contractive_loss:  1.6172539861568396\n",
      "====> Epoch: 1401 Average loss: 3.4893  reconstruction loss:  3.4885593547563247 contractive_loss:  7.161865588786543\n",
      "====> Epoch: 1402 Average loss: 2.2855  reconstruction loss:  2.2847441547303866 contractive_loss:  7.438018782110941\n",
      "====> Epoch: 1403 Average loss: 1.7053  reconstruction loss:  1.7045694551248385 contractive_loss:  7.546847982496157\n",
      "====> Epoch: 1404 Average loss: 1.5131  reconstruction loss:  1.5123038059668432 contractive_loss:  7.721999750051552\n",
      "====> Epoch: 1405 Average loss: 1.0941  reconstruction loss:  1.093360710026638 contractive_loss:  7.8403904893954\n",
      "====> Epoch: 1406 Average loss: 1.0871  reconstruction loss:  1.0863533800655227 contractive_loss:  7.878407092829203\n",
      "====> Epoch: 1407 Average loss: 1.1004  reconstruction loss:  1.0996807503575246 contractive_loss:  7.589851703392605\n",
      "====> Epoch: 1408 Average loss: 1.2362  reconstruction loss:  1.2359463339349734 contractive_loss:  2.563263046296811\n",
      "====> Epoch: 1409 Average loss: 1.4494  reconstruction loss:  1.4485941112230476 contractive_loss:  8.110375475525492\n",
      "====> Epoch: 1410 Average loss: 1.4328  reconstruction loss:  1.4319544787168461 contractive_loss:  8.561906175281202\n",
      "====> Epoch: 1411 Average loss: 1.4166  reconstruction loss:  1.4157686182492768 contractive_loss:  7.888885448141298\n",
      "====> Epoch: 1412 Average loss: 2.0414  reconstruction loss:  2.0408552649936773 contractive_loss:  5.8571985861759694\n",
      "====> Epoch: 1413 Average loss: 1.1805  reconstruction loss:  1.1803423363260608 contractive_loss:  1.6652996298908864\n",
      "====> Epoch: 1414 Average loss: 1.5697  reconstruction loss:  1.5695050524884773 contractive_loss:  1.8045724507083698\n",
      "====> Epoch: 1415 Average loss: 1.3912  reconstruction loss:  1.3908831609370167 contractive_loss:  2.813992248491413\n",
      "====> Epoch: 1416 Average loss: 1.1162  reconstruction loss:  1.1153806241119915 contractive_loss:  8.43759524127954\n",
      "====> Epoch: 1417 Average loss: 0.9473  reconstruction loss:  0.9464243071458849 contractive_loss:  8.297507955979508\n",
      "====> Epoch: 1418 Average loss: 1.8257  reconstruction loss:  1.8251743623290237 contractive_loss:  4.7892780317621\n",
      "====> Epoch: 1419 Average loss: 0.9136  reconstruction loss:  0.9133827241420929 contractive_loss:  2.1097294313142125\n",
      "====> Epoch: 1420 Average loss: 1.2251  reconstruction loss:  1.2244393712329391 contractive_loss:  6.210088387082925\n",
      "====> Epoch: 1421 Average loss: 2.0037  reconstruction loss:  2.003104177999717 contractive_loss:  5.539526428317604\n",
      "====> Epoch: 1422 Average loss: 0.8943  reconstruction loss:  0.8941110431954128 contractive_loss:  1.8967859215419194\n",
      "====> Epoch: 1423 Average loss: 1.6622  reconstruction loss:  1.6615804962235472 contractive_loss:  6.492290133082926\n",
      "====> Epoch: 1424 Average loss: 1.6736  reconstruction loss:  1.6728389663661998 contractive_loss:  7.929264314345304\n",
      "====> Epoch: 1425 Average loss: 1.9114  reconstruction loss:  1.9106507668821644 contractive_loss:  7.935189230866807\n",
      "====> Epoch: 1426 Average loss: 1.1411  reconstruction loss:  1.1402454541207772 contractive_loss:  8.097654724613887\n",
      "====> Epoch: 1427 Average loss: 1.9517  reconstruction loss:  1.9511069302463977 contractive_loss:  5.887605710411261\n",
      "====> Epoch: 1428 Average loss: 1.5160  reconstruction loss:  1.515667500495874 contractive_loss:  2.906680889842485\n",
      "====> Epoch: 1429 Average loss: 1.2661  reconstruction loss:  1.2659924331144676 contractive_loss:  1.275406723328041\n",
      "====> Epoch: 1430 Average loss: 1.8557  reconstruction loss:  1.8553926143627848 contractive_loss:  2.62559400516876\n",
      "====> Epoch: 1431 Average loss: 1.2086  reconstruction loss:  1.2084706272394803 contractive_loss:  1.1414279103470173\n",
      "====> Epoch: 1432 Average loss: 1.7840  reconstruction loss:  1.783906363334453 contractive_loss:  1.1529318264668824\n",
      "====> Epoch: 1433 Average loss: 1.7092  reconstruction loss:  1.7088125098874403 contractive_loss:  4.067581004997689\n",
      "====> Epoch: 1434 Average loss: 1.7958  reconstruction loss:  1.7953050536071589 contractive_loss:  4.897161783143988\n",
      "====> Epoch: 1435 Average loss: 1.9989  reconstruction loss:  1.9984588926173847 contractive_loss:  4.806451052581289\n",
      "====> Epoch: 1436 Average loss: 1.5004  reconstruction loss:  1.4997663163971766 contractive_loss:  6.347698390335936\n",
      "====> Epoch: 1437 Average loss: 1.1759  reconstruction loss:  1.1751210131826186 contractive_loss:  8.257991172650112\n",
      "====> Epoch: 1438 Average loss: 1.5442  reconstruction loss:  1.5433463335546944 contractive_loss:  8.736444646209193\n",
      "====> Epoch: 1439 Average loss: 1.6226  reconstruction loss:  1.621766867688528 contractive_loss:  8.577524047509549\n",
      "====> Epoch: 1440 Average loss: 1.3030  reconstruction loss:  1.3026454827321843 contractive_loss:  3.5912204059434267\n",
      "====> Epoch: 1441 Average loss: 1.6775  reconstruction loss:  1.6772771735168521 contractive_loss:  2.0371168060955522\n",
      "====> Epoch: 1442 Average loss: 1.3052  reconstruction loss:  1.3049488900395605 contractive_loss:  2.748626160946248\n",
      "====> Epoch: 1443 Average loss: 1.2242  reconstruction loss:  1.2239474275913886 contractive_loss:  2.9011738171129977\n",
      "====> Epoch: 1444 Average loss: 1.7715  reconstruction loss:  1.7709724322886191 contractive_loss:  5.467524112371227\n",
      "====> Epoch: 1445 Average loss: 1.5100  reconstruction loss:  1.509631331398565 contractive_loss:  3.4578840001611315\n",
      "====> Epoch: 1446 Average loss: 1.1513  reconstruction loss:  1.1511220725187485 contractive_loss:  1.6848056192492764\n",
      "====> Epoch: 1447 Average loss: 1.3774  reconstruction loss:  1.3767561858308914 contractive_loss:  6.132060465049808\n",
      "====> Epoch: 1448 Average loss: 1.0059  reconstruction loss:  1.00555936657838 contractive_loss:  3.6299274856529746\n",
      "====> Epoch: 1449 Average loss: 0.9546  reconstruction loss:  0.9543317893838064 contractive_loss:  2.5670211178274243\n",
      "====> Epoch: 1450 Average loss: 1.3989  reconstruction loss:  1.397370608148216 contractive_loss:  15.677794925396338\n",
      "====> Epoch: 1451 Average loss: 2.0008  reconstruction loss:  1.9992910074681083 contractive_loss:  15.362358813039185\n",
      "====> Epoch: 1452 Average loss: 1.6888  reconstruction loss:  1.6874976038734675 contractive_loss:  13.445691286620097\n",
      "====> Epoch: 1453 Average loss: 1.5783  reconstruction loss:  1.577622272546444 contractive_loss:  6.987955381484675\n",
      "====> Epoch: 1454 Average loss: 2.0703  reconstruction loss:  2.069015060125331 contractive_loss:  12.407036008930499\n",
      "====> Epoch: 1455 Average loss: 0.9158  reconstruction loss:  0.9149938078696996 contractive_loss:  8.282535009225219\n",
      "====> Epoch: 1456 Average loss: 0.9517  reconstruction loss:  0.951377128635259 contractive_loss:  2.86229993080416\n",
      "====> Epoch: 1457 Average loss: 1.6765  reconstruction loss:  1.6750312446098712 contractive_loss:  15.11939969589537\n",
      "====> Epoch: 1458 Average loss: 1.9082  reconstruction loss:  1.9069141560335299 contractive_loss:  13.320321933664342\n",
      "====> Epoch: 1459 Average loss: 0.9408  reconstruction loss:  0.9396796736160673 contractive_loss:  11.396440263338267\n",
      "====> Epoch: 1460 Average loss: 1.1810  reconstruction loss:  1.1807100707273075 contractive_loss:  2.5063459745013383\n",
      "====> Epoch: 1461 Average loss: 1.2565  reconstruction loss:  1.255937756667394 contractive_loss:  5.960734742665978\n",
      "====> Epoch: 1462 Average loss: 1.4944  reconstruction loss:  1.493515616798348 contractive_loss:  9.309550372736847\n",
      "====> Epoch: 1463 Average loss: 1.4275  reconstruction loss:  1.426655033650535 contractive_loss:  8.840870212027268\n",
      "====> Epoch: 1464 Average loss: 2.4357  reconstruction loss:  2.434546921982957 contractive_loss:  11.800652169311547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1465 Average loss: 1.3892  reconstruction loss:  1.3882608948635373 contractive_loss:  9.265299917268527\n",
      "====> Epoch: 1466 Average loss: 1.9076  reconstruction loss:  1.9066718454419829 contractive_loss:  9.142323701735421\n",
      "====> Epoch: 1467 Average loss: 1.1738  reconstruction loss:  1.1728441358094002 contractive_loss:  9.646211973469308\n",
      "====> Epoch: 1468 Average loss: 1.1294  reconstruction loss:  1.128425647640046 contractive_loss:  10.05711644237337\n",
      "====> Epoch: 1469 Average loss: 1.2168  reconstruction loss:  1.2158772160389342 contractive_loss:  9.295617373589648\n",
      "====> Epoch: 1470 Average loss: 1.7362  reconstruction loss:  1.735552590476635 contractive_loss:  6.1679643428579425\n",
      "====> Epoch: 1471 Average loss: 1.5537  reconstruction loss:  1.5531258540309656 contractive_loss:  5.25693264035922\n",
      "====> Epoch: 1472 Average loss: 1.0190  reconstruction loss:  1.0180310963556254 contractive_loss:  10.169653193181968\n",
      "====> Epoch: 1473 Average loss: 1.4496  reconstruction loss:  1.4485936907642651 contractive_loss:  10.049225539286022\n",
      "====> Epoch: 1474 Average loss: 1.1422  reconstruction loss:  1.1411403877870212 contractive_loss:  10.151917172828524\n",
      "====> Epoch: 1475 Average loss: 1.5034  reconstruction loss:  1.502358245749484 contractive_loss:  10.080565326864988\n",
      "====> Epoch: 1476 Average loss: 1.4404  reconstruction loss:  1.4394480002150802 contractive_loss:  9.517079040016602\n",
      "====> Epoch: 1477 Average loss: 1.6676  reconstruction loss:  1.6666096181008445 contractive_loss:  9.652008873557074\n",
      "====> Epoch: 1478 Average loss: 1.3910  reconstruction loss:  1.3900078123756114 contractive_loss:  10.406486847953243\n",
      "====> Epoch: 1479 Average loss: 1.0998  reconstruction loss:  1.0988616940122065 contractive_loss:  9.73611916819949\n",
      "====> Epoch: 1480 Average loss: 1.3943  reconstruction loss:  1.3933135256539595 contractive_loss:  9.626635802926415\n",
      "====> Epoch: 1481 Average loss: 0.9417  reconstruction loss:  0.9406696668305204 contractive_loss:  10.095558639139432\n",
      "====> Epoch: 1482 Average loss: 1.2134  reconstruction loss:  1.2123788193783878 contractive_loss:  10.02125019112281\n",
      "====> Epoch: 1483 Average loss: 0.9448  reconstruction loss:  0.9437729339330019 contractive_loss:  10.255511068670312\n",
      "====> Epoch: 1484 Average loss: 1.7441  reconstruction loss:  1.742795731213019 contractive_loss:  12.58281179790837\n",
      "====> Epoch: 1485 Average loss: 1.6197  reconstruction loss:  1.6182867188169416 contractive_loss:  14.268862560039606\n",
      "====> Epoch: 1486 Average loss: 2.5322  reconstruction loss:  2.530489551631127 contractive_loss:  17.198153100384758\n",
      "====> Epoch: 1487 Average loss: 1.8960  reconstruction loss:  1.8940724263829622 contractive_loss:  19.634130491227378\n",
      "====> Epoch: 1488 Average loss: 1.5556  reconstruction loss:  1.5539943892014094 contractive_loss:  15.813485549544776\n",
      "====> Epoch: 1489 Average loss: 1.3553  reconstruction loss:  1.353580545149072 contractive_loss:  16.818844462455605\n",
      "====> Epoch: 1490 Average loss: 1.3575  reconstruction loss:  1.3555632874058121 contractive_loss:  19.051111250151582\n",
      "====> Epoch: 1491 Average loss: 0.9514  reconstruction loss:  0.94946342131806 contractive_loss:  19.42819604698714\n",
      "====> Epoch: 1492 Average loss: 1.1152  reconstruction loss:  1.1132432781316932 contractive_loss:  19.100655853905526\n",
      "====> Epoch: 1493 Average loss: 1.9085  reconstruction loss:  1.906748664792337 contractive_loss:  17.15274289646873\n",
      "====> Epoch: 1494 Average loss: 1.2729  reconstruction loss:  1.2709842947823606 contractive_loss:  19.21713767672376\n",
      "====> Epoch: 1495 Average loss: 2.3367  reconstruction loss:  2.3348275754018624 contractive_loss:  19.025774325105438\n",
      "====> Epoch: 1496 Average loss: 2.2718  reconstruction loss:  2.2692743184447806 contractive_loss:  24.789253666670035\n",
      "====> Epoch: 1497 Average loss: 2.4233  reconstruction loss:  2.4207579519830897 contractive_loss:  25.08773329879699\n",
      "====> Epoch: 1498 Average loss: 1.8447  reconstruction loss:  1.8419962913571153 contractive_loss:  26.566257812567923\n",
      "====> Epoch: 1499 Average loss: 1.9512  reconstruction loss:  1.9486099611983414 contractive_loss:  25.87363098889281\n",
      "====> Epoch: 1500 Average loss: 1.6799  reconstruction loss:  1.6775589741402572 contractive_loss:  23.7828482143934\n",
      "====> Epoch: 1501 Average loss: 1.5975  reconstruction loss:  1.595098969891884 contractive_loss:  23.80412449644455\n",
      "====> Epoch: 1502 Average loss: 0.8949  reconstruction loss:  0.8922713283427645 contractive_loss:  26.375353144790687\n",
      "model saved!\n",
      "====> Epoch: 1503 Average loss: 0.8237  reconstruction loss:  0.8210754932182863 contractive_loss:  26.336469205344848\n",
      "====> Epoch: 1504 Average loss: 1.5282  reconstruction loss:  1.5253717737040975 contractive_loss:  28.592087924671045\n",
      "====> Epoch: 1505 Average loss: 2.2614  reconstruction loss:  2.258635884057055 contractive_loss:  27.84669525684293\n",
      "====> Epoch: 1506 Average loss: 2.3008  reconstruction loss:  2.298040052920968 contractive_loss:  27.576448878804296\n",
      "====> Epoch: 1507 Average loss: 1.7166  reconstruction loss:  1.7140033858937875 contractive_loss:  26.077743045616725\n",
      "====> Epoch: 1508 Average loss: 1.6265  reconstruction loss:  1.6236010307502422 contractive_loss:  29.249612872545544\n",
      "====> Epoch: 1509 Average loss: 1.3187  reconstruction loss:  1.3158839430607312 contractive_loss:  28.466679591876826\n",
      "====> Epoch: 1510 Average loss: 1.1580  reconstruction loss:  1.1551042080501766 contractive_loss:  29.141129308249592\n",
      "====> Epoch: 1511 Average loss: 1.0974  reconstruction loss:  1.095000556142906 contractive_loss:  23.955687254172517\n",
      "====> Epoch: 1512 Average loss: 0.9637  reconstruction loss:  0.9624477213360074 contractive_loss:  12.816053582286798\n",
      "====> Epoch: 1513 Average loss: 1.9901  reconstruction loss:  1.9879430191089653 contractive_loss:  21.94512050334551\n",
      "====> Epoch: 1514 Average loss: 1.9235  reconstruction loss:  1.9214513952751024 contractive_loss:  20.92179345282848\n",
      "====> Epoch: 1515 Average loss: 1.4366  reconstruction loss:  1.4343929270388378 contractive_loss:  21.79949868773532\n",
      "====> Epoch: 1516 Average loss: 1.7956  reconstruction loss:  1.7935458572456457 contractive_loss:  20.139549643268474\n",
      "====> Epoch: 1517 Average loss: 1.3934  reconstruction loss:  1.3914983168917212 contractive_loss:  18.88241256186796\n",
      "====> Epoch: 1518 Average loss: 1.7222  reconstruction loss:  1.7199175534945277 contractive_loss:  22.404621139210573\n",
      "====> Epoch: 1519 Average loss: 1.6032  reconstruction loss:  1.6011080756422138 contractive_loss:  20.780480380453046\n",
      "====> Epoch: 1520 Average loss: 1.9132  reconstruction loss:  1.9103927010997455 contractive_loss:  27.625371222339506\n",
      "====> Epoch: 1521 Average loss: 2.1690  reconstruction loss:  2.1661214038666343 contractive_loss:  28.529645021570143\n",
      "====> Epoch: 1522 Average loss: 1.9503  reconstruction loss:  1.947580075371024 contractive_loss:  27.24499311443642\n",
      "model saved!\n",
      "====> Epoch: 1523 Average loss: 0.5352  reconstruction loss:  0.5323919377916111 contractive_loss:  28.41707180739388\n",
      "====> Epoch: 1524 Average loss: 0.6027  reconstruction loss:  0.6006696157772681 contractive_loss:  20.0983076938221\n",
      "====> Epoch: 1525 Average loss: 0.7063  reconstruction loss:  0.7054549565081755 contractive_loss:  8.091500059361818\n",
      "model saved!\n",
      "====> Epoch: 1526 Average loss: 0.4953  reconstruction loss:  0.49442868151344427 contractive_loss:  8.236776507817742\n",
      "====> Epoch: 1527 Average loss: 0.5947  reconstruction loss:  0.594061583184166 contractive_loss:  6.258020390628347\n",
      "====> Epoch: 1528 Average loss: 0.6699  reconstruction loss:  0.6692151520290841 contractive_loss:  6.574914133063376\n",
      "====> Epoch: 1529 Average loss: 1.0086  reconstruction loss:  1.007746693081213 contractive_loss:  8.980776665967802\n",
      "====> Epoch: 1530 Average loss: 1.4189  reconstruction loss:  1.4182765838838207 contractive_loss:  6.5381110829446625\n",
      "====> Epoch: 1531 Average loss: 0.7432  reconstruction loss:  0.7422066096418921 contractive_loss:  9.66694260251184\n",
      "====> Epoch: 1532 Average loss: 0.6492  reconstruction loss:  0.6471246777112168 contractive_loss:  20.360798135088928\n",
      "====> Epoch: 1533 Average loss: 0.6411  reconstruction loss:  0.6388203833271288 contractive_loss:  23.094565130841712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1534 Average loss: 1.9619  reconstruction loss:  1.9588879854907089 contractive_loss:  30.20952114999197\n",
      "====> Epoch: 1535 Average loss: 1.5890  reconstruction loss:  1.5861953842788 contractive_loss:  28.174666886152266\n",
      "====> Epoch: 1536 Average loss: 1.7576  reconstruction loss:  1.7545187416568284 contractive_loss:  30.487730310467775\n",
      "====> Epoch: 1537 Average loss: 1.3237  reconstruction loss:  1.3223367499856606 contractive_loss:  13.880989163626705\n",
      "====> Epoch: 1538 Average loss: 1.1056  reconstruction loss:  1.1054441858490738 contractive_loss:  1.0860166276456642\n",
      "====> Epoch: 1539 Average loss: 0.8039  reconstruction loss:  0.803652128744969 contractive_loss:  2.5982437586494362\n",
      "====> Epoch: 1540 Average loss: 1.9559  reconstruction loss:  1.9556593309072168 contractive_loss:  2.426580025806154\n",
      "====> Epoch: 1541 Average loss: 1.0652  reconstruction loss:  1.0650849219502756 contractive_loss:  1.020531079054531\n",
      "====> Epoch: 1542 Average loss: 0.6876  reconstruction loss:  0.6874297348674148 contractive_loss:  1.4870347968411044\n",
      "====> Epoch: 1543 Average loss: 0.7068  reconstruction loss:  0.7059284211535555 contractive_loss:  8.570313185661304\n",
      "====> Epoch: 1544 Average loss: 0.5362  reconstruction loss:  0.5352696065041634 contractive_loss:  9.52577856876763\n",
      "====> Epoch: 1545 Average loss: 0.5226  reconstruction loss:  0.5214979227246619 contractive_loss:  11.09781714787746\n",
      "====> Epoch: 1546 Average loss: 0.9885  reconstruction loss:  0.9882109203300438 contractive_loss:  3.12164056462399\n",
      "====> Epoch: 1547 Average loss: 0.5606  reconstruction loss:  0.560442257734086 contractive_loss:  1.1370303143660847\n",
      "====> Epoch: 1548 Average loss: 0.9930  reconstruction loss:  0.992745966687442 contractive_loss:  2.294466818100195\n",
      "====> Epoch: 1549 Average loss: 1.1494  reconstruction loss:  1.1487236211758451 contractive_loss:  6.835269847605162\n",
      "====> Epoch: 1550 Average loss: 0.9197  reconstruction loss:  0.9182261702812826 contractive_loss:  15.18021959719564\n",
      "====> Epoch: 1551 Average loss: 0.8135  reconstruction loss:  0.8118642724784937 contractive_loss:  16.060287973966563\n",
      "====> Epoch: 1552 Average loss: 0.7321  reconstruction loss:  0.7305780091159361 contractive_loss:  15.363116417254437\n",
      "====> Epoch: 1553 Average loss: 1.7442  reconstruction loss:  1.7423741588282353 contractive_loss:  17.88583576133469\n",
      "====> Epoch: 1554 Average loss: 1.5531  reconstruction loss:  1.5510821459027313 contractive_loss:  20.27729595321721\n",
      "====> Epoch: 1555 Average loss: 0.7235  reconstruction loss:  0.7217124862739945 contractive_loss:  18.314765524514193\n",
      "====> Epoch: 1556 Average loss: 2.1852  reconstruction loss:  2.183668829370155 contractive_loss:  15.728963078611153\n",
      "====> Epoch: 1557 Average loss: 2.4601  reconstruction loss:  2.459357901944254 contractive_loss:  7.317572111464339\n",
      "====> Epoch: 1558 Average loss: 2.5008  reconstruction loss:  2.4998203777033026 contractive_loss:  9.53880945550108\n",
      "====> Epoch: 1559 Average loss: 2.4880  reconstruction loss:  2.487050153930086 contractive_loss:  9.028129426973782\n",
      "====> Epoch: 1560 Average loss: 1.1342  reconstruction loss:  1.133449019393712 contractive_loss:  7.601014279295204\n",
      "====> Epoch: 1561 Average loss: 1.0383  reconstruction loss:  1.0375001776882695 contractive_loss:  7.952682010185348\n",
      "====> Epoch: 1562 Average loss: 1.5346  reconstruction loss:  1.533572932857115 contractive_loss:  9.931305389746967\n",
      "====> Epoch: 1563 Average loss: 1.4645  reconstruction loss:  1.4636604294235025 contractive_loss:  8.018673622661028\n",
      "====> Epoch: 1564 Average loss: 1.3534  reconstruction loss:  1.3525705729167834 contractive_loss:  8.099382076136013\n",
      "====> Epoch: 1565 Average loss: 1.1753  reconstruction loss:  1.1744956924397008 contractive_loss:  8.339552908834182\n",
      "====> Epoch: 1566 Average loss: 1.5970  reconstruction loss:  1.5960071154940672 contractive_loss:  9.737783896423956\n",
      "====> Epoch: 1567 Average loss: 1.5781  reconstruction loss:  1.5771716906244684 contractive_loss:  9.325932373479704\n",
      "====> Epoch: 1568 Average loss: 1.3402  reconstruction loss:  1.339272741647206 contractive_loss:  8.871441267032658\n",
      "====> Epoch: 1569 Average loss: 1.0722  reconstruction loss:  1.0713318005888308 contractive_loss:  8.342723174157674\n",
      "====> Epoch: 1570 Average loss: 0.8374  reconstruction loss:  0.8362523614934514 contractive_loss:  11.660671536005168\n",
      "====> Epoch: 1571 Average loss: 1.0282  reconstruction loss:  1.0272993837019095 contractive_loss:  8.704131515490866\n",
      "====> Epoch: 1572 Average loss: 1.3466  reconstruction loss:  1.3456133493735871 contractive_loss:  9.570794195898975\n",
      "====> Epoch: 1573 Average loss: 1.6331  reconstruction loss:  1.6321327048269472 contractive_loss:  9.880835132891384\n",
      "====> Epoch: 1574 Average loss: 2.4048  reconstruction loss:  2.403696547646981 contractive_loss:  10.551391143754882\n",
      "====> Epoch: 1575 Average loss: 1.2929  reconstruction loss:  1.2919623679990335 contractive_loss:  9.429669668055157\n",
      "====> Epoch: 1576 Average loss: 0.7492  reconstruction loss:  0.7485251924652221 contractive_loss:  7.031626750406005\n",
      "====> Epoch: 1577 Average loss: 0.5016  reconstruction loss:  0.5009363666968515 contractive_loss:  6.566927927081035\n",
      "model saved!\n",
      "====> Epoch: 1578 Average loss: 0.4548  reconstruction loss:  0.4537269279359042 contractive_loss:  10.370704650666585\n",
      "====> Epoch: 1579 Average loss: 0.5470  reconstruction loss:  0.5461523283950285 contractive_loss:  8.133243678262714\n",
      "====> Epoch: 1580 Average loss: 0.7197  reconstruction loss:  0.7184393785542282 contractive_loss:  12.696782063690726\n",
      "====> Epoch: 1581 Average loss: 0.6992  reconstruction loss:  0.6978809898920033 contractive_loss:  13.2321636963973\n",
      "====> Epoch: 1582 Average loss: 1.8539  reconstruction loss:  1.8531023822926016 contractive_loss:  7.603410966474988\n",
      "====> Epoch: 1583 Average loss: 1.4668  reconstruction loss:  1.4650897030430992 contractive_loss:  17.44053367106705\n",
      "====> Epoch: 1584 Average loss: 0.9205  reconstruction loss:  0.9187288064465432 contractive_loss:  18.0394349737034\n",
      "====> Epoch: 1585 Average loss: 1.4380  reconstruction loss:  1.4371317973437043 contractive_loss:  8.617666375473124\n",
      "====> Epoch: 1586 Average loss: 1.7462  reconstruction loss:  1.7450079588111236 contractive_loss:  11.49755067530004\n",
      "====> Epoch: 1587 Average loss: 1.0445  reconstruction loss:  1.0422776835435477 contractive_loss:  22.621485800451342\n",
      "====> Epoch: 1588 Average loss: 0.7366  reconstruction loss:  0.7343119587051814 contractive_loss:  22.8076829343546\n",
      "====> Epoch: 1589 Average loss: 0.7939  reconstruction loss:  0.7916536746256952 contractive_loss:  22.783057816808064\n",
      "====> Epoch: 1590 Average loss: 1.3154  reconstruction loss:  1.3133095340778593 contractive_loss:  20.99716196570202\n",
      "====> Epoch: 1591 Average loss: 0.9784  reconstruction loss:  0.9767446713651162 contractive_loss:  16.181254014062855\n",
      "====> Epoch: 1592 Average loss: 1.0936  reconstruction loss:  1.0921533800136498 contractive_loss:  14.000371973612312\n",
      "====> Epoch: 1593 Average loss: 0.8398  reconstruction loss:  0.8377833717930333 contractive_loss:  20.09065446880973\n",
      "====> Epoch: 1594 Average loss: 0.8289  reconstruction loss:  0.8268870853593164 contractive_loss:  19.689793182974242\n",
      "====> Epoch: 1595 Average loss: 0.9380  reconstruction loss:  0.9361183620757825 contractive_loss:  19.04740975126132\n",
      "====> Epoch: 1596 Average loss: 0.7846  reconstruction loss:  0.7825875088322843 contractive_loss:  20.14062279065104\n",
      "====> Epoch: 1597 Average loss: 0.7672  reconstruction loss:  0.7651802830570685 contractive_loss:  19.72674272742318\n",
      "====> Epoch: 1598 Average loss: 0.6637  reconstruction loss:  0.6618207327336834 contractive_loss:  18.45627618635301\n",
      "====> Epoch: 1599 Average loss: 0.7787  reconstruction loss:  0.7766636670468589 contractive_loss:  19.934763491453612\n",
      "====> Epoch: 1600 Average loss: 0.6630  reconstruction loss:  0.6610497440159355 contractive_loss:  19.540963669862673\n",
      "====> Epoch: 1601 Average loss: 0.6356  reconstruction loss:  0.6337192582458995 contractive_loss:  18.340257145972075\n",
      "====> Epoch: 1602 Average loss: 0.8337  reconstruction loss:  0.8317029625011223 contractive_loss:  20.15800408776411\n",
      "====> Epoch: 1603 Average loss: 0.6936  reconstruction loss:  0.6915955932533561 contractive_loss:  19.62542689703094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1604 Average loss: 0.5555  reconstruction loss:  0.5537533926805644 contractive_loss:  17.85642660061882\n",
      "====> Epoch: 1605 Average loss: 1.5225  reconstruction loss:  1.5203073615314457 contractive_loss:  21.71618199647381\n",
      "====> Epoch: 1606 Average loss: 1.3440  reconstruction loss:  1.3417222148930685 contractive_loss:  22.57663476734802\n",
      "====> Epoch: 1607 Average loss: 2.0661  reconstruction loss:  2.063943076844489 contractive_loss:  21.340789667367407\n",
      "====> Epoch: 1608 Average loss: 0.7613  reconstruction loss:  0.7593660325564057 contractive_loss:  19.424702534366734\n",
      "====> Epoch: 1609 Average loss: 0.7210  reconstruction loss:  0.7191187344407416 contractive_loss:  18.612542225932067\n",
      "====> Epoch: 1610 Average loss: 1.1014  reconstruction loss:  1.0993732969542644 contractive_loss:  19.884410278615775\n",
      "====> Epoch: 1611 Average loss: 1.4259  reconstruction loss:  1.4237892958657283 contractive_loss:  21.03158916056766\n",
      "====> Epoch: 1612 Average loss: 0.7851  reconstruction loss:  0.7832591670327256 contractive_loss:  18.672994426102413\n",
      "====> Epoch: 1613 Average loss: 0.6622  reconstruction loss:  0.6603597438052244 contractive_loss:  18.590726346904706\n",
      "====> Epoch: 1614 Average loss: 1.5999  reconstruction loss:  1.597727704732723 contractive_loss:  22.063133413741898\n",
      "====> Epoch: 1615 Average loss: 1.8094  reconstruction loss:  1.8071393243444456 contractive_loss:  22.533377712517535\n",
      "====> Epoch: 1616 Average loss: 2.1415  reconstruction loss:  2.139450963628387 contractive_loss:  20.687876329749074\n",
      "====> Epoch: 1617 Average loss: 1.6686  reconstruction loss:  1.666473305377652 contractive_loss:  21.351075995504406\n",
      "====> Epoch: 1618 Average loss: 0.7787  reconstruction loss:  0.7767015393039969 contractive_loss:  20.307621735594832\n",
      "====> Epoch: 1619 Average loss: 0.6321  reconstruction loss:  0.6302539590770643 contractive_loss:  18.600568085918848\n",
      "====> Epoch: 1620 Average loss: 0.7081  reconstruction loss:  0.7061238737400853 contractive_loss:  20.1457878962488\n",
      "====> Epoch: 1621 Average loss: 0.8211  reconstruction loss:  0.8190210472361634 contractive_loss:  20.60066146467696\n",
      "====> Epoch: 1622 Average loss: 0.5746  reconstruction loss:  0.5727424199670496 contractive_loss:  18.713246342797948\n",
      "====> Epoch: 1623 Average loss: 0.7018  reconstruction loss:  0.6998809947931346 contractive_loss:  19.360649014983974\n",
      "====> Epoch: 1624 Average loss: 0.5635  reconstruction loss:  0.5616163029887964 contractive_loss:  18.619228774956923\n",
      "====> Epoch: 1625 Average loss: 0.5132  reconstruction loss:  0.5114288386943349 contractive_loss:  17.358533708441243\n",
      "====> Epoch: 1626 Average loss: 0.5599  reconstruction loss:  0.5581109482088435 contractive_loss:  18.042746808050303\n",
      "====> Epoch: 1627 Average loss: 0.8147  reconstruction loss:  0.8128321337431007 contractive_loss:  18.925088671750295\n",
      "====> Epoch: 1628 Average loss: 0.6754  reconstruction loss:  0.67363434013849 contractive_loss:  18.011646614564125\n",
      "====> Epoch: 1629 Average loss: 3.2459  reconstruction loss:  3.243670744667233 contractive_loss:  22.242173281834848\n",
      "====> Epoch: 1630 Average loss: 1.4699  reconstruction loss:  1.4691574167963741 contractive_loss:  7.79211447516419\n",
      "====> Epoch: 1631 Average loss: 2.0980  reconstruction loss:  2.09719896726046 contractive_loss:  7.962420206237093\n",
      "====> Epoch: 1632 Average loss: 1.1976  reconstruction loss:  1.196697797948254 contractive_loss:  9.150152177828017\n",
      "====> Epoch: 1633 Average loss: 0.8308  reconstruction loss:  0.8300059637195452 contractive_loss:  8.029925711103788\n",
      "====> Epoch: 1634 Average loss: 1.0088  reconstruction loss:  1.0078655911306904 contractive_loss:  9.123824617367832\n",
      "====> Epoch: 1635 Average loss: 2.3782  reconstruction loss:  2.3772203484003622 contractive_loss:  10.197129908464559\n",
      "====> Epoch: 1636 Average loss: 1.0795  reconstruction loss:  1.0785909332802017 contractive_loss:  9.316449286737729\n",
      "====> Epoch: 1637 Average loss: 1.1813  reconstruction loss:  1.1803669854783556 contractive_loss:  9.455763432765838\n",
      "====> Epoch: 1638 Average loss: 0.6727  reconstruction loss:  0.6718941869978042 contractive_loss:  8.4222978696159\n",
      "====> Epoch: 1639 Average loss: 1.3066  reconstruction loss:  1.3055491814519038 contractive_loss:  10.215796639727063\n",
      "====> Epoch: 1640 Average loss: 0.7546  reconstruction loss:  0.7537534215259282 contractive_loss:  8.822642036757117\n",
      "model saved!\n",
      "====> Epoch: 1641 Average loss: 0.4541  reconstruction loss:  0.45261690527352455 contractive_loss:  14.401436042053001\n",
      "====> Epoch: 1642 Average loss: 0.5095  reconstruction loss:  0.5079065125663004 contractive_loss:  15.921350974144046\n",
      "====> Epoch: 1643 Average loss: 0.6308  reconstruction loss:  0.6291664660415044 contractive_loss:  16.050642228429716\n",
      "====> Epoch: 1644 Average loss: 1.3808  reconstruction loss:  1.3796862969690238 contractive_loss:  10.797788741830082\n",
      "====> Epoch: 1645 Average loss: 1.1669  reconstruction loss:  1.1666948363833032 contractive_loss:  2.1176924118404363\n",
      "====> Epoch: 1646 Average loss: 0.9308  reconstruction loss:  0.9305294421484581 contractive_loss:  2.5686205780283196\n",
      "====> Epoch: 1647 Average loss: 0.5081  reconstruction loss:  0.5079218357738607 contractive_loss:  2.150853223920331\n",
      "====> Epoch: 1648 Average loss: 0.6424  reconstruction loss:  0.6422006039051719 contractive_loss:  1.7838862496598584\n",
      "====> Epoch: 1649 Average loss: 0.5617  reconstruction loss:  0.5612508064732937 contractive_loss:  4.786347302990085\n",
      "model saved!\n",
      "====> Epoch: 1650 Average loss: 0.4421  reconstruction loss:  0.4412187534162381 contractive_loss:  9.29984519400837\n",
      "model saved!\n",
      "====> Epoch: 1651 Average loss: 0.3845  reconstruction loss:  0.38316633889848783 contractive_loss:  13.120251137154318\n",
      "====> Epoch: 1652 Average loss: 0.7281  reconstruction loss:  0.7265167747958009 contractive_loss:  15.603211636461756\n",
      "====> Epoch: 1653 Average loss: 0.7833  reconstruction loss:  0.7829199057453495 contractive_loss:  3.4379583027690535\n",
      "====> Epoch: 1654 Average loss: 1.0756  reconstruction loss:  1.075285491598074 contractive_loss:  2.800736405796151\n",
      "====> Epoch: 1655 Average loss: 0.3894  reconstruction loss:  0.38911062235630434 contractive_loss:  2.725593021825753\n",
      "====> Epoch: 1656 Average loss: 0.6258  reconstruction loss:  0.6255162193612812 contractive_loss:  2.437391981341298\n",
      "====> Epoch: 1657 Average loss: 0.6911  reconstruction loss:  0.6909480089735024 contractive_loss:  1.9050400477849032\n",
      "====> Epoch: 1658 Average loss: 1.0705  reconstruction loss:  1.0702106627385206 contractive_loss:  3.1830259392774116\n",
      "====> Epoch: 1659 Average loss: 1.0304  reconstruction loss:  1.0301825721162972 contractive_loss:  2.3112854415533426\n",
      "====> Epoch: 1660 Average loss: 0.4638  reconstruction loss:  0.46356601869637215 contractive_loss:  2.823116767144333\n",
      "====> Epoch: 1661 Average loss: 0.4397  reconstruction loss:  0.4394027909435137 contractive_loss:  3.2962398748083572\n",
      "====> Epoch: 1662 Average loss: 0.5706  reconstruction loss:  0.5703712291488889 contractive_loss:  1.8576593000307027\n",
      "====> Epoch: 1663 Average loss: 0.5766  reconstruction loss:  0.576435096954482 contractive_loss:  1.684117239027226\n",
      "====> Epoch: 1664 Average loss: 0.4647  reconstruction loss:  0.46440526200496035 contractive_loss:  2.736677110580722\n",
      "====> Epoch: 1665 Average loss: 0.4343  reconstruction loss:  0.43404898024681343 contractive_loss:  2.347224821047088\n",
      "====> Epoch: 1666 Average loss: 0.5691  reconstruction loss:  0.5687747347573634 contractive_loss:  3.001972188092029\n",
      "====> Epoch: 1667 Average loss: 0.4061  reconstruction loss:  0.4059216349443392 contractive_loss:  2.2735987526964574\n",
      "model saved!\n",
      "====> Epoch: 1668 Average loss: 0.3732  reconstruction loss:  0.37289567582118516 contractive_loss:  2.6037092380591176\n",
      "====> Epoch: 1669 Average loss: 2.6759  reconstruction loss:  2.675734452418098 contractive_loss:  1.7157222806426744\n",
      "====> Epoch: 1670 Average loss: 3.1686  reconstruction loss:  3.1679767964027814 contractive_loss:  5.9648319622836965\n",
      "====> Epoch: 1671 Average loss: 1.2628  reconstruction loss:  1.262135799497664 contractive_loss:  7.116136749238312\n",
      "====> Epoch: 1672 Average loss: 1.3731  reconstruction loss:  1.3721634235243447 contractive_loss:  9.340339493053307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1673 Average loss: 2.0036  reconstruction loss:  2.002278842580098 contractive_loss:  13.30955514113621\n",
      "====> Epoch: 1674 Average loss: 1.3236  reconstruction loss:  1.3208786027887829 contractive_loss:  27.686756129667067\n",
      "====> Epoch: 1675 Average loss: 1.9189  reconstruction loss:  1.915208774473793 contractive_loss:  36.75394111589219\n",
      "====> Epoch: 1676 Average loss: 3.0014  reconstruction loss:  2.9990324160104165 contractive_loss:  23.838438755870346\n",
      "====> Epoch: 1677 Average loss: 2.3900  reconstruction loss:  2.387621682377339 contractive_loss:  23.38773434373361\n",
      "====> Epoch: 1678 Average loss: 1.9833  reconstruction loss:  1.980990226621888 contractive_loss:  23.316222238512122\n",
      "====> Epoch: 1679 Average loss: 1.8354  reconstruction loss:  1.832876069465418 contractive_loss:  24.914883066958108\n",
      "====> Epoch: 1680 Average loss: 1.5325  reconstruction loss:  1.5297745356515733 contractive_loss:  27.048068810633566\n",
      "====> Epoch: 1681 Average loss: 1.6697  reconstruction loss:  1.6670747639594368 contractive_loss:  26.017536475325052\n",
      "====> Epoch: 1682 Average loss: 1.9485  reconstruction loss:  1.9460681198778595 contractive_loss:  24.15721856135249\n",
      "====> Epoch: 1683 Average loss: 2.1141  reconstruction loss:  2.1117207634861472 contractive_loss:  23.44316951323108\n",
      "====> Epoch: 1684 Average loss: 1.6171  reconstruction loss:  1.614509022206927 contractive_loss:  26.349016901376277\n",
      "====> Epoch: 1685 Average loss: 1.1508  reconstruction loss:  1.1475681617286562 contractive_loss:  32.226958239411765\n",
      "====> Epoch: 1686 Average loss: 1.1157  reconstruction loss:  1.1126578838726555 contractive_loss:  30.612100851248705\n",
      "====> Epoch: 1687 Average loss: 1.4921  reconstruction loss:  1.4892704808743356 contractive_loss:  28.704297620603107\n",
      "====> Epoch: 1688 Average loss: 1.9188  reconstruction loss:  1.9162523511141858 contractive_loss:  25.443699821765076\n",
      "====> Epoch: 1689 Average loss: 1.9295  reconstruction loss:  1.9269636129282635 contractive_loss:  24.889387838354757\n",
      "====> Epoch: 1690 Average loss: 1.1860  reconstruction loss:  1.1830720129244876 contractive_loss:  29.378406061488157\n",
      "====> Epoch: 1691 Average loss: 1.4912  reconstruction loss:  1.4883602807904008 contractive_loss:  28.762950372041477\n",
      "====> Epoch: 1692 Average loss: 2.0490  reconstruction loss:  2.0463654725939686 contractive_loss:  26.62410259373125\n",
      "====> Epoch: 1693 Average loss: 2.0910  reconstruction loss:  2.0884967481506713 contractive_loss:  25.195814249137527\n",
      "====> Epoch: 1694 Average loss: 1.7091  reconstruction loss:  1.7063461495254797 contractive_loss:  27.22778176180237\n",
      "====> Epoch: 1695 Average loss: 1.2580  reconstruction loss:  1.2548095562778314 contractive_loss:  31.740621189834332\n",
      "====> Epoch: 1696 Average loss: 1.4881  reconstruction loss:  1.4850976140876069 contractive_loss:  29.54673063728425\n",
      "====> Epoch: 1697 Average loss: 1.7503  reconstruction loss:  1.747511198001676 contractive_loss:  27.41118920172298\n",
      "====> Epoch: 1698 Average loss: 1.9081  reconstruction loss:  1.9054768140809029 contractive_loss:  26.14050809097745\n",
      "====> Epoch: 1699 Average loss: 1.8966  reconstruction loss:  1.8939754520848675 contractive_loss:  26.675074719040108\n",
      "====> Epoch: 1700 Average loss: 1.6402  reconstruction loss:  1.637320143576049 contractive_loss:  28.561111112598937\n",
      "====> Epoch: 1701 Average loss: 2.0318  reconstruction loss:  2.029178620944702 contractive_loss:  26.434083756102062\n",
      "====> Epoch: 1702 Average loss: 1.9400  reconstruction loss:  1.9373739624281299 contractive_loss:  25.907809274767708\n",
      "====> Epoch: 1703 Average loss: 2.0253  reconstruction loss:  2.022738235949193 contractive_loss:  25.771445153103\n",
      "====> Epoch: 1704 Average loss: 1.2941  reconstruction loss:  1.2909836121380827 contractive_loss:  31.115447353915613\n",
      "====> Epoch: 1705 Average loss: 1.0615  reconstruction loss:  1.0583139061784403 contractive_loss:  32.25293825375713\n",
      "====> Epoch: 1706 Average loss: 1.6032  reconstruction loss:  1.600251097459328 contractive_loss:  29.81322080217086\n",
      "====> Epoch: 1707 Average loss: 2.0885  reconstruction loss:  2.0856303339678814 contractive_loss:  28.40256577410299\n",
      "====> Epoch: 1708 Average loss: 1.3393  reconstruction loss:  1.3355571669183643 contractive_loss:  37.858072658962165\n",
      "====> Epoch: 1709 Average loss: 1.6175  reconstruction loss:  1.6140200816072734 contractive_loss:  35.003155601326846\n",
      "====> Epoch: 1710 Average loss: 1.6280  reconstruction loss:  1.6260441357549953 contractive_loss:  19.735585553537437\n",
      "====> Epoch: 1711 Average loss: 1.8277  reconstruction loss:  1.8247371884169978 contractive_loss:  29.536086268861645\n",
      "====> Epoch: 1712 Average loss: 2.1042  reconstruction loss:  2.1015790339125098 contractive_loss:  25.786806647172142\n",
      "====> Epoch: 1713 Average loss: 1.6706  reconstruction loss:  1.6667583737767115 contractive_loss:  38.166862107145995\n",
      "====> Epoch: 1714 Average loss: 1.3808  reconstruction loss:  1.377106311128603 contractive_loss:  37.0129288472702\n",
      "====> Epoch: 1715 Average loss: 1.4682  reconstruction loss:  1.46504776469724 contractive_loss:  31.28490448862953\n",
      "====> Epoch: 1716 Average loss: 2.0091  reconstruction loss:  2.006417791162043 contractive_loss:  26.979994785889794\n",
      "====> Epoch: 1717 Average loss: 1.5160  reconstruction loss:  1.5121414804986124 contractive_loss:  38.32675299536326\n",
      "====> Epoch: 1718 Average loss: 1.9664  reconstruction loss:  1.962250644808406 contractive_loss:  41.58402072288588\n",
      "====> Epoch: 1719 Average loss: 1.2708  reconstruction loss:  1.267537468498627 contractive_loss:  32.253145933084745\n",
      "====> Epoch: 1720 Average loss: 1.9694  reconstruction loss:  1.9666811545588094 contractive_loss:  27.683007471017042\n",
      "====> Epoch: 1721 Average loss: 1.4302  reconstruction loss:  1.4270851574189454 contractive_loss:  31.082637186029338\n",
      "====> Epoch: 1722 Average loss: 1.0211  reconstruction loss:  1.017658777762814 contractive_loss:  34.50277820626387\n",
      "====> Epoch: 1723 Average loss: 1.3223  reconstruction loss:  1.3190976241805685 contractive_loss:  32.324328356860946\n",
      "====> Epoch: 1724 Average loss: 1.8814  reconstruction loss:  1.8784960122731074 contractive_loss:  28.998201056139447\n",
      "====> Epoch: 1725 Average loss: 1.6603  reconstruction loss:  1.6568023498928293 contractive_loss:  34.713491726573714\n",
      "====> Epoch: 1726 Average loss: 1.9560  reconstruction loss:  1.9512657700944658 contractive_loss:  47.73641683301965\n",
      "====> Epoch: 1727 Average loss: 1.8526  reconstruction loss:  1.8486326276317215 contractive_loss:  39.67159521504557\n",
      "====> Epoch: 1728 Average loss: 1.8444  reconstruction loss:  1.8417113835712917 contractive_loss:  26.43466037256344\n",
      "====> Epoch: 1729 Average loss: 1.8331  reconstruction loss:  1.828764173037849 contractive_loss:  42.89415419575955\n",
      "====> Epoch: 1730 Average loss: 2.3847  reconstruction loss:  2.380723935318516 contractive_loss:  39.850875667926914\n",
      "====> Epoch: 1731 Average loss: 2.3619  reconstruction loss:  2.357125184117071 contractive_loss:  47.41900785689473\n",
      "====> Epoch: 1732 Average loss: 1.2314  reconstruction loss:  1.227498647799029 contractive_loss:  39.43891980028901\n",
      "====> Epoch: 1733 Average loss: 1.3123  reconstruction loss:  1.3090522537816198 contractive_loss:  32.89613176557294\n",
      "====> Epoch: 1734 Average loss: 2.5574  reconstruction loss:  2.5546870658358554 contractive_loss:  27.0892813675739\n",
      "====> Epoch: 1735 Average loss: 1.4429  reconstruction loss:  1.4384577091877184 contractive_loss:  44.119183933586925\n",
      "====> Epoch: 1736 Average loss: 2.5348  reconstruction loss:  2.5275897948207846 contractive_loss:  71.78360835875344\n",
      "====> Epoch: 1737 Average loss: 0.8248  reconstruction loss:  0.8178301696678048 contractive_loss:  69.59157015949138\n",
      "====> Epoch: 1738 Average loss: 1.5727  reconstruction loss:  1.5654304132294699 contractive_loss:  72.60349316612074\n",
      "====> Epoch: 1739 Average loss: 1.9373  reconstruction loss:  1.9298973785833515 contractive_loss:  73.82549290771271\n",
      "====> Epoch: 1740 Average loss: 2.4743  reconstruction loss:  2.466818650739736 contractive_loss:  75.15902611911145\n",
      "====> Epoch: 1741 Average loss: 0.7656  reconstruction loss:  0.7584845868201872 contractive_loss:  71.38711979773278\n",
      "====> Epoch: 1742 Average loss: 0.8722  reconstruction loss:  0.8649721631771332 contractive_loss:  72.2125295073454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1743 Average loss: 2.3126  reconstruction loss:  2.305064204080293 contractive_loss:  75.36367818014233\n",
      "====> Epoch: 1744 Average loss: 0.7992  reconstruction loss:  0.7922790870950487 contractive_loss:  69.05058030063522\n",
      "====> Epoch: 1745 Average loss: 0.5058  reconstruction loss:  0.49859571063520136 contractive_loss:  71.66828370222954\n",
      "====> Epoch: 1746 Average loss: 1.6072  reconstruction loss:  1.5996886275113866 contractive_loss:  75.00879323541051\n",
      "====> Epoch: 1747 Average loss: 1.6897  reconstruction loss:  1.6822204855862577 contractive_loss:  75.20307717083836\n",
      "====> Epoch: 1748 Average loss: 2.0532  reconstruction loss:  2.045602885281068 contractive_loss:  75.47561430086998\n",
      "====> Epoch: 1749 Average loss: 1.2858  reconstruction loss:  1.2784332309548576 contractive_loss:  74.05090055598471\n",
      "====> Epoch: 1750 Average loss: 0.6089  reconstruction loss:  0.6018234237677502 contractive_loss:  70.86634761028405\n",
      "====> Epoch: 1751 Average loss: 1.5761  reconstruction loss:  1.568599666557759 contractive_loss:  74.73139798396959\n",
      "====> Epoch: 1752 Average loss: 1.3054  reconstruction loss:  1.297992546292301 contractive_loss:  74.12051975977666\n",
      "====> Epoch: 1753 Average loss: 1.5004  reconstruction loss:  1.4929075454644896 contractive_loss:  75.07509534184224\n",
      "====> Epoch: 1754 Average loss: 0.8850  reconstruction loss:  0.8777141203814641 contractive_loss:  73.30869403258684\n",
      "====> Epoch: 1755 Average loss: 0.6264  reconstruction loss:  0.6197778591046879 contractive_loss:  65.8946939464734\n",
      "====> Epoch: 1756 Average loss: 1.6774  reconstruction loss:  1.6700453798469566 contractive_loss:  73.86660158051147\n",
      "====> Epoch: 1757 Average loss: 1.3281  reconstruction loss:  1.3206837189360057 contractive_loss:  73.8758917566321\n",
      "====> Epoch: 1758 Average loss: 1.5502  reconstruction loss:  1.5427381352364378 contractive_loss:  74.21100839597912\n",
      "====> Epoch: 1759 Average loss: 0.7776  reconstruction loss:  0.7700562685744379 contractive_loss:  75.6902004512574\n",
      "====> Epoch: 1760 Average loss: 1.8433  reconstruction loss:  1.8357009920568956 contractive_loss:  75.6767576557698\n",
      "====> Epoch: 1761 Average loss: 1.6684  reconstruction loss:  1.6616913865854368 contractive_loss:  67.275749463128\n",
      "====> Epoch: 1762 Average loss: 1.3610  reconstruction loss:  1.3563587690475736 contractive_loss:  46.61214720010908\n",
      "====> Epoch: 1763 Average loss: 0.8291  reconstruction loss:  0.828958767829467 contractive_loss:  1.420410152669028\n",
      "====> Epoch: 1764 Average loss: 0.7094  reconstruction loss:  0.709240268628292 contractive_loss:  1.5920738066529563\n",
      "====> Epoch: 1765 Average loss: 0.4859  reconstruction loss:  0.48577567513978354 contractive_loss:  1.605335490554947\n",
      "====> Epoch: 1766 Average loss: 0.3749  reconstruction loss:  0.37478247612895954 contractive_loss:  1.0392095849322807\n",
      "====> Epoch: 1767 Average loss: 0.4319  reconstruction loss:  0.43179309477909145 contractive_loss:  1.0067916200792681\n",
      "model saved!\n",
      "====> Epoch: 1768 Average loss: 0.3000  reconstruction loss:  0.2998598218253616 contractive_loss:  1.3450931110032662\n",
      "====> Epoch: 1769 Average loss: 0.5476  reconstruction loss:  0.5474935589975002 contractive_loss:  0.7558928600331709\n",
      "====> Epoch: 1770 Average loss: 0.4700  reconstruction loss:  0.46984462602719673 contractive_loss:  1.4204768995985495\n",
      "====> Epoch: 1771 Average loss: 0.3974  reconstruction loss:  0.397316187787296 contractive_loss:  0.87185770040705\n",
      "====> Epoch: 1772 Average loss: 0.6151  reconstruction loss:  0.6148117087854923 contractive_loss:  2.961511958249676\n",
      "====> Epoch: 1773 Average loss: 0.9446  reconstruction loss:  0.9445571741704094 contractive_loss:  0.7149431932399759\n",
      "====> Epoch: 1774 Average loss: 1.0355  reconstruction loss:  1.034594518141717 contractive_loss:  8.62076464586766\n",
      "====> Epoch: 1775 Average loss: 1.1626  reconstruction loss:  1.1620722452909384 contractive_loss:  5.7728184459225895\n",
      "====> Epoch: 1776 Average loss: 1.4348  reconstruction loss:  1.4347092781898658 contractive_loss:  0.975422236849329\n",
      "====> Epoch: 1777 Average loss: 0.4725  reconstruction loss:  0.4722386928506126 contractive_loss:  2.410867988150713\n",
      "====> Epoch: 1778 Average loss: 1.0141  reconstruction loss:  1.0130766770612831 contractive_loss:  10.500162589327239\n",
      "====> Epoch: 1779 Average loss: 0.5431  reconstruction loss:  0.5425548464664603 contractive_loss:  5.507292356956114\n",
      "====> Epoch: 1780 Average loss: 3.1983  reconstruction loss:  3.1975934574164104 contractive_loss:  7.134338612666524\n",
      "====> Epoch: 1781 Average loss: 1.8871  reconstruction loss:  1.88617238545102 contractive_loss:  8.968773694548418\n",
      "====> Epoch: 1782 Average loss: 2.4876  reconstruction loss:  2.486450922568208 contractive_loss:  11.871492430845949\n",
      "====> Epoch: 1783 Average loss: 1.5648  reconstruction loss:  1.5639200495327674 contractive_loss:  9.04306711253007\n",
      "====> Epoch: 1784 Average loss: 1.5136  reconstruction loss:  1.5130028973456893 contractive_loss:  6.218395888672902\n",
      "====> Epoch: 1785 Average loss: 2.4555  reconstruction loss:  2.4552644017889786 contractive_loss:  2.587897463799605\n",
      "====> Epoch: 1786 Average loss: 0.7253  reconstruction loss:  0.7250902658395959 contractive_loss:  2.2215815209320824\n",
      "====> Epoch: 1787 Average loss: 0.4985  reconstruction loss:  0.4982945627985946 contractive_loss:  2.2499520722102684\n",
      "====> Epoch: 1788 Average loss: 3.2626  reconstruction loss:  3.2618460196610966 contractive_loss:  7.730376454132005\n",
      "====> Epoch: 1789 Average loss: 3.4862  reconstruction loss:  3.4850901638291925 contractive_loss:  11.059378588385204\n",
      "====> Epoch: 1790 Average loss: 1.7014  reconstruction loss:  1.7001239856780423 contractive_loss:  12.763035288129293\n",
      "====> Epoch: 1791 Average loss: 0.9670  reconstruction loss:  0.9656903742553282 contractive_loss:  12.681066574108378\n",
      "====> Epoch: 1792 Average loss: 1.4969  reconstruction loss:  1.4957857049973164 contractive_loss:  11.615918066291993\n",
      "====> Epoch: 1793 Average loss: 1.5950  reconstruction loss:  1.5930169571893589 contractive_loss:  19.999453384875622\n",
      "====> Epoch: 1794 Average loss: 1.4941  reconstruction loss:  1.4923469678840096 contractive_loss:  17.44406732105458\n",
      "====> Epoch: 1795 Average loss: 0.8215  reconstruction loss:  0.8203484690763474 contractive_loss:  11.35844592728592\n",
      "====> Epoch: 1796 Average loss: 1.0374  reconstruction loss:  1.0363349509342097 contractive_loss:  11.050787278674495\n",
      "====> Epoch: 1797 Average loss: 1.4932  reconstruction loss:  1.4919732520790305 contractive_loss:  12.562892394766886\n",
      "====> Epoch: 1798 Average loss: 1.6924  reconstruction loss:  1.6910490570288532 contractive_loss:  13.844307850863409\n",
      "====> Epoch: 1799 Average loss: 0.8492  reconstruction loss:  0.8481030401093784 contractive_loss:  11.074787878749175\n",
      "====> Epoch: 1800 Average loss: 0.7473  reconstruction loss:  0.7461477869930944 contractive_loss:  11.570317578323587\n",
      "====> Epoch: 1801 Average loss: 1.0789  reconstruction loss:  1.0776156971921764 contractive_loss:  12.586052907837207\n",
      "====> Epoch: 1802 Average loss: 0.7224  reconstruction loss:  0.7212159213343254 contractive_loss:  12.03188382455806\n",
      "====> Epoch: 1803 Average loss: 0.6514  reconstruction loss:  0.6502693349901446 contractive_loss:  11.570964641886746\n",
      "====> Epoch: 1804 Average loss: 0.9666  reconstruction loss:  0.9653017200069866 contractive_loss:  12.902764989068293\n",
      "====> Epoch: 1805 Average loss: 0.7590  reconstruction loss:  0.7577721203458901 contractive_loss:  12.12797750051564\n",
      "====> Epoch: 1806 Average loss: 0.7413  reconstruction loss:  0.7401182938281938 contractive_loss:  12.14030936591136\n",
      "====> Epoch: 1807 Average loss: 1.3992  reconstruction loss:  1.3977650686388237 contractive_loss:  13.988564359095328\n",
      "====> Epoch: 1808 Average loss: 0.8492  reconstruction loss:  0.8480105271115993 contractive_loss:  12.333088614021705\n",
      "====> Epoch: 1809 Average loss: 1.3225  reconstruction loss:  1.3208616649152105 contractive_loss:  16.59356223984453\n",
      "====> Epoch: 1810 Average loss: 0.4244  reconstruction loss:  0.4223266642857956 contractive_loss:  21.05621614643534\n",
      "====> Epoch: 1811 Average loss: 0.4760  reconstruction loss:  0.4747244031312771 contractive_loss:  12.843517723649688\n",
      "====> Epoch: 1812 Average loss: 0.6866  reconstruction loss:  0.6855390668168008 contractive_loss:  10.378618402575992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1813 Average loss: 0.5597  reconstruction loss:  0.5582472835849671 contractive_loss:  14.14452301658048\n",
      "====> Epoch: 1814 Average loss: 0.4278  reconstruction loss:  0.4256776997899257 contractive_loss:  20.86773952580378\n",
      "====> Epoch: 1815 Average loss: 0.8989  reconstruction loss:  0.8973266178284272 contractive_loss:  16.086627543530103\n",
      "====> Epoch: 1816 Average loss: 0.4868  reconstruction loss:  0.4865111831367757 contractive_loss:  2.777366614555211\n",
      "====> Epoch: 1817 Average loss: 0.4999  reconstruction loss:  0.4996898827134746 contractive_loss:  2.446665887643193\n",
      "====> Epoch: 1818 Average loss: 0.3872  reconstruction loss:  0.3868822873256764 contractive_loss:  3.036393077236278\n",
      "====> Epoch: 1819 Average loss: 0.4704  reconstruction loss:  0.4701981213105269 contractive_loss:  2.2525894020196224\n",
      "====> Epoch: 1820 Average loss: 0.4396  reconstruction loss:  0.4392812698146814 contractive_loss:  2.984039425332449\n",
      "====> Epoch: 1821 Average loss: 0.3245  reconstruction loss:  0.32421234581866915 contractive_loss:  2.5928903625019797\n",
      "====> Epoch: 1822 Average loss: 0.4669  reconstruction loss:  0.4665926871541821 contractive_loss:  2.7448561511685616\n",
      "====> Epoch: 1823 Average loss: 0.3805  reconstruction loss:  0.3801369017689548 contractive_loss:  3.668783562544041\n",
      "====> Epoch: 1824 Average loss: 0.4614  reconstruction loss:  0.46128222722606593 contractive_loss:  1.4125222154648431\n",
      "====> Epoch: 1825 Average loss: 0.6128  reconstruction loss:  0.6125778613702082 contractive_loss:  2.0788782855937167\n",
      "====> Epoch: 1826 Average loss: 1.0678  reconstruction loss:  1.0664335200648225 contractive_loss:  13.264404088155556\n",
      "====> Epoch: 1827 Average loss: 1.3993  reconstruction loss:  1.394219325952074 contractive_loss:  51.16536994817519\n",
      "====> Epoch: 1828 Average loss: 1.7072  reconstruction loss:  1.7028361211475995 contractive_loss:  43.428932353529234\n",
      "====> Epoch: 1829 Average loss: 1.1818  reconstruction loss:  1.1779567346120303 contractive_loss:  38.91935172600286\n",
      "====> Epoch: 1830 Average loss: 1.0887  reconstruction loss:  1.0854617006202656 contractive_loss:  32.46240935947544\n",
      "====> Epoch: 1831 Average loss: 1.8833  reconstruction loss:  1.8805927169968104 contractive_loss:  27.311577623358275\n",
      "====> Epoch: 1832 Average loss: 2.0230  reconstruction loss:  2.0204128139062423 contractive_loss:  25.49579137483316\n",
      "====> Epoch: 1833 Average loss: 1.8965  reconstruction loss:  1.8939134240580864 contractive_loss:  26.289873567925603\n",
      "====> Epoch: 1834 Average loss: 1.9014  reconstruction loss:  1.8989414804118967 contractive_loss:  24.815264548414753\n",
      "====> Epoch: 1835 Average loss: 1.9553  reconstruction loss:  1.9525902544040707 contractive_loss:  26.86755240050533\n",
      "====> Epoch: 1836 Average loss: 1.3667  reconstruction loss:  1.3635944058959537 contractive_loss:  30.609575099613043\n",
      "====> Epoch: 1837 Average loss: 1.8439  reconstruction loss:  1.8410557205333953 contractive_loss:  28.415302576468186\n",
      "====> Epoch: 1838 Average loss: 2.1934  reconstruction loss:  2.190655240688045 contractive_loss:  27.40340741111314\n",
      "====> Epoch: 1839 Average loss: 1.9548  reconstruction loss:  1.9523003454745946 contractive_loss:  24.83710197407371\n",
      "====> Epoch: 1840 Average loss: 0.9834  reconstruction loss:  0.9798705008442345 contractive_loss:  35.2986762840665\n",
      "====> Epoch: 1841 Average loss: 1.3836  reconstruction loss:  1.379478043360217 contractive_loss:  41.66341193455657\n",
      "====> Epoch: 1842 Average loss: 1.2237  reconstruction loss:  1.2197266218040699 contractive_loss:  40.17618286237087\n",
      "====> Epoch: 1843 Average loss: 0.9681  reconstruction loss:  0.9644728973450748 contractive_loss:  35.992909619853286\n",
      "====> Epoch: 1844 Average loss: 1.0665  reconstruction loss:  1.0632686714581252 contractive_loss:  32.333708987170354\n",
      "====> Epoch: 1845 Average loss: 1.9169  reconstruction loss:  1.9141319416558837 contractive_loss:  28.06155142626639\n",
      "====> Epoch: 1846 Average loss: 1.6222  reconstruction loss:  1.6199158236818223 contractive_loss:  22.752225608405148\n",
      "====> Epoch: 1847 Average loss: 1.8921  reconstruction loss:  1.8871040937878198 contractive_loss:  49.73517581312617\n",
      "====> Epoch: 1848 Average loss: 1.1940  reconstruction loss:  1.1898581533624955 contractive_loss:  41.87628093892065\n",
      "====> Epoch: 1849 Average loss: 0.8956  reconstruction loss:  0.8919600687196703 contractive_loss:  36.738419741005586\n",
      "====> Epoch: 1850 Average loss: 1.4815  reconstruction loss:  1.4784555893790832 contractive_loss:  30.61059090913081\n",
      "====> Epoch: 1851 Average loss: 1.9167  reconstruction loss:  1.9140750372529909 contractive_loss:  26.500263604947502\n",
      "====> Epoch: 1852 Average loss: 1.5409  reconstruction loss:  1.5378901598627612 contractive_loss:  30.498469077962945\n",
      "====> Epoch: 1853 Average loss: 1.0220  reconstruction loss:  1.018188036231062 contractive_loss:  38.25542306587113\n",
      "====> Epoch: 1854 Average loss: 1.0771  reconstruction loss:  1.0731934148850402 contractive_loss:  39.16804928221647\n",
      "====> Epoch: 1855 Average loss: 1.0296  reconstruction loss:  1.026182347175296 contractive_loss:  34.46843811714773\n",
      "====> Epoch: 1856 Average loss: 1.8163  reconstruction loss:  1.8132685050999005 contractive_loss:  30.04283836390312\n",
      "====> Epoch: 1857 Average loss: 2.0894  reconstruction loss:  2.0866677240921296 contractive_loss:  26.840796033120846\n",
      "====> Epoch: 1858 Average loss: 2.1402  reconstruction loss:  2.1378725877533884 contractive_loss:  23.724559777019493\n",
      "====> Epoch: 1859 Average loss: 1.1033  reconstruction loss:  1.0974968472278477 contractive_loss:  58.48768870167782\n",
      "====> Epoch: 1860 Average loss: 0.9575  reconstruction loss:  0.9512739582309103 contractive_loss:  61.85586633064831\n",
      "====> Epoch: 1861 Average loss: 0.9019  reconstruction loss:  0.895774408033125 contractive_loss:  60.97462094991966\n",
      "====> Epoch: 1862 Average loss: 1.6568  reconstruction loss:  1.6519457771682136 contractive_loss:  48.83991095017881\n",
      "====> Epoch: 1863 Average loss: 1.6451  reconstruction loss:  1.642158550839705 contractive_loss:  29.834173259054058\n",
      "====> Epoch: 1864 Average loss: 1.3297  reconstruction loss:  1.3274493986634475 contractive_loss:  22.893725420860083\n",
      "====> Epoch: 1865 Average loss: 1.3637  reconstruction loss:  1.360522258997037 contractive_loss:  31.861734222180683\n",
      "====> Epoch: 1866 Average loss: 1.8065  reconstruction loss:  1.8015243134934247 contractive_loss:  49.701628433907274\n",
      "====> Epoch: 1867 Average loss: 2.4556  reconstruction loss:  2.4509846433339773 contractive_loss:  46.555173615698145\n",
      "====> Epoch: 1868 Average loss: 0.8849  reconstruction loss:  0.8809820786397805 contractive_loss:  39.66445643163523\n",
      "====> Epoch: 1869 Average loss: 2.0856  reconstruction loss:  2.0823755172808522 contractive_loss:  31.948470211239954\n",
      "====> Epoch: 1870 Average loss: 1.9296  reconstruction loss:  1.92685500389536 contractive_loss:  27.26166429840463\n",
      "====> Epoch: 1871 Average loss: 1.7191  reconstruction loss:  1.716059609088868 contractive_loss:  29.932798003696934\n",
      "====> Epoch: 1872 Average loss: 0.8658  reconstruction loss:  0.862230576849149 contractive_loss:  35.84484883762554\n",
      "====> Epoch: 1873 Average loss: 1.6749  reconstruction loss:  1.671604834116132 contractive_loss:  32.67568995397782\n",
      "====> Epoch: 1874 Average loss: 1.8075  reconstruction loss:  1.8044972039082203 contractive_loss:  29.84424232601764\n",
      "====> Epoch: 1875 Average loss: 1.9162  reconstruction loss:  1.913588335057331 contractive_loss:  26.33996278931156\n",
      "====> Epoch: 1876 Average loss: 0.8235  reconstruction loss:  0.8196229277944098 contractive_loss:  38.35091268661152\n",
      "====> Epoch: 1877 Average loss: 0.8535  reconstruction loss:  0.8492515045890485 contractive_loss:  42.05103506103187\n",
      "====> Epoch: 1878 Average loss: 0.7343  reconstruction loss:  0.7304237892902475 contractive_loss:  38.75852133692789\n",
      "====> Epoch: 1879 Average loss: 0.9551  reconstruction loss:  0.9516192486924101 contractive_loss:  34.64326087500401\n",
      "====> Epoch: 1880 Average loss: 1.7850  reconstruction loss:  1.7820292360401484 contractive_loss:  29.960476863610328\n",
      "====> Epoch: 1881 Average loss: 1.6600  reconstruction loss:  1.6573394234486356 contractive_loss:  26.902657103506925\n",
      "====> Epoch: 1882 Average loss: 2.0980  reconstruction loss:  2.0926129155892714 contractive_loss:  53.66236751041461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1883 Average loss: 1.8326  reconstruction loss:  1.825657582405998 contractive_loss:  69.50125853267203\n",
      "====> Epoch: 1884 Average loss: 1.4597  reconstruction loss:  1.4529503444709186 contractive_loss:  67.83141597569703\n",
      "====> Epoch: 1885 Average loss: 1.7022  reconstruction loss:  1.6952069736173154 contractive_loss:  70.12273209171718\n",
      "====> Epoch: 1886 Average loss: 0.9234  reconstruction loss:  0.9169858167538568 contractive_loss:  64.15567195244229\n",
      "====> Epoch: 1887 Average loss: 1.4780  reconstruction loss:  1.470813356554042 contractive_loss:  72.22681578472954\n",
      "====> Epoch: 1888 Average loss: 1.1914  reconstruction loss:  1.1835077541401438 contractive_loss:  79.24106776043857\n",
      "====> Epoch: 1889 Average loss: 1.0771  reconstruction loss:  1.0694565511482272 contractive_loss:  76.18122753200885\n",
      "====> Epoch: 1890 Average loss: 1.6385  reconstruction loss:  1.6308613621473538 contractive_loss:  76.43581804954037\n",
      "====> Epoch: 1891 Average loss: 1.5412  reconstruction loss:  1.5334138515000526 contractive_loss:  77.41218729861008\n",
      "====> Epoch: 1892 Average loss: 0.4221  reconstruction loss:  0.4146319068862248 contractive_loss:  74.69889104335813\n",
      "====> Epoch: 1893 Average loss: 0.5763  reconstruction loss:  0.5688097930127598 contractive_loss:  74.42860384608773\n",
      "====> Epoch: 1894 Average loss: 1.5631  reconstruction loss:  1.5554617266081388 contractive_loss:  76.31981241636846\n",
      "====> Epoch: 1895 Average loss: 1.2119  reconstruction loss:  1.204143217148901 contractive_loss:  77.50527617709476\n",
      "====> Epoch: 1896 Average loss: 0.7066  reconstruction loss:  0.6992506457470989 contractive_loss:  73.54462292596568\n",
      "====> Epoch: 1897 Average loss: 0.7492  reconstruction loss:  0.743598707458578 contractive_loss:  55.640680562355016\n",
      "====> Epoch: 1898 Average loss: 0.8687  reconstruction loss:  0.8631039178953069 contractive_loss:  55.816600844939714\n",
      "====> Epoch: 1899 Average loss: 1.5073  reconstruction loss:  1.502076782594074 contractive_loss:  51.73869022925499\n",
      "====> Epoch: 1900 Average loss: 0.8517  reconstruction loss:  0.8461149947830602 contractive_loss:  55.93591351733404\n",
      "====> Epoch: 1901 Average loss: 0.7480  reconstruction loss:  0.7423123139107966 contractive_loss:  57.055899759057006\n",
      "====> Epoch: 1902 Average loss: 0.7090  reconstruction loss:  0.7032345078887191 contractive_loss:  57.46986158498993\n",
      "====> Epoch: 1903 Average loss: 1.1264  reconstruction loss:  1.1210112239420473 contractive_loss:  53.63692215696632\n",
      "====> Epoch: 1904 Average loss: 1.6361  reconstruction loss:  1.6310004583107454 contractive_loss:  51.107076154689054\n",
      "====> Epoch: 1905 Average loss: 1.1622  reconstruction loss:  1.1569006702731586 contractive_loss:  53.42407667807422\n",
      "====> Epoch: 1906 Average loss: 1.6050  reconstruction loss:  1.5998545046545751 contractive_loss:  51.86028016190496\n",
      "====> Epoch: 1907 Average loss: 0.8011  reconstruction loss:  0.7955267477114222 contractive_loss:  56.144157148487345\n",
      "====> Epoch: 1908 Average loss: 1.3944  reconstruction loss:  1.389307636424606 contractive_loss:  51.03262613285834\n",
      "====> Epoch: 1909 Average loss: 1.0192  reconstruction loss:  1.013994551278243 contractive_loss:  51.93769956364603\n",
      "====> Epoch: 1910 Average loss: 0.8687  reconstruction loss:  0.8632255495275445 contractive_loss:  55.172182970154935\n",
      "====> Epoch: 1911 Average loss: 1.3611  reconstruction loss:  1.3554015240943351 contractive_loss:  56.685537272304884\n",
      "====> Epoch: 1912 Average loss: 2.3146  reconstruction loss:  2.3097674181486108 contractive_loss:  48.180620230544676\n",
      "====> Epoch: 1913 Average loss: 1.1132  reconstruction loss:  1.1080574293949395 contractive_loss:  51.03602238699509\n",
      "====> Epoch: 1914 Average loss: 0.6232  reconstruction loss:  0.6177659761771028 contractive_loss:  54.73401874949268\n",
      "====> Epoch: 1915 Average loss: 1.5675  reconstruction loss:  1.562597444612208 contractive_loss:  48.82946413796063\n",
      "====> Epoch: 1916 Average loss: 1.0252  reconstruction loss:  1.0202345666707864 contractive_loss:  49.70476351710055\n",
      "====> Epoch: 1917 Average loss: 0.7319  reconstruction loss:  0.7266963117563069 contractive_loss:  51.91340286697768\n",
      "====> Epoch: 1918 Average loss: 3.2284  reconstruction loss:  3.2235048249022755 contractive_loss:  48.72367317095205\n",
      "====> Epoch: 1919 Average loss: 1.2308  reconstruction loss:  1.2261100392366764 contractive_loss:  47.23940614266362\n",
      "====> Epoch: 1920 Average loss: 1.2587  reconstruction loss:  1.2544315990758588 contractive_loss:  42.983240238586966\n",
      "====> Epoch: 1921 Average loss: 0.9279  reconstruction loss:  0.9238112325415335 contractive_loss:  41.128248305468105\n",
      "====> Epoch: 1922 Average loss: 0.5962  reconstruction loss:  0.5920255975432421 contractive_loss:  41.5902280191112\n",
      "====> Epoch: 1923 Average loss: 0.8007  reconstruction loss:  0.7965825401098721 contractive_loss:  41.64574886456228\n",
      "====> Epoch: 1924 Average loss: 0.7540  reconstruction loss:  0.7498055039347892 contractive_loss:  41.56320209370594\n",
      "====> Epoch: 1925 Average loss: 0.5857  reconstruction loss:  0.581556548330287 contractive_loss:  41.43436855225317\n",
      "====> Epoch: 1926 Average loss: 0.3230  reconstruction loss:  0.31889374351598354 contractive_loss:  41.30470980239786\n",
      "model saved!\n",
      "====> Epoch: 1927 Average loss: 0.2972  reconstruction loss:  0.2931096030212078 contractive_loss:  41.12673934916976\n",
      "====> Epoch: 1928 Average loss: 0.4739  reconstruction loss:  0.46976183878561345 contractive_loss:  41.00154944634184\n",
      "====> Epoch: 1929 Average loss: 0.5677  reconstruction loss:  0.5635978119547422 contractive_loss:  40.85820350690591\n",
      "====> Epoch: 1930 Average loss: 0.4198  reconstruction loss:  0.41571354824072543 contractive_loss:  40.83775423851977\n",
      "====> Epoch: 1931 Average loss: 0.7957  reconstruction loss:  0.7916231074370156 contractive_loss:  40.77619224633555\n",
      "====> Epoch: 1932 Average loss: 1.7196  reconstruction loss:  1.7155262258326718 contractive_loss:  40.87876645960894\n",
      "====> Epoch: 1933 Average loss: 1.6271  reconstruction loss:  1.6230133734902634 contractive_loss:  40.874916819450014\n",
      "====> Epoch: 1934 Average loss: 1.7610  reconstruction loss:  1.7569406437853905 contractive_loss:  40.8847246511319\n",
      "====> Epoch: 1935 Average loss: 1.7596  reconstruction loss:  1.7554886642891612 contractive_loss:  40.983281281810385\n",
      "====> Epoch: 1936 Average loss: 1.6699  reconstruction loss:  1.6657835231520906 contractive_loss:  40.94581975870823\n",
      "====> Epoch: 1937 Average loss: 1.9110  reconstruction loss:  1.906939817736596 contractive_loss:  40.955080847932564\n",
      "====> Epoch: 1938 Average loss: 1.7301  reconstruction loss:  1.7260098444535197 contractive_loss:  40.74611277958556\n",
      "====> Epoch: 1939 Average loss: 1.8256  reconstruction loss:  1.8215008304924363 contractive_loss:  40.82163515700229\n",
      "====> Epoch: 1940 Average loss: 1.6688  reconstruction loss:  1.6647678723827766 contractive_loss:  40.778768645790755\n",
      "====> Epoch: 1941 Average loss: 1.7703  reconstruction loss:  1.7662268899113076 contractive_loss:  40.77846965621252\n",
      "====> Epoch: 1942 Average loss: 1.7413  reconstruction loss:  1.7372010848987205 contractive_loss:  40.753476121800915\n",
      "====> Epoch: 1943 Average loss: 1.7451  reconstruction loss:  1.741015889112204 contractive_loss:  40.71498433091959\n",
      "====> Epoch: 1944 Average loss: 1.6717  reconstruction loss:  1.6676090067826272 contractive_loss:  40.68365790768188\n",
      "====> Epoch: 1945 Average loss: 1.6783  reconstruction loss:  1.6742236225651776 contractive_loss:  40.71546822504412\n",
      "====> Epoch: 1946 Average loss: 1.6351  reconstruction loss:  1.6310782843170233 contractive_loss:  40.57740651060651\n",
      "====> Epoch: 1947 Average loss: 1.7085  reconstruction loss:  1.7044354062920581 contractive_loss:  40.48043124663181\n",
      "====> Epoch: 1948 Average loss: 1.6641  reconstruction loss:  1.6600327056671136 contractive_loss:  40.51167559546784\n",
      "====> Epoch: 1949 Average loss: 1.6822  reconstruction loss:  1.6781165049361653 contractive_loss:  40.53170931048059\n",
      "====> Epoch: 1950 Average loss: 1.6822  reconstruction loss:  1.6781754688890569 contractive_loss:  40.38331068071041\n",
      "====> Epoch: 1951 Average loss: 1.4469  reconstruction loss:  1.4428333538936027 contractive_loss:  40.307047964909216\n",
      "====> Epoch: 1952 Average loss: 1.6041  reconstruction loss:  1.6001240218986523 contractive_loss:  40.24345362399589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1953 Average loss: 2.3063  reconstruction loss:  2.30223873126619 contractive_loss:  40.16012587991662\n",
      "====> Epoch: 1954 Average loss: 1.7388  reconstruction loss:  1.7347819059402065 contractive_loss:  40.26996081701975\n",
      "====> Epoch: 1955 Average loss: 1.7781  reconstruction loss:  1.7740963581913984 contractive_loss:  40.28652448501676\n",
      "====> Epoch: 1956 Average loss: 1.9168  reconstruction loss:  1.9127261220018148 contractive_loss:  40.242829021737755\n",
      "====> Epoch: 1957 Average loss: 1.8904  reconstruction loss:  1.8863417507476703 contractive_loss:  40.18904296385357\n",
      "====> Epoch: 1958 Average loss: 1.6155  reconstruction loss:  1.6115078190390326 contractive_loss:  40.114807923911115\n",
      "====> Epoch: 1959 Average loss: 1.4279  reconstruction loss:  1.423907131439153 contractive_loss:  40.09440711350392\n",
      "====> Epoch: 1960 Average loss: 1.5983  reconstruction loss:  1.5942708086099777 contractive_loss:  39.99941291233068\n",
      "====> Epoch: 1961 Average loss: 1.5942  reconstruction loss:  1.5902139766245234 contractive_loss:  39.97547222885807\n",
      "====> Epoch: 1962 Average loss: 1.7919  reconstruction loss:  1.7878646699963232 contractive_loss:  39.93554413801796\n",
      "====> Epoch: 1963 Average loss: 2.1002  reconstruction loss:  2.0962028889604123 contractive_loss:  39.92974707965692\n",
      "====> Epoch: 1964 Average loss: 1.1601  reconstruction loss:  1.156084242247983 contractive_loss:  39.94993504334009\n",
      "====> Epoch: 1965 Average loss: 1.2970  reconstruction loss:  1.2929924728312165 contractive_loss:  39.88084695400237\n",
      "====> Epoch: 1966 Average loss: 1.1100  reconstruction loss:  1.1059897716898255 contractive_loss:  39.918716721809865\n",
      "====> Epoch: 1967 Average loss: 1.4112  reconstruction loss:  1.407238154700447 contractive_loss:  39.79441473246884\n",
      "====> Epoch: 1968 Average loss: 1.8022  reconstruction loss:  1.7981950150312174 contractive_loss:  39.586962361564865\n",
      "====> Epoch: 1969 Average loss: 1.7916  reconstruction loss:  1.7876525919476176 contractive_loss:  39.66648058198329\n",
      "====> Epoch: 1970 Average loss: 1.9683  reconstruction loss:  1.9642984584573395 contractive_loss:  39.55429886032875\n",
      "====> Epoch: 1971 Average loss: 4.6767  reconstruction loss:  4.6727669212509655 contractive_loss:  39.52209103687693\n",
      "====> Epoch: 1972 Average loss: 3.1861  reconstruction loss:  3.182112770032677 contractive_loss:  39.70014981193658\n",
      "====> Epoch: 1973 Average loss: 2.0462  reconstruction loss:  2.04226361394533 contractive_loss:  39.861881710379286\n",
      "====> Epoch: 1974 Average loss: 2.2572  reconstruction loss:  2.2532179826029126 contractive_loss:  39.503166823818276\n",
      "====> Epoch: 1975 Average loss: 2.1415  reconstruction loss:  2.1375716165057246 contractive_loss:  39.04401939896145\n",
      "====> Epoch: 1976 Average loss: 1.7316  reconstruction loss:  1.7276832819177568 contractive_loss:  39.528311716830075\n",
      "====> Epoch: 1977 Average loss: 1.4370  reconstruction loss:  1.4330307725152922 contractive_loss:  39.59061295335633\n",
      "====> Epoch: 1978 Average loss: 1.7377  reconstruction loss:  1.7337652361504454 contractive_loss:  39.29144184063184\n",
      "====> Epoch: 1979 Average loss: 1.9449  reconstruction loss:  1.9409651617777748 contractive_loss:  38.96458240832339\n",
      "====> Epoch: 1980 Average loss: 2.2139  reconstruction loss:  2.2100534614207366 contractive_loss:  38.75802590385452\n",
      "====> Epoch: 1981 Average loss: 1.9395  reconstruction loss:  1.9355872697914214 contractive_loss:  38.79355475799162\n",
      "====> Epoch: 1982 Average loss: 2.0400  reconstruction loss:  2.0361255487038203 contractive_loss:  38.82757444807524\n",
      "====> Epoch: 1983 Average loss: 2.2721  reconstruction loss:  2.268244835732897 contractive_loss:  38.91420167095252\n",
      "====> Epoch: 1984 Average loss: 2.0023  reconstruction loss:  1.9984751938033372 contractive_loss:  38.42261060984377\n",
      "====> Epoch: 1985 Average loss: 1.3108  reconstruction loss:  1.306855486369892 contractive_loss:  39.11301163968942\n",
      "====> Epoch: 1986 Average loss: 1.6129  reconstruction loss:  1.6090426999342284 contractive_loss:  38.98227567206962\n",
      "====> Epoch: 1987 Average loss: 1.7374  reconstruction loss:  1.7334937061900035 contractive_loss:  38.77096534319693\n",
      "====> Epoch: 1988 Average loss: 2.1383  reconstruction loss:  2.134449598775469 contractive_loss:  38.327847417987904\n",
      "====> Epoch: 1989 Average loss: 1.9072  reconstruction loss:  1.9034034034942564 contractive_loss:  38.134663083875424\n",
      "====> Epoch: 1990 Average loss: 2.0971  reconstruction loss:  2.0932678744061946 contractive_loss:  38.150832378034224\n",
      "====> Epoch: 1991 Average loss: 1.5892  reconstruction loss:  1.5853534142995624 contractive_loss:  38.70482408430802\n",
      "====> Epoch: 1992 Average loss: 1.8522  reconstruction loss:  1.8483722102143767 contractive_loss:  38.63109522102042\n",
      "====> Epoch: 1993 Average loss: 2.2746  reconstruction loss:  2.270795076195418 contractive_loss:  38.48851127205723\n",
      "====> Epoch: 1994 Average loss: 2.0387  reconstruction loss:  2.034847927452529 contractive_loss:  38.029169874807955\n",
      "====> Epoch: 1995 Average loss: 1.9427  reconstruction loss:  1.9388945996863998 contractive_loss:  38.38989667371717\n",
      "====> Epoch: 1996 Average loss: 1.9852  reconstruction loss:  1.9813937042923886 contractive_loss:  38.10752331894837\n",
      "====> Epoch: 1997 Average loss: 1.9291  reconstruction loss:  1.925353715911398 contractive_loss:  37.86632578256981\n",
      "====> Epoch: 1998 Average loss: 1.8727  reconstruction loss:  1.8688824720773118 contractive_loss:  37.98187345277293\n",
      "====> Epoch: 1999 Average loss: 1.9043  reconstruction loss:  1.9004758072416683 contractive_loss:  37.911407585913885\n",
      "====> Epoch: 2000 Average loss: 1.8617  reconstruction loss:  1.8579155804747385 contractive_loss:  37.874069399636575\n",
      "====> Epoch: 2001 Average loss: 1.9051  reconstruction loss:  1.90129666279267 contractive_loss:  37.73951748970294\n",
      "====> Epoch: 2002 Average loss: 1.8555  reconstruction loss:  1.8517244691626555 contractive_loss:  37.749467813826634\n",
      "====> Epoch: 2003 Average loss: 1.9172  reconstruction loss:  1.9134342409225642 contractive_loss:  37.653300895981296\n",
      "====> Epoch: 2004 Average loss: 1.8934  reconstruction loss:  1.8896478231958362 contractive_loss:  37.62075763520231\n",
      "====> Epoch: 2005 Average loss: 1.9634  reconstruction loss:  1.9595861231446805 contractive_loss:  37.65624713116016\n",
      "====> Epoch: 2006 Average loss: 1.8662  reconstruction loss:  1.86246318959103 contractive_loss:  37.58012937178699\n",
      "====> Epoch: 2007 Average loss: 1.8091  reconstruction loss:  1.8053806462533584 contractive_loss:  37.685813625308526\n",
      "====> Epoch: 2008 Average loss: 1.9023  reconstruction loss:  1.8985703360576784 contractive_loss:  37.31821786280758\n",
      "====> Epoch: 2009 Average loss: 1.8421  reconstruction loss:  1.8383436289814794 contractive_loss:  37.46149741968433\n",
      "====> Epoch: 2010 Average loss: 2.0910  reconstruction loss:  2.0872465404810923 contractive_loss:  37.12463214098718\n",
      "====> Epoch: 2011 Average loss: 1.8861  reconstruction loss:  1.8823605686011446 contractive_loss:  37.053262738498184\n",
      "====> Epoch: 2012 Average loss: 1.8443  reconstruction loss:  1.8405090770824024 contractive_loss:  37.440897258030816\n",
      "====> Epoch: 2013 Average loss: 1.8218  reconstruction loss:  1.8181293399083316 contractive_loss:  37.17334549513801\n",
      "====> Epoch: 2014 Average loss: 1.8107  reconstruction loss:  1.8070053163650024 contractive_loss:  36.95505178936644\n",
      "====> Epoch: 2015 Average loss: 1.8040  reconstruction loss:  1.8002380692929225 contractive_loss:  37.19037482269592\n",
      "====> Epoch: 2016 Average loss: 1.8716  reconstruction loss:  1.8679362131260577 contractive_loss:  36.93827560498444\n",
      "====> Epoch: 2017 Average loss: 1.9537  reconstruction loss:  1.950001089651791 contractive_loss:  36.6537543392517\n",
      "====> Epoch: 2018 Average loss: 1.9356  reconstruction loss:  1.9318831030201034 contractive_loss:  37.2510591078093\n",
      "====> Epoch: 2019 Average loss: 1.8367  reconstruction loss:  1.8330124649988573 contractive_loss:  36.89982720068941\n",
      "====> Epoch: 2020 Average loss: 1.8539  reconstruction loss:  1.8501979264518946 contractive_loss:  36.53227651487742\n",
      "====> Epoch: 2021 Average loss: 1.7954  reconstruction loss:  1.7917518727745836 contractive_loss:  36.639965594524725\n",
      "====> Epoch: 2022 Average loss: 1.9385  reconstruction loss:  1.9348609896568576 contractive_loss:  36.604895697034266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2023 Average loss: 2.0756  reconstruction loss:  2.0719872738942895 contractive_loss:  36.49222439335505\n",
      "====> Epoch: 2024 Average loss: 2.1353  reconstruction loss:  2.1316799048366635 contractive_loss:  36.407851660721114\n",
      "====> Epoch: 2025 Average loss: 1.7788  reconstruction loss:  1.7750959908723614 contractive_loss:  36.91865642635532\n",
      "====> Epoch: 2026 Average loss: 1.8555  reconstruction loss:  1.8518710296489482 contractive_loss:  36.558502386501104\n",
      "====> Epoch: 2027 Average loss: 1.7665  reconstruction loss:  1.7627927757318855 contractive_loss:  36.74920173896592\n",
      "model saved!\n",
      "====> Epoch: 2028 Average loss: 0.2283  reconstruction loss:  0.22441520439790033 contractive_loss:  38.4346004986588\n",
      "====> Epoch: 2029 Average loss: 1.8998  reconstruction loss:  1.895997167478756 contractive_loss:  37.58834926502574\n",
      "====> Epoch: 2030 Average loss: 1.9574  reconstruction loss:  1.953637187317415 contractive_loss:  37.87131557915878\n",
      "====> Epoch: 2031 Average loss: 0.2622  reconstruction loss:  0.25822844048807797 contractive_loss:  40.19515314630516\n",
      "====> Epoch: 2032 Average loss: 0.2631  reconstruction loss:  0.2590849668696007 contractive_loss:  40.18620421477169\n",
      "====> Epoch: 2033 Average loss: 0.2469  reconstruction loss:  0.24287519283721606 contractive_loss:  40.31885698437707\n",
      "====> Epoch: 2034 Average loss: 0.3241  reconstruction loss:  0.3200940816695797 contractive_loss:  39.902434465119214\n",
      "====> Epoch: 2035 Average loss: 0.3437  reconstruction loss:  0.3398144421719263 contractive_loss:  39.015365694853514\n",
      "====> Epoch: 2036 Average loss: 0.9248  reconstruction loss:  0.9209776443413656 contractive_loss:  38.5169494600668\n",
      "====> Epoch: 2037 Average loss: 1.7254  reconstruction loss:  1.7216379802672994 contractive_loss:  37.6809897352733\n",
      "====> Epoch: 2038 Average loss: 1.7937  reconstruction loss:  1.7899722393450728 contractive_loss:  37.190004434245694\n",
      "====> Epoch: 2039 Average loss: 1.7790  reconstruction loss:  1.7752502499167855 contractive_loss:  37.28844775069934\n",
      "====> Epoch: 2040 Average loss: 1.9437  reconstruction loss:  1.9400229884818696 contractive_loss:  37.26236597554491\n",
      "====> Epoch: 2041 Average loss: 1.7946  reconstruction loss:  1.7909091808903017 contractive_loss:  36.89874250359571\n",
      "====> Epoch: 2042 Average loss: 1.3076  reconstruction loss:  1.3038329773308457 contractive_loss:  37.81803038983513\n",
      "====> Epoch: 2043 Average loss: 1.5022  reconstruction loss:  1.498472557336564 contractive_loss:  37.3696300233672\n",
      "====> Epoch: 2044 Average loss: 1.8579  reconstruction loss:  1.8542055372213448 contractive_loss:  36.78210894182739\n",
      "====> Epoch: 2045 Average loss: 1.8864  reconstruction loss:  1.8827641226821172 contractive_loss:  36.55380133305823\n",
      "====> Epoch: 2046 Average loss: 3.2776  reconstruction loss:  3.2740067240264743 contractive_loss:  36.21029996907534\n",
      "====> Epoch: 2047 Average loss: 2.3084  reconstruction loss:  2.3047104714205755 contractive_loss:  36.57585333618484\n",
      "====> Epoch: 2048 Average loss: 2.2981  reconstruction loss:  2.294456879416931 contractive_loss:  36.79732910095493\n",
      "====> Epoch: 2049 Average loss: 2.4065  reconstruction loss:  2.402862490307824 contractive_loss:  36.11600955161401\n",
      "====> Epoch: 2050 Average loss: 1.4198  reconstruction loss:  1.4160425104404328 contractive_loss:  37.227906276246145\n",
      "====> Epoch: 2051 Average loss: 0.2881  reconstruction loss:  0.28431753107414565 contractive_loss:  37.78533572491657\n",
      "====> Epoch: 2052 Average loss: 0.3864  reconstruction loss:  0.38264235932264495 contractive_loss:  37.3514304718958\n",
      "====> Epoch: 2053 Average loss: 0.7857  reconstruction loss:  0.7820094718563568 contractive_loss:  36.96096573515192\n",
      "====> Epoch: 2054 Average loss: 1.9916  reconstruction loss:  1.987984183124987 contractive_loss:  35.78252224728415\n",
      "====> Epoch: 2055 Average loss: 1.8013  reconstruction loss:  1.7977739972349758 contractive_loss:  35.26525207806179\n",
      "====> Epoch: 2056 Average loss: 1.3222  reconstruction loss:  1.3185658386893866 contractive_loss:  36.44357392183428\n",
      "model saved!\n",
      "====> Epoch: 2057 Average loss: 0.1952  reconstruction loss:  0.19138299131549008 contractive_loss:  37.957424974915966\n",
      "====> Epoch: 2058 Average loss: 0.6399  reconstruction loss:  0.6361445902831899 contractive_loss:  37.48486158219896\n",
      "====> Epoch: 2059 Average loss: 0.2029  reconstruction loss:  0.19919667952992157 contractive_loss:  36.95401541172646\n",
      "====> Epoch: 2060 Average loss: 0.7245  reconstruction loss:  0.7208666604305982 contractive_loss:  36.07934764985225\n",
      "====> Epoch: 2061 Average loss: 1.6886  reconstruction loss:  1.6851790073877058 contractive_loss:  34.40713140711734\n",
      "====> Epoch: 2062 Average loss: 0.8209  reconstruction loss:  0.8173342931260098 contractive_loss:  35.806386937545\n",
      "====> Epoch: 2063 Average loss: 0.7507  reconstruction loss:  0.7471106491432105 contractive_loss:  35.69796263743646\n",
      "====> Epoch: 2064 Average loss: 1.7667  reconstruction loss:  1.7632834045210177 contractive_loss:  34.63280142843359\n",
      "====> Epoch: 2065 Average loss: 1.6780  reconstruction loss:  1.6746029546425438 contractive_loss:  33.96367443382556\n",
      "====> Epoch: 2066 Average loss: 1.4965  reconstruction loss:  1.4930480072643768 contractive_loss:  34.67454526402718\n",
      "====> Epoch: 2067 Average loss: 1.5163  reconstruction loss:  1.5127967232506185 contractive_loss:  34.64504284370905\n",
      "====> Epoch: 2068 Average loss: 1.7064  reconstruction loss:  1.7030186192016532 contractive_loss:  34.212070753580974\n",
      "====> Epoch: 2069 Average loss: 1.8411  reconstruction loss:  1.837799348348771 contractive_loss:  33.33295089211419\n",
      "====> Epoch: 2070 Average loss: 1.9638  reconstruction loss:  1.9603709052732414 contractive_loss:  33.8622431122236\n",
      "====> Epoch: 2071 Average loss: 1.7753  reconstruction loss:  1.7719317885012424 contractive_loss:  33.69532150035879\n",
      "====> Epoch: 2072 Average loss: 1.7664  reconstruction loss:  1.7630521189803 contractive_loss:  33.4303196281864\n",
      "====> Epoch: 2073 Average loss: 1.7630  reconstruction loss:  1.759651295761782 contractive_loss:  33.61808657398154\n",
      "====> Epoch: 2074 Average loss: 1.7787  reconstruction loss:  1.775366979442876 contractive_loss:  33.29308777368872\n",
      "====> Epoch: 2075 Average loss: 1.7594  reconstruction loss:  1.7560618998827622 contractive_loss:  33.14559098761002\n",
      "====> Epoch: 2076 Average loss: 1.3932  reconstruction loss:  1.3897835673322771 contractive_loss:  33.967000565816775\n",
      "====> Epoch: 2077 Average loss: 1.7871  reconstruction loss:  1.7837834917953108 contractive_loss:  33.12218070404444\n",
      "====> Epoch: 2078 Average loss: 1.7996  reconstruction loss:  1.7963451244168476 contractive_loss:  32.942229446202475\n",
      "====> Epoch: 2079 Average loss: 1.7255  reconstruction loss:  1.722303972607473 contractive_loss:  32.29441624986864\n",
      "====> Epoch: 2080 Average loss: 0.4594  reconstruction loss:  0.45595665534707364 contractive_loss:  34.15240779556457\n",
      "====> Epoch: 2081 Average loss: 0.6819  reconstruction loss:  0.6785212375624404 contractive_loss:  33.56527690182405\n",
      "====> Epoch: 2082 Average loss: 0.5210  reconstruction loss:  0.5175912538462732 contractive_loss:  33.60439356597418\n",
      "====> Epoch: 2083 Average loss: 1.5883  reconstruction loss:  1.5850801067776168 contractive_loss:  32.69809833428964\n",
      "====> Epoch: 2084 Average loss: 1.8349  reconstruction loss:  1.8316815343905444 contractive_loss:  32.40050682297992\n",
      "====> Epoch: 2085 Average loss: 1.7745  reconstruction loss:  1.7712581295671714 contractive_loss:  32.19251876511718\n",
      "====> Epoch: 2086 Average loss: 1.8666  reconstruction loss:  1.863433514824149 contractive_loss:  32.05772691834142\n",
      "====> Epoch: 2087 Average loss: 1.6537  reconstruction loss:  1.6505281454714877 contractive_loss:  32.165830478376506\n",
      "====> Epoch: 2088 Average loss: 2.1205  reconstruction loss:  2.117365449489336 contractive_loss:  31.678895728596256\n",
      "====> Epoch: 2089 Average loss: 1.8405  reconstruction loss:  1.8373702371942315 contractive_loss:  31.75059259756299\n",
      "====> Epoch: 2090 Average loss: 1.7230  reconstruction loss:  1.719913834875288 contractive_loss:  31.077802476502047\n",
      "====> Epoch: 2091 Average loss: 0.3840  reconstruction loss:  0.38070860862786515 contractive_loss:  32.6503837911448\n",
      "====> Epoch: 2092 Average loss: 0.4482  reconstruction loss:  0.4448886940491073 contractive_loss:  32.72200515834066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2093 Average loss: 0.9896  reconstruction loss:  0.9863424522407079 contractive_loss:  32.14951087555133\n",
      "====> Epoch: 2094 Average loss: 1.9920  reconstruction loss:  1.9888923932759495 contractive_loss:  30.93900988193298\n",
      "====> Epoch: 2095 Average loss: 1.8425  reconstruction loss:  1.8394079214534023 contractive_loss:  30.890434306499532\n",
      "====> Epoch: 2096 Average loss: 1.8306  reconstruction loss:  1.8275673120701519 contractive_loss:  30.521246874964717\n",
      "====> Epoch: 2097 Average loss: 0.6898  reconstruction loss:  0.6864603305931695 contractive_loss:  33.45711933849717\n",
      "====> Epoch: 2098 Average loss: 0.3988  reconstruction loss:  0.3954785547984168 contractive_loss:  33.11821111824857\n",
      "====> Epoch: 2099 Average loss: 0.1973  reconstruction loss:  0.19406562570097186 contractive_loss:  32.3489738488377\n",
      "====> Epoch: 2100 Average loss: 0.9583  reconstruction loss:  0.9554190949808445 contractive_loss:  28.401808058517865\n",
      "====> Epoch: 2101 Average loss: 2.0026  reconstruction loss:  1.9983689903997166 contractive_loss:  42.41545163096312\n",
      "====> Epoch: 2102 Average loss: 2.2233  reconstruction loss:  2.2163291888326584 contractive_loss:  70.0515693101398\n",
      "====> Epoch: 2103 Average loss: 1.7443  reconstruction loss:  1.7383179676697762 contractive_loss:  60.31496813273444\n",
      "====> Epoch: 2104 Average loss: 1.3124  reconstruction loss:  1.3070591132013127 contractive_loss:  52.95794041283886\n",
      "====> Epoch: 2105 Average loss: 0.4174  reconstruction loss:  0.41303743862475556 contractive_loss:  43.241621717491256\n",
      "====> Epoch: 2106 Average loss: 0.3921  reconstruction loss:  0.38765573735632086 contractive_loss:  44.691198044641474\n",
      "====> Epoch: 2107 Average loss: 1.3335  reconstruction loss:  1.3292291084265857 contractive_loss:  42.89552719221602\n",
      "====> Epoch: 2108 Average loss: 1.8958  reconstruction loss:  1.891564102445694 contractive_loss:  42.276433272287136\n",
      "====> Epoch: 2109 Average loss: 0.5161  reconstruction loss:  0.5117929392459306 contractive_loss:  43.13145462542395\n",
      "====> Epoch: 2110 Average loss: 1.7037  reconstruction loss:  1.6995009050351209 contractive_loss:  42.13159115949038\n",
      "====> Epoch: 2111 Average loss: 1.5329  reconstruction loss:  1.5287424613388234 contractive_loss:  41.718673865889336\n",
      "====> Epoch: 2112 Average loss: 1.5353  reconstruction loss:  1.5309038234772714 contractive_loss:  43.938841947102105\n",
      "====> Epoch: 2113 Average loss: 1.2507  reconstruction loss:  1.2454793474336194 contractive_loss:  51.99009540147909\n",
      "====> Epoch: 2114 Average loss: 1.3957  reconstruction loss:  1.3913310488139243 contractive_loss:  44.13183761217286\n",
      "====> Epoch: 2115 Average loss: 0.8685  reconstruction loss:  0.8642545848782345 contractive_loss:  42.55901511445198\n",
      "====> Epoch: 2116 Average loss: 2.7633  reconstruction loss:  2.75733643587612 contractive_loss:  59.14054943099781\n",
      "====> Epoch: 2117 Average loss: 3.0501  reconstruction loss:  3.0408215431438053 contractive_loss:  93.0254173856118\n",
      "====> Epoch: 2118 Average loss: 1.9989  reconstruction loss:  1.9925893748761092 contractive_loss:  62.99278508586915\n",
      "====> Epoch: 2119 Average loss: 2.0765  reconstruction loss:  2.067580208553644 contractive_loss:  89.28915995456481\n",
      "====> Epoch: 2120 Average loss: 2.3056  reconstruction loss:  2.2999124466208825 contractive_loss:  57.04742870091751\n",
      "====> Epoch: 2121 Average loss: 1.5955  reconstruction loss:  1.5911622772141676 contractive_loss:  42.91554913441091\n",
      "====> Epoch: 2122 Average loss: 0.6559  reconstruction loss:  0.6512795250310878 contractive_loss:  45.827205192343655\n",
      "====> Epoch: 2123 Average loss: 1.7029  reconstruction loss:  1.6985770027445315 contractive_loss:  43.666328431969234\n",
      "====> Epoch: 2124 Average loss: 1.3349  reconstruction loss:  1.3304091944438117 contractive_loss:  45.065581488883964\n",
      "====> Epoch: 2125 Average loss: 1.8577  reconstruction loss:  1.8504171876690632 contractive_loss:  72.4833168260278\n",
      "====> Epoch: 2126 Average loss: 1.6906  reconstruction loss:  1.6837117840697688 contractive_loss:  69.03939577871405\n",
      "====> Epoch: 2127 Average loss: 2.1100  reconstruction loss:  2.1050330984880548 contractive_loss:  49.3790096570005\n",
      "====> Epoch: 2128 Average loss: 0.5710  reconstruction loss:  0.566510176554154 contractive_loss:  45.011215845054956\n",
      "====> Epoch: 2129 Average loss: 0.6213  reconstruction loss:  0.6166560549065774 contractive_loss:  46.5855076557465\n",
      "====> Epoch: 2130 Average loss: 0.3211  reconstruction loss:  0.3162467189584779 contractive_loss:  48.215599701822995\n",
      "====> Epoch: 2131 Average loss: 0.3869  reconstruction loss:  0.38212051554743043 contractive_loss:  47.99911497788188\n",
      "====> Epoch: 2132 Average loss: 0.4849  reconstruction loss:  0.48017283085690826 contractive_loss:  47.542499347479925\n",
      "====> Epoch: 2133 Average loss: 0.5279  reconstruction loss:  0.5231441335677027 contractive_loss:  47.73930555627409\n",
      "====> Epoch: 2134 Average loss: 0.6078  reconstruction loss:  0.6030228748913288 contractive_loss:  47.50768710461242\n",
      "====> Epoch: 2135 Average loss: 0.8835  reconstruction loss:  0.878753716092297 contractive_loss:  47.27355326440547\n",
      "====> Epoch: 2136 Average loss: 0.7270  reconstruction loss:  0.7223619372148474 contractive_loss:  46.290529638040496\n",
      "====> Epoch: 2137 Average loss: 1.6177  reconstruction loss:  1.6131384924008623 contractive_loss:  45.66868930841248\n",
      "====> Epoch: 2138 Average loss: 1.6145  reconstruction loss:  1.6099583020467945 contractive_loss:  45.48278479014842\n",
      "====> Epoch: 2139 Average loss: 1.7236  reconstruction loss:  1.7188546682066375 contractive_loss:  47.6755442745349\n",
      "====> Epoch: 2140 Average loss: 2.9608  reconstruction loss:  2.951967506016152 contractive_loss:  88.60232651725073\n",
      "====> Epoch: 2141 Average loss: 3.7844  reconstruction loss:  3.7752773778463777 contractive_loss:  91.58596632365196\n",
      "====> Epoch: 2142 Average loss: 2.0348  reconstruction loss:  2.0254870093019135 contractive_loss:  93.24047375216152\n",
      "====> Epoch: 2143 Average loss: 1.4129  reconstruction loss:  1.4036736935755458 contractive_loss:  91.8061085911863\n",
      "====> Epoch: 2144 Average loss: 1.2305  reconstruction loss:  1.2213937536921156 contractive_loss:  91.45558652727183\n",
      "====> Epoch: 2145 Average loss: 1.6599  reconstruction loss:  1.6504857856021895 contractive_loss:  94.4914892697844\n",
      "====> Epoch: 2146 Average loss: 1.3443  reconstruction loss:  1.3349816818331153 contractive_loss:  93.67592977880454\n",
      "====> Epoch: 2147 Average loss: 0.4567  reconstruction loss:  0.4470674715131318 contractive_loss:  96.36317737616935\n",
      "====> Epoch: 2148 Average loss: 0.8085  reconstruction loss:  0.8051157180868792 contractive_loss:  33.908458301586926\n",
      "====> Epoch: 2149 Average loss: 0.4767  reconstruction loss:  0.47443528512806965 contractive_loss:  22.57751455723257\n",
      "====> Epoch: 2150 Average loss: 0.5465  reconstruction loss:  0.5439052669808706 contractive_loss:  26.06273215680403\n",
      "====> Epoch: 2151 Average loss: 0.5107  reconstruction loss:  0.5076828050193428 contractive_loss:  30.128589681885646\n",
      "====> Epoch: 2152 Average loss: 0.6018  reconstruction loss:  0.597970287599902 contractive_loss:  38.0431810897645\n",
      "====> Epoch: 2153 Average loss: 0.4289  reconstruction loss:  0.4231709488669545 contractive_loss:  56.83253882494329\n",
      "====> Epoch: 2154 Average loss: 1.1493  reconstruction loss:  1.1398322670900913 contractive_loss:  94.98260951617284\n",
      "====> Epoch: 2155 Average loss: 1.1664  reconstruction loss:  1.1620922914641094 contractive_loss:  42.965277133923685\n",
      "====> Epoch: 2156 Average loss: 0.3866  reconstruction loss:  0.38181240672386607 contractive_loss:  47.83313050615124\n",
      "====> Epoch: 2157 Average loss: 1.0825  reconstruction loss:  1.077865588098352 contractive_loss:  46.76608328805556\n",
      "====> Epoch: 2158 Average loss: 1.0134  reconstruction loss:  1.00879137886093 contractive_loss:  45.74785953447992\n",
      "====> Epoch: 2159 Average loss: 0.7263  reconstruction loss:  0.7213992176859253 contractive_loss:  48.57856115380659\n",
      "====> Epoch: 2160 Average loss: 0.6561  reconstruction loss:  0.6516882731289902 contractive_loss:  44.280102251178164\n",
      "====> Epoch: 2161 Average loss: 1.2631  reconstruction loss:  1.25620627537637 contractive_loss:  69.19860203125688\n",
      "====> Epoch: 2162 Average loss: 1.8282  reconstruction loss:  1.820555890517257 contractive_loss:  76.33814359993592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2163 Average loss: 0.9185  reconstruction loss:  0.9141491124649671 contractive_loss:  43.73579472176254\n",
      "====> Epoch: 2164 Average loss: 0.3785  reconstruction loss:  0.3737343058445429 contractive_loss:  48.13169509525782\n",
      "====> Epoch: 2165 Average loss: 0.2829  reconstruction loss:  0.2780316053684746 contractive_loss:  48.6806669919978\n",
      "====> Epoch: 2166 Average loss: 1.6098  reconstruction loss:  1.6050683020828496 contractive_loss:  46.88979601472225\n",
      "====> Epoch: 2167 Average loss: 1.4523  reconstruction loss:  1.4476084175158643 contractive_loss:  46.883042508287\n",
      "====> Epoch: 2168 Average loss: 1.4741  reconstruction loss:  1.4693877322823967 contractive_loss:  46.66367192630509\n",
      "====> Epoch: 2169 Average loss: 1.4474  reconstruction loss:  1.4427415872762779 contractive_loss:  46.382154322720396\n",
      "====> Epoch: 2170 Average loss: 1.6884  reconstruction loss:  1.6837389532874036 contractive_loss:  46.44445385498349\n",
      "====> Epoch: 2171 Average loss: 1.5789  reconstruction loss:  1.5742231422199993 contractive_loss:  46.36273629729047\n",
      "====> Epoch: 2172 Average loss: 1.4811  reconstruction loss:  1.4764850549414428 contractive_loss:  46.123256765259164\n",
      "====> Epoch: 2173 Average loss: 1.4083  reconstruction loss:  1.4037537864767649 contractive_loss:  45.9545955110577\n",
      "====> Epoch: 2174 Average loss: 1.7592  reconstruction loss:  1.7545647514101557 contractive_loss:  46.16056457575124\n",
      "====> Epoch: 2175 Average loss: 1.2376  reconstruction loss:  1.2329309741606636 contractive_loss:  46.536478265548844\n",
      "====> Epoch: 2176 Average loss: 1.2966  reconstruction loss:  1.2919778435753495 contractive_loss:  46.30518858599404\n",
      "====> Epoch: 2177 Average loss: 1.6384  reconstruction loss:  1.6337626511134957 contractive_loss:  45.9306787304099\n",
      "====> Epoch: 2178 Average loss: 1.6338  reconstruction loss:  1.629278619231557 contractive_loss:  45.71287759107026\n",
      "====> Epoch: 2179 Average loss: 1.5825  reconstruction loss:  1.5779955092818023 contractive_loss:  45.254918086934794\n",
      "====> Epoch: 2180 Average loss: 1.5883  reconstruction loss:  1.583745502348744 contractive_loss:  45.32165587498873\n",
      "====> Epoch: 2181 Average loss: 1.5329  reconstruction loss:  1.5284384295802147 contractive_loss:  45.07530200852852\n",
      "====> Epoch: 2182 Average loss: 1.5850  reconstruction loss:  1.5805201145528074 contractive_loss:  44.906161021123914\n",
      "====> Epoch: 2183 Average loss: 1.6256  reconstruction loss:  1.6210772403603166 contractive_loss:  44.92088072079981\n",
      "====> Epoch: 2184 Average loss: 1.6152  reconstruction loss:  1.610752877344659 contractive_loss:  44.794400584056376\n",
      "====> Epoch: 2185 Average loss: 1.6043  reconstruction loss:  1.599848777593144 contractive_loss:  44.66241854403108\n",
      "====> Epoch: 2186 Average loss: 1.7585  reconstruction loss:  1.7540820562960477 contractive_loss:  44.608093127755325\n",
      "====> Epoch: 2187 Average loss: 1.7791  reconstruction loss:  1.7746059960192353 contractive_loss:  44.71119265544549\n",
      "====> Epoch: 2188 Average loss: 1.5440  reconstruction loss:  1.5395907738633707 contractive_loss:  44.07829501188927\n",
      "====> Epoch: 2189 Average loss: 1.7145  reconstruction loss:  1.7100906466208021 contractive_loss:  44.127416207840966\n",
      "====> Epoch: 2190 Average loss: 1.6274  reconstruction loss:  1.6230100703047734 contractive_loss:  44.010214504364896\n",
      "====> Epoch: 2191 Average loss: 1.6670  reconstruction loss:  1.6626199002882514 contractive_loss:  43.944594677182664\n",
      "====> Epoch: 2192 Average loss: 1.7761  reconstruction loss:  1.771678296341301 contractive_loss:  43.88855089493256\n",
      "====> Epoch: 2193 Average loss: 1.6848  reconstruction loss:  1.6803965374552126 contractive_loss:  43.79117159628517\n",
      "====> Epoch: 2194 Average loss: 1.6496  reconstruction loss:  1.6452474523001392 contractive_loss:  43.545202057301694\n",
      "====> Epoch: 2195 Average loss: 1.6658  reconstruction loss:  1.66146318282327 contractive_loss:  43.43073535862296\n",
      "====> Epoch: 2196 Average loss: 1.6724  reconstruction loss:  1.6680658478091954 contractive_loss:  43.00262325489969\n",
      "====> Epoch: 2197 Average loss: 1.7426  reconstruction loss:  1.738250511099642 contractive_loss:  43.287883142468026\n",
      "====> Epoch: 2198 Average loss: 1.7524  reconstruction loss:  1.7480883417470339 contractive_loss:  43.39013091181763\n",
      "====> Epoch: 2199 Average loss: 1.6592  reconstruction loss:  1.6548924793575817 contractive_loss:  42.81440281960025\n",
      "====> Epoch: 2200 Average loss: 1.5599  reconstruction loss:  1.5556844555278107 contractive_loss:  42.484595327796335\n",
      "====> Epoch: 2201 Average loss: 0.5678  reconstruction loss:  0.5633630097860675 contractive_loss:  44.08400915746075\n",
      "====> Epoch: 2202 Average loss: 0.2815  reconstruction loss:  0.27706980539897136 contractive_loss:  43.99036771854743\n",
      "====> Epoch: 2203 Average loss: 0.3734  reconstruction loss:  0.36904858381197586 contractive_loss:  43.781893572180834\n",
      "====> Epoch: 2204 Average loss: 0.8495  reconstruction loss:  0.8451234693135352 contractive_loss:  43.5812415723355\n",
      "====> Epoch: 2205 Average loss: 1.8676  reconstruction loss:  1.8633633958934304 contractive_loss:  42.83268537216335\n",
      "====> Epoch: 2206 Average loss: 1.8099  reconstruction loss:  1.8056269869456805 contractive_loss:  42.35561048647549\n",
      "====> Epoch: 2207 Average loss: 1.6330  reconstruction loss:  1.6288765372273915 contractive_loss:  41.6513668125575\n",
      "====> Epoch: 2208 Average loss: 1.7861  reconstruction loss:  1.7818973660449613 contractive_loss:  41.61839695091233\n",
      "====> Epoch: 2209 Average loss: 1.7788  reconstruction loss:  1.7746839986816416 contractive_loss:  41.65868756256463\n",
      "====> Epoch: 2210 Average loss: 1.7200  reconstruction loss:  1.7158590415102146 contractive_loss:  41.467473736225266\n",
      "====> Epoch: 2211 Average loss: 1.7245  reconstruction loss:  1.7203390685958018 contractive_loss:  41.35419083963646\n",
      "====> Epoch: 2212 Average loss: 1.8178  reconstruction loss:  1.8137092414860883 contractive_loss:  41.32668419555375\n",
      "====> Epoch: 2213 Average loss: 1.5801  reconstruction loss:  1.5760674257844114 contractive_loss:  40.60362478114465\n",
      "====> Epoch: 2214 Average loss: 0.6474  reconstruction loss:  0.6431208639653354 contractive_loss:  42.30876197062767\n",
      "====> Epoch: 2215 Average loss: 0.2472  reconstruction loss:  0.24283404415173174 contractive_loss:  43.558640749024015\n",
      "====> Epoch: 2216 Average loss: 0.2102  reconstruction loss:  0.20594119894501958 contractive_loss:  43.001591232782935\n",
      "====> Epoch: 2217 Average loss: 0.2446  reconstruction loss:  0.24038042011705577 contractive_loss:  42.588395093506165\n",
      "====> Epoch: 2218 Average loss: 1.6059  reconstruction loss:  1.6018020214156419 contractive_loss:  40.81526264755111\n",
      "====> Epoch: 2219 Average loss: 1.8911  reconstruction loss:  1.8870699568711544 contractive_loss:  40.121471983196\n",
      "====> Epoch: 2220 Average loss: 1.4438  reconstruction loss:  1.439924710957607 contractive_loss:  39.237366504290094\n",
      "====> Epoch: 2221 Average loss: 0.5405  reconstruction loss:  0.5362273501263702 contractive_loss:  42.46180322162471\n",
      "====> Epoch: 2222 Average loss: 0.8239  reconstruction loss:  0.8196125435227857 contractive_loss:  42.51013764225017\n",
      "====> Epoch: 2223 Average loss: 1.4429  reconstruction loss:  1.43826895195402 contractive_loss:  46.502786080559346\n",
      "====> Epoch: 2224 Average loss: 1.1525  reconstruction loss:  1.1480364573217015 contractive_loss:  44.63143075060556\n",
      "====> Epoch: 2225 Average loss: 0.6498  reconstruction loss:  0.6456785326309595 contractive_loss:  40.758575039756856\n",
      "====> Epoch: 2226 Average loss: 2.3310  reconstruction loss:  2.3235614726020555 contractive_loss:  74.49579915012636\n",
      "====> Epoch: 2227 Average loss: 3.6395  reconstruction loss:  3.629893386160329 contractive_loss:  95.6768445780731\n",
      "====> Epoch: 2228 Average loss: 1.2122  reconstruction loss:  1.2034771628471197 contractive_loss:  87.60817434688663\n",
      "====> Epoch: 2229 Average loss: 1.8236  reconstruction loss:  1.8162542967217399 contractive_loss:  73.09687326535317\n",
      "====> Epoch: 2230 Average loss: 1.5361  reconstruction loss:  1.5264344425942216 contractive_loss:  96.68459162197594\n",
      "====> Epoch: 2231 Average loss: 1.0021  reconstruction loss:  0.9931152747227384 contractive_loss:  89.6339849357393\n",
      "====> Epoch: 2232 Average loss: 1.7678  reconstruction loss:  1.7602490484600823 contractive_loss:  75.44591083388254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2233 Average loss: 0.5685  reconstruction loss:  0.564482982837947 contractive_loss:  40.16166712326838\n",
      "====> Epoch: 2234 Average loss: 0.8079  reconstruction loss:  0.8040271431570218 contractive_loss:  39.120764564939115\n",
      "====> Epoch: 2235 Average loss: 0.2443  reconstruction loss:  0.24028777057993259 contractive_loss:  40.424644915841945\n",
      "====> Epoch: 2236 Average loss: 1.2515  reconstruction loss:  1.2484912716897358 contractive_loss:  30.257915922741336\n",
      "====> Epoch: 2237 Average loss: 1.1516  reconstruction loss:  1.1484908183325504 contractive_loss:  31.240245290710092\n",
      "====> Epoch: 2238 Average loss: 0.9565  reconstruction loss:  0.9523926163288533 contractive_loss:  40.84473633945491\n",
      "====> Epoch: 2239 Average loss: 0.7730  reconstruction loss:  0.7693358010449529 contractive_loss:  37.13989712705664\n",
      "====> Epoch: 2240 Average loss: 1.1982  reconstruction loss:  1.1939827490515076 contractive_loss:  42.64100543987497\n",
      "====> Epoch: 2241 Average loss: 2.3309  reconstruction loss:  2.326290072454775 contractive_loss:  45.99394762773308\n",
      "====> Epoch: 2242 Average loss: 1.6352  reconstruction loss:  1.6307708408336952 contractive_loss:  44.4674650877661\n",
      "====> Epoch: 2243 Average loss: 0.7124  reconstruction loss:  0.70794976986656 contractive_loss:  44.86943423857964\n",
      "====> Epoch: 2244 Average loss: 0.3365  reconstruction loss:  0.3319607093563624 contractive_loss:  45.71216947645697\n",
      "====> Epoch: 2245 Average loss: 0.3182  reconstruction loss:  0.3136636498958438 contractive_loss:  45.61871256201279\n",
      "====> Epoch: 2246 Average loss: 0.3270  reconstruction loss:  0.32247274933740444 contractive_loss:  44.87833533990152\n",
      "====> Epoch: 2247 Average loss: 1.2523  reconstruction loss:  1.2479468222855492 contractive_loss:  43.6481333873373\n",
      "====> Epoch: 2248 Average loss: 1.5413  reconstruction loss:  1.5369326329725488 contractive_loss:  43.19071492845054\n",
      "====> Epoch: 2249 Average loss: 1.5413  reconstruction loss:  1.537052471154224 contractive_loss:  42.88234956024789\n",
      "====> Epoch: 2250 Average loss: 1.3857  reconstruction loss:  1.3814573096162488 contractive_loss:  42.375133906232065\n",
      "====> Epoch: 2251 Average loss: 1.5240  reconstruction loss:  1.5197126582199638 contractive_loss:  43.029139473872064\n",
      "====> Epoch: 2252 Average loss: 1.1934  reconstruction loss:  1.189069662732443 contractive_loss:  43.20937013343729\n",
      "====> Epoch: 2253 Average loss: 1.4169  reconstruction loss:  1.412584644380076 contractive_loss:  42.945915712424906\n",
      "====> Epoch: 2254 Average loss: 1.7817  reconstruction loss:  1.777423888580696 contractive_loss:  42.44423792203925\n",
      "====> Epoch: 2255 Average loss: 1.5821  reconstruction loss:  1.577894672648386 contractive_loss:  41.878203555741976\n",
      "====> Epoch: 2256 Average loss: 1.4683  reconstruction loss:  1.4641144566189161 contractive_loss:  41.6105658995265\n",
      "====> Epoch: 2257 Average loss: 1.6160  reconstruction loss:  1.6118361708586428 contractive_loss:  41.80992570947267\n",
      "====> Epoch: 2258 Average loss: 1.7146  reconstruction loss:  1.7103995965747323 contractive_loss:  41.963015473441665\n",
      "====> Epoch: 2259 Average loss: 1.6024  reconstruction loss:  1.598231942683766 contractive_loss:  41.54470225733485\n",
      "====> Epoch: 2260 Average loss: 1.5758  reconstruction loss:  1.5716479656763278 contractive_loss:  41.37724319130902\n",
      "====> Epoch: 2261 Average loss: 1.5199  reconstruction loss:  1.5157472294016445 contractive_loss:  41.1205518911332\n",
      "====> Epoch: 2262 Average loss: 1.7548  reconstruction loss:  1.7506474939618 contractive_loss:  41.44958153308041\n",
      "====> Epoch: 2263 Average loss: 1.6487  reconstruction loss:  1.644580047408327 contractive_loss:  41.05746964966452\n",
      "====> Epoch: 2264 Average loss: 1.6295  reconstruction loss:  1.6254582893914178 contractive_loss:  40.75053384193615\n",
      "====> Epoch: 2265 Average loss: 1.6719  reconstruction loss:  1.6678907088909367 contractive_loss:  40.355300724716535\n",
      "====> Epoch: 2266 Average loss: 1.5851  reconstruction loss:  1.581002347275738 contractive_loss:  41.194090866130615\n",
      "====> Epoch: 2267 Average loss: 0.2521  reconstruction loss:  0.24776179799991035 contractive_loss:  43.52947276492074\n",
      "====> Epoch: 2268 Average loss: 0.2465  reconstruction loss:  0.24217702824323195 contractive_loss:  42.85020749433853\n",
      "====> Epoch: 2269 Average loss: 0.2020  reconstruction loss:  0.19764761332457156 contractive_loss:  43.26719513316456\n",
      "====> Epoch: 2270 Average loss: 0.2911  reconstruction loss:  0.2867877576708244 contractive_loss:  42.64289266343713\n",
      "====> Epoch: 2271 Average loss: 0.3049  reconstruction loss:  0.30075116298638654 contractive_loss:  41.35111658153323\n",
      "====> Epoch: 2272 Average loss: 0.8662  reconstruction loss:  0.8621339906238678 contractive_loss:  40.82891578395417\n",
      "====> Epoch: 2273 Average loss: 1.5239  reconstruction loss:  1.5198288701088085 contractive_loss:  40.23804337495255\n",
      "====> Epoch: 2274 Average loss: 1.7181  reconstruction loss:  1.7141140954298384 contractive_loss:  39.53468941549935\n",
      "====> Epoch: 2275 Average loss: 1.7147  reconstruction loss:  1.710752954523227 contractive_loss:  39.17818844995961\n",
      "====> Epoch: 2276 Average loss: 1.8777  reconstruction loss:  1.8737682305447105 contractive_loss:  39.445894140752294\n",
      "====> Epoch: 2277 Average loss: 1.6778  reconstruction loss:  1.6738121998483828 contractive_loss:  39.67622237100741\n",
      "====> Epoch: 2278 Average loss: 1.6608  reconstruction loss:  1.6568976641900708 contractive_loss:  39.43330472638443\n",
      "====> Epoch: 2279 Average loss: 1.6474  reconstruction loss:  1.6435226260532894 contractive_loss:  38.81739497820096\n",
      "====> Epoch: 2280 Average loss: 1.6139  reconstruction loss:  1.6100562136948975 contractive_loss:  38.40105258907557\n",
      "====> Epoch: 2281 Average loss: 1.8312  reconstruction loss:  1.827328071017675 contractive_loss:  38.875783832481616\n",
      "====> Epoch: 2282 Average loss: 1.6485  reconstruction loss:  1.644682319213952 contractive_loss:  38.295708510668305\n",
      "====> Epoch: 2283 Average loss: 1.7363  reconstruction loss:  1.7324442651808325 contractive_loss:  38.271591340434014\n",
      "====> Epoch: 2284 Average loss: 1.6877  reconstruction loss:  1.6838565797582405 contractive_loss:  38.21872026399767\n",
      "====> Epoch: 2285 Average loss: 1.7481  reconstruction loss:  1.7442925081314653 contractive_loss:  37.872408876720705\n",
      "====> Epoch: 2286 Average loss: 1.6482  reconstruction loss:  1.6443830673162194 contractive_loss:  37.6772130601865\n",
      "====> Epoch: 2287 Average loss: 1.7112  reconstruction loss:  1.7073757026586762 contractive_loss:  37.87008330439113\n",
      "====> Epoch: 2288 Average loss: 1.8721  reconstruction loss:  1.8683852217174144 contractive_loss:  37.2873863951646\n",
      "====> Epoch: 2289 Average loss: 1.8855  reconstruction loss:  1.8817311915587254 contractive_loss:  38.08480383666199\n",
      "====> Epoch: 2290 Average loss: 2.1133  reconstruction loss:  2.1094677930570316 contractive_loss:  38.213378141413344\n",
      "====> Epoch: 2291 Average loss: 1.9307  reconstruction loss:  1.9269396848356637 contractive_loss:  37.39182289716721\n",
      "====> Epoch: 2292 Average loss: 1.7358  reconstruction loss:  1.7321040986318028 contractive_loss:  37.216932634928106\n",
      "====> Epoch: 2293 Average loss: 1.5492  reconstruction loss:  1.5454109221529728 contractive_loss:  37.819555570890486\n",
      "====> Epoch: 2294 Average loss: 1.7211  reconstruction loss:  1.7173071485426539 contractive_loss:  37.53232353348038\n",
      "====> Epoch: 2295 Average loss: 1.4044  reconstruction loss:  1.4006424936295534 contractive_loss:  37.933779225235256\n",
      "====> Epoch: 2296 Average loss: 1.8238  reconstruction loss:  1.820105138467475 contractive_loss:  37.1640381404903\n",
      "====> Epoch: 2297 Average loss: 1.7511  reconstruction loss:  1.7474246720388067 contractive_loss:  36.67918516864187\n",
      "====> Epoch: 2298 Average loss: 1.7122  reconstruction loss:  1.7084949422567777 contractive_loss:  36.702685266013276\n",
      "====> Epoch: 2299 Average loss: 1.7539  reconstruction loss:  1.750299358183686 contractive_loss:  36.29223992940091\n",
      "====> Epoch: 2300 Average loss: 1.7324  reconstruction loss:  1.7287428710649717 contractive_loss:  36.40795411545999\n",
      "====> Epoch: 2301 Average loss: 1.7149  reconstruction loss:  1.7113021033340396 contractive_loss:  36.28933086665819\n",
      "====> Epoch: 2302 Average loss: 1.6657  reconstruction loss:  1.6620816762367172 contractive_loss:  35.766139794361635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2303 Average loss: 1.7095  reconstruction loss:  1.7058530911319287 contractive_loss:  36.196044642653376\n",
      "====> Epoch: 2304 Average loss: 1.6759  reconstruction loss:  1.6722416651234298 contractive_loss:  36.184031564415086\n",
      "====> Epoch: 2305 Average loss: 1.8705  reconstruction loss:  1.8669694845573286 contractive_loss:  35.72053907950084\n",
      "====> Epoch: 2306 Average loss: 1.7092  reconstruction loss:  1.7055995714108467 contractive_loss:  35.541872271283026\n",
      "====> Epoch: 2307 Average loss: 1.7838  reconstruction loss:  1.7802304980285655 contractive_loss:  35.44974876416103\n",
      "====> Epoch: 2308 Average loss: 1.6491  reconstruction loss:  1.645472511763671 contractive_loss:  35.7930269177471\n",
      "====> Epoch: 2309 Average loss: 1.1375  reconstruction loss:  1.1338910464660754 contractive_loss:  36.30942390749371\n",
      "====> Epoch: 2310 Average loss: 1.6017  reconstruction loss:  1.5981321861977733 contractive_loss:  35.47348233086572\n",
      "====> Epoch: 2311 Average loss: 2.1938  reconstruction loss:  2.190222546460317 contractive_loss:  36.01441217761834\n",
      "====> Epoch: 2312 Average loss: 1.8310  reconstruction loss:  1.82744978157942 contractive_loss:  35.02726143464455\n",
      "====> Epoch: 2313 Average loss: 1.7784  reconstruction loss:  1.7748543157546863 contractive_loss:  35.186933904592784\n",
      "====> Epoch: 2314 Average loss: 1.7311  reconstruction loss:  1.7275844259828732 contractive_loss:  34.848608647060296\n",
      "====> Epoch: 2315 Average loss: 1.7598  reconstruction loss:  1.7563063927802693 contractive_loss:  34.78141533293221\n",
      "====> Epoch: 2316 Average loss: 1.7341  reconstruction loss:  1.7305915546970219 contractive_loss:  34.604423272422466\n",
      "====> Epoch: 2317 Average loss: 1.7422  reconstruction loss:  1.7387863707953692 contractive_loss:  34.37614624798525\n",
      "====> Epoch: 2318 Average loss: 1.4679  reconstruction loss:  1.4644145140705325 contractive_loss:  34.99681912243087\n",
      "====> Epoch: 2319 Average loss: 1.7112  reconstruction loss:  1.7077206068637005 contractive_loss:  34.57526180551088\n",
      "====> Epoch: 2320 Average loss: 1.7628  reconstruction loss:  1.7593732037998213 contractive_loss:  34.70643843297217\n",
      "====> Epoch: 2321 Average loss: 2.1645  reconstruction loss:  2.161131405086239 contractive_loss:  33.621950638715276\n",
      "====> Epoch: 2322 Average loss: 0.6183  reconstruction loss:  0.6147878362409074 contractive_loss:  35.50770528440165\n",
      "====> Epoch: 2323 Average loss: 0.6353  reconstruction loss:  0.63178503934858 contractive_loss:  35.07529955855849\n",
      "====> Epoch: 2324 Average loss: 1.4238  reconstruction loss:  1.4203795209626089 contractive_loss:  34.5483510632796\n",
      "====> Epoch: 2325 Average loss: 1.7107  reconstruction loss:  1.7072500689893737 contractive_loss:  34.34705752035261\n",
      "====> Epoch: 2326 Average loss: 1.7261  reconstruction loss:  1.7227604663234743 contractive_loss:  33.71547968458529\n",
      "====> Epoch: 2327 Average loss: 1.7469  reconstruction loss:  1.7435193490839682 contractive_loss:  33.824681786286995\n",
      "====> Epoch: 2328 Average loss: 1.7070  reconstruction loss:  1.703571447234984 contractive_loss:  33.934972717557606\n",
      "====> Epoch: 2329 Average loss: 1.7289  reconstruction loss:  1.7255252298394712 contractive_loss:  33.44002117766585\n",
      "====> Epoch: 2330 Average loss: 1.7192  reconstruction loss:  1.7158198324557887 contractive_loss:  33.669917963409304\n",
      "====> Epoch: 2331 Average loss: 1.7478  reconstruction loss:  1.7444573560744545 contractive_loss:  33.603207377209245\n",
      "====> Epoch: 2332 Average loss: 1.7703  reconstruction loss:  1.766994292067669 contractive_loss:  33.51968780758497\n",
      "====> Epoch: 2333 Average loss: 1.7476  reconstruction loss:  1.744296311905318 contractive_loss:  33.36255865229769\n",
      "====> Epoch: 2334 Average loss: 1.7743  reconstruction loss:  1.7709827970908005 contractive_loss:  33.07467717558498\n",
      "====> Epoch: 2335 Average loss: 1.7362  reconstruction loss:  1.7328410422761342 contractive_loss:  33.32709932057015\n",
      "====> Epoch: 2336 Average loss: 1.6936  reconstruction loss:  1.6902247124368133 contractive_loss:  33.46534509540514\n",
      "====> Epoch: 2337 Average loss: 1.8519  reconstruction loss:  1.8485880021668033 contractive_loss:  32.950272438392794\n",
      "====> Epoch: 2338 Average loss: 1.5987  reconstruction loss:  1.595341140266944 contractive_loss:  33.88242458999676\n",
      "====> Epoch: 2339 Average loss: 0.3720  reconstruction loss:  0.3685905431115589 contractive_loss:  34.26378709707809\n",
      "====> Epoch: 2340 Average loss: 0.3727  reconstruction loss:  0.36928098279081895 contractive_loss:  34.3446902214823\n",
      "====> Epoch: 2341 Average loss: 1.0139  reconstruction loss:  1.010568717628447 contractive_loss:  33.80711701514206\n",
      "====> Epoch: 2342 Average loss: 1.6795  reconstruction loss:  1.6761511132794105 contractive_loss:  33.203255954652285\n",
      "====> Epoch: 2343 Average loss: 1.7531  reconstruction loss:  1.7498365144503083 contractive_loss:  32.75657620345076\n",
      "====> Epoch: 2344 Average loss: 1.6714  reconstruction loss:  1.668043446712578 contractive_loss:  33.626295841267044\n",
      "====> Epoch: 2345 Average loss: 0.7819  reconstruction loss:  0.7784511479411366 contractive_loss:  34.81602077955838\n",
      "====> Epoch: 2346 Average loss: 0.2716  reconstruction loss:  0.2681082260174496 contractive_loss:  34.794359592288274\n",
      "====> Epoch: 2347 Average loss: 0.3337  reconstruction loss:  0.3302782817824109 contractive_loss:  33.98986700405749\n",
      "====> Epoch: 2348 Average loss: 1.5848  reconstruction loss:  1.5814391383683704 contractive_loss:  33.903334223192715\n",
      "====> Epoch: 2349 Average loss: 1.9772  reconstruction loss:  1.9738583382373787 contractive_loss:  32.946784906188284\n",
      "====> Epoch: 2350 Average loss: 2.1028  reconstruction loss:  2.0995420546482113 contractive_loss:  32.787083039172344\n",
      "====> Epoch: 2351 Average loss: 2.2941  reconstruction loss:  2.290820143890921 contractive_loss:  32.81253044369851\n",
      "====> Epoch: 2352 Average loss: 1.9190  reconstruction loss:  1.9156813298846083 contractive_loss:  32.850993836172634\n",
      "====> Epoch: 2353 Average loss: 1.7397  reconstruction loss:  1.7363255732938634 contractive_loss:  33.63788501815003\n",
      "====> Epoch: 2354 Average loss: 1.7263  reconstruction loss:  1.7229754855872716 contractive_loss:  32.99789144215198\n",
      "====> Epoch: 2355 Average loss: 1.6679  reconstruction loss:  1.6645428622313199 contractive_loss:  33.51389550715774\n",
      "====> Epoch: 2356 Average loss: 1.8203  reconstruction loss:  1.8170458738527677 contractive_loss:  32.413352485867975\n",
      "====> Epoch: 2357 Average loss: 1.8029  reconstruction loss:  1.7995662167430126 contractive_loss:  32.968975257935924\n",
      "====> Epoch: 2358 Average loss: 1.6267  reconstruction loss:  1.6234167026763502 contractive_loss:  33.11623835838403\n",
      "====> Epoch: 2359 Average loss: 1.6955  reconstruction loss:  1.692226731351861 contractive_loss:  32.74302505150859\n",
      "====> Epoch: 2360 Average loss: 1.7515  reconstruction loss:  1.7481808545884114 contractive_loss:  32.83366691514084\n",
      "====> Epoch: 2361 Average loss: 1.7492  reconstruction loss:  1.7460078442554785 contractive_loss:  32.30669548896904\n",
      "====> Epoch: 2362 Average loss: 2.2316  reconstruction loss:  2.2282975454091805 contractive_loss:  33.38097722122122\n",
      "====> Epoch: 2363 Average loss: 1.9709  reconstruction loss:  1.9676137874500301 contractive_loss:  32.632422066657256\n",
      "====> Epoch: 2364 Average loss: 1.7333  reconstruction loss:  1.7300092947023926 contractive_loss:  32.554925567074804\n",
      "====> Epoch: 2365 Average loss: 2.6720  reconstruction loss:  2.6686677224243884 contractive_loss:  33.740867821342704\n",
      "====> Epoch: 2366 Average loss: 1.9953  reconstruction loss:  1.992009184519535 contractive_loss:  32.45562595250537\n",
      "====> Epoch: 2367 Average loss: 0.4062  reconstruction loss:  0.402766933280151 contractive_loss:  34.29640608234171\n",
      "====> Epoch: 2368 Average loss: 1.4154  reconstruction loss:  1.4120050399000905 contractive_loss:  33.87965038655101\n",
      "====> Epoch: 2369 Average loss: 2.4940  reconstruction loss:  2.4907385367585713 contractive_loss:  32.335540657133166\n",
      "====> Epoch: 2370 Average loss: 2.2831  reconstruction loss:  2.2798527521411716 contractive_loss:  32.577873399439554\n",
      "====> Epoch: 2371 Average loss: 2.1689  reconstruction loss:  2.1655700358833014 contractive_loss:  33.041221020080044\n",
      "====> Epoch: 2372 Average loss: 1.7879  reconstruction loss:  1.7846332601094774 contractive_loss:  32.79055733448063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2373 Average loss: 1.6176  reconstruction loss:  1.6142849682750957 contractive_loss:  33.41613562816038\n",
      "====> Epoch: 2374 Average loss: 1.9733  reconstruction loss:  1.9699424109676633 contractive_loss:  33.321177237035705\n",
      "====> Epoch: 2375 Average loss: 1.5966  reconstruction loss:  1.59333045373383 contractive_loss:  32.73235184019075\n",
      "====> Epoch: 2376 Average loss: 1.5613  reconstruction loss:  1.5579647488574475 contractive_loss:  33.34641238738939\n",
      "====> Epoch: 2377 Average loss: 0.2526  reconstruction loss:  0.24903091675312183 contractive_loss:  35.67425971129121\n",
      "====> Epoch: 2378 Average loss: 0.1972  reconstruction loss:  0.19383781146378984 contractive_loss:  34.093920000464536\n",
      "====> Epoch: 2379 Average loss: 0.8114  reconstruction loss:  0.8080668944213901 contractive_loss:  33.590373663198186\n",
      "====> Epoch: 2380 Average loss: 1.6257  reconstruction loss:  1.6223684467405952 contractive_loss:  33.184425732780106\n",
      "====> Epoch: 2381 Average loss: 1.7139  reconstruction loss:  1.7105535437456394 contractive_loss:  33.059561400742105\n",
      "====> Epoch: 2382 Average loss: 1.6648  reconstruction loss:  1.6614886372587465 contractive_loss:  33.382009535030136\n",
      "====> Epoch: 2383 Average loss: 1.2543  reconstruction loss:  1.250975570786142 contractive_loss:  33.56875842052047\n",
      "====> Epoch: 2384 Average loss: 1.8063  reconstruction loss:  1.8030308434317652 contractive_loss:  33.08572480585976\n",
      "====> Epoch: 2385 Average loss: 1.7043  reconstruction loss:  1.701038475965238 contractive_loss:  32.80638149373375\n",
      "====> Epoch: 2386 Average loss: 1.5274  reconstruction loss:  1.5240459086181997 contractive_loss:  33.71549236523317\n",
      "====> Epoch: 2387 Average loss: 1.4669  reconstruction loss:  1.463503761493708 contractive_loss:  34.336475694026234\n",
      "====> Epoch: 2388 Average loss: 1.6751  reconstruction loss:  1.6717674789716264 contractive_loss:  33.66775973372927\n",
      "====> Epoch: 2389 Average loss: 1.6495  reconstruction loss:  1.646171108501281 contractive_loss:  32.87517754040425\n",
      "====> Epoch: 2390 Average loss: 1.7584  reconstruction loss:  1.7550985032762818 contractive_loss:  32.941708654435686\n",
      "====> Epoch: 2391 Average loss: 1.5806  reconstruction loss:  1.5771908868321203 contractive_loss:  33.59266080775295\n",
      "====> Epoch: 2392 Average loss: 1.6584  reconstruction loss:  1.6550771286583634 contractive_loss:  33.14915127464057\n",
      "====> Epoch: 2393 Average loss: 1.5832  reconstruction loss:  1.5798828455950646 contractive_loss:  33.20450456079184\n",
      "====> Epoch: 2394 Average loss: 1.2024  reconstruction loss:  1.1990259266540846 contractive_loss:  33.532803689294816\n",
      "====> Epoch: 2395 Average loss: 1.5834  reconstruction loss:  1.580096400599076 contractive_loss:  33.066948282007935\n",
      "====> Epoch: 2396 Average loss: 1.7040  reconstruction loss:  1.7007018215884246 contractive_loss:  32.79504922209606\n",
      "====> Epoch: 2397 Average loss: 1.7328  reconstruction loss:  1.7294440492333751 contractive_loss:  33.09664020430217\n",
      "====> Epoch: 2398 Average loss: 1.0037  reconstruction loss:  1.0002788039125676 contractive_loss:  33.868236846232065\n",
      "====> Epoch: 2399 Average loss: 1.9608  reconstruction loss:  1.9575457092161868 contractive_loss:  32.747881820063455\n",
      "====> Epoch: 2400 Average loss: 1.5855  reconstruction loss:  1.582195349131311 contractive_loss:  33.442780283059825\n",
      "====> Epoch: 2401 Average loss: 1.6955  reconstruction loss:  1.6921807859250664 contractive_loss:  33.381266418004394\n",
      "====> Epoch: 2402 Average loss: 1.6793  reconstruction loss:  1.6758976751537198 contractive_loss:  33.63774277718028\n",
      "====> Epoch: 2403 Average loss: 1.6598  reconstruction loss:  1.6564712141181384 contractive_loss:  33.2535029106861\n",
      "====> Epoch: 2404 Average loss: 1.6345  reconstruction loss:  1.6311411784779435 contractive_loss:  33.625168481368256\n",
      "====> Epoch: 2405 Average loss: 1.6692  reconstruction loss:  1.6659406646302388 contractive_loss:  33.065878868787955\n",
      "====> Epoch: 2406 Average loss: 2.3441  reconstruction loss:  2.340794855443889 contractive_loss:  32.81047502221672\n",
      "====> Epoch: 2407 Average loss: 0.2247  reconstruction loss:  0.22114426822202204 contractive_loss:  35.08598715710005\n",
      "====> Epoch: 2408 Average loss: 0.3917  reconstruction loss:  0.3880986669036878 contractive_loss:  35.70990113050256\n",
      "====> Epoch: 2409 Average loss: 0.1970  reconstruction loss:  0.19347055348725598 contractive_loss:  35.006213772695496\n",
      "====> Epoch: 2410 Average loss: 1.3225  reconstruction loss:  1.3190923778089185 contractive_loss:  33.6623478435676\n",
      "====> Epoch: 2411 Average loss: 1.4282  reconstruction loss:  1.4250251286007056 contractive_loss:  31.966127843965463\n",
      "====> Epoch: 2412 Average loss: 2.1098  reconstruction loss:  2.099945559003812 contractive_loss:  98.75613384776163\n",
      "====> Epoch: 2413 Average loss: 0.7943  reconstruction loss:  0.787319064641693 contractive_loss:  70.22801249821855\n",
      "====> Epoch: 2414 Average loss: 1.8026  reconstruction loss:  1.7940255047021672 contractive_loss:  85.30867816954583\n",
      "====> Epoch: 2415 Average loss: 1.1373  reconstruction loss:  1.1334280184940098 contractive_loss:  39.10645488056382\n",
      "====> Epoch: 2416 Average loss: 1.2769  reconstruction loss:  1.2674767988298679 contractive_loss:  93.83500084417567\n",
      "====> Epoch: 2417 Average loss: 1.0093  reconstruction loss:  1.0037379546889895 contractive_loss:  56.0151875896354\n",
      "====> Epoch: 2418 Average loss: 0.9482  reconstruction loss:  0.9391413404799885 contractive_loss:  90.85824406878258\n",
      "====> Epoch: 2419 Average loss: 1.7917  reconstruction loss:  1.7875710110675436 contractive_loss:  41.10336909541894\n",
      "====> Epoch: 2420 Average loss: 2.0828  reconstruction loss:  2.0783908644043847 contractive_loss:  43.60170777939473\n",
      "====> Epoch: 2421 Average loss: 0.6453  reconstruction loss:  0.6406052359938513 contractive_loss:  46.46518757911809\n",
      "====> Epoch: 2422 Average loss: 1.1587  reconstruction loss:  1.1549431865290545 contractive_loss:  37.645213052486106\n",
      "====> Epoch: 2423 Average loss: 1.9715  reconstruction loss:  1.9670943100055893 contractive_loss:  44.26490200247531\n",
      "====> Epoch: 2424 Average loss: 1.5072  reconstruction loss:  1.5023312273569525 contractive_loss:  48.83820865568898\n",
      "====> Epoch: 2425 Average loss: 1.5693  reconstruction loss:  1.5640269841824772 contractive_loss:  52.503933149456806\n",
      "====> Epoch: 2426 Average loss: 0.7352  reconstruction loss:  0.7302035196713996 contractive_loss:  49.57314754788236\n",
      "====> Epoch: 2427 Average loss: 0.7731  reconstruction loss:  0.7678219436356575 contractive_loss:  52.80960008570257\n",
      "====> Epoch: 2428 Average loss: 0.5150  reconstruction loss:  0.5101322265062869 contractive_loss:  48.56927219862935\n",
      "====> Epoch: 2429 Average loss: 0.5768  reconstruction loss:  0.5714320838786638 contractive_loss:  54.06575288535627\n",
      "====> Epoch: 2430 Average loss: 0.5302  reconstruction loss:  0.5251180166124392 contractive_loss:  50.68737995192594\n",
      "====> Epoch: 2431 Average loss: 0.3354  reconstruction loss:  0.3303194152385744 contractive_loss:  50.36555134294545\n",
      "====> Epoch: 2432 Average loss: 0.4130  reconstruction loss:  0.4080540181030466 contractive_loss:  49.486035660796176\n",
      "====> Epoch: 2433 Average loss: 0.4624  reconstruction loss:  0.45736774330409075 contractive_loss:  50.26048810704378\n",
      "====> Epoch: 2434 Average loss: 0.7872  reconstruction loss:  0.7821801204806753 contractive_loss:  50.343143386986824\n",
      "====> Epoch: 2435 Average loss: 0.3206  reconstruction loss:  0.31564469269455125 contractive_loss:  49.299052390946144\n",
      "====> Epoch: 2436 Average loss: 0.2621  reconstruction loss:  0.2572722814902044 contractive_loss:  47.94570195313044\n",
      "====> Epoch: 2437 Average loss: 0.8947  reconstruction loss:  0.8899114041875488 contractive_loss:  47.432080411930066\n",
      "====> Epoch: 2438 Average loss: 1.7032  reconstruction loss:  1.6985199943342693 contractive_loss:  46.72988896408035\n",
      "====> Epoch: 2439 Average loss: 1.5655  reconstruction loss:  1.560844765213277 contractive_loss:  46.498610038024836\n",
      "====> Epoch: 2440 Average loss: 0.5529  reconstruction loss:  0.5480007432318981 contractive_loss:  48.949723441669\n",
      "====> Epoch: 2441 Average loss: 1.1868  reconstruction loss:  1.1824277995343715 contractive_loss:  43.5831565994842\n",
      "====> Epoch: 2442 Average loss: 1.1538  reconstruction loss:  1.1493553408295032 contractive_loss:  44.01971220296929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2443 Average loss: 1.0376  reconstruction loss:  1.033216434414977 contractive_loss:  44.007458573616304\n",
      "====> Epoch: 2444 Average loss: 0.7134  reconstruction loss:  0.7091269825330919 contractive_loss:  42.95494404774011\n",
      "====> Epoch: 2445 Average loss: 2.5689  reconstruction loss:  2.5643858031836806 contractive_loss:  45.605943639609556\n",
      "====> Epoch: 2446 Average loss: 1.1598  reconstruction loss:  1.1513334308881884 contractive_loss:  84.75189421699162\n",
      "====> Epoch: 2447 Average loss: 1.0836  reconstruction loss:  1.079788648831192 contractive_loss:  37.65645970007544\n",
      "====> Epoch: 2448 Average loss: 0.4677  reconstruction loss:  0.46599070984654417 contractive_loss:  17.352152341030013\n",
      "====> Epoch: 2449 Average loss: 0.3651  reconstruction loss:  0.36213620512747907 contractive_loss:  29.71931427191107\n",
      "====> Epoch: 2450 Average loss: 1.1130  reconstruction loss:  1.1115324951412882 contractive_loss:  14.268189721473037\n",
      "====> Epoch: 2451 Average loss: 2.3350  reconstruction loss:  2.333446268491944 contractive_loss:  15.798926411207054\n",
      "====> Epoch: 2452 Average loss: 2.2924  reconstruction loss:  2.284928215261981 contractive_loss:  74.76813099013994\n",
      "====> Epoch: 2453 Average loss: 1.5223  reconstruction loss:  1.5117497576837866 contractive_loss:  105.77151243211324\n",
      "====> Epoch: 2454 Average loss: 1.6187  reconstruction loss:  1.610193941172088 contractive_loss:  85.22023982152336\n",
      "====> Epoch: 2455 Average loss: 0.8856  reconstruction loss:  0.8796168603786576 contractive_loss:  59.94359722635186\n",
      "====> Epoch: 2456 Average loss: 0.8630  reconstruction loss:  0.8583973003552962 contractive_loss:  46.451475516858\n",
      "====> Epoch: 2457 Average loss: 0.7374  reconstruction loss:  0.732653995077996 contractive_loss:  47.73618523854189\n",
      "====> Epoch: 2458 Average loss: 0.7705  reconstruction loss:  0.7657857924668001 contractive_loss:  47.16204719884797\n",
      "====> Epoch: 2459 Average loss: 0.7222  reconstruction loss:  0.7175001699770175 contractive_loss:  46.868809220364724\n",
      "====> Epoch: 2460 Average loss: 0.7150  reconstruction loss:  0.7102750284072805 contractive_loss:  46.785078467725484\n",
      "====> Epoch: 2461 Average loss: 1.8247  reconstruction loss:  1.8193354538130393 contractive_loss:  53.205362702928824\n",
      "====> Epoch: 2462 Average loss: 1.7898  reconstruction loss:  1.7821216407582736 contractive_loss:  76.68043730280029\n",
      "====> Epoch: 2463 Average loss: 1.0808  reconstruction loss:  1.0761535300801608 contractive_loss:  46.101652184658086\n",
      "====> Epoch: 2464 Average loss: 0.6095  reconstruction loss:  0.6048450387737095 contractive_loss:  46.645044978376056\n",
      "====> Epoch: 2465 Average loss: 0.5188  reconstruction loss:  0.5138456369384415 contractive_loss:  49.534589285582456\n",
      "====> Epoch: 2466 Average loss: 0.3719  reconstruction loss:  0.36676627489138325 contractive_loss:  51.7156336489103\n",
      "====> Epoch: 2467 Average loss: 0.2436  reconstruction loss:  0.2385224736976874 contractive_loss:  50.741696982072895\n",
      "====> Epoch: 2468 Average loss: 0.3839  reconstruction loss:  0.37888525756894736 contractive_loss:  49.852190204524945\n",
      "====> Epoch: 2469 Average loss: 1.5494  reconstruction loss:  1.5445859361303176 contractive_loss:  48.53885054024177\n",
      "====> Epoch: 2470 Average loss: 1.4350  reconstruction loss:  1.4302179160033608 contractive_loss:  48.09565907402276\n",
      "====> Epoch: 2471 Average loss: 1.4183  reconstruction loss:  1.4134828860760507 contractive_loss:  48.057872787025424\n",
      "====> Epoch: 2472 Average loss: 1.6060  reconstruction loss:  1.6012069866318428 contractive_loss:  48.09840071241793\n",
      "====> Epoch: 2473 Average loss: 2.0511  reconstruction loss:  2.0463219809315847 contractive_loss:  47.615159217545035\n",
      "====> Epoch: 2474 Average loss: 1.6840  reconstruction loss:  1.6791875175092243 contractive_loss:  47.884178190702045\n",
      "====> Epoch: 2475 Average loss: 1.5938  reconstruction loss:  1.5890138907166187 contractive_loss:  47.55670297206919\n",
      "====> Epoch: 2476 Average loss: 1.4135  reconstruction loss:  1.4087645056601104 contractive_loss:  47.05200156958004\n",
      "====> Epoch: 2477 Average loss: 1.6233  reconstruction loss:  1.6185814988112457 contractive_loss:  47.366179810244525\n",
      "====> Epoch: 2478 Average loss: 1.6166  reconstruction loss:  1.6118721340664146 contractive_loss:  47.11436477485499\n",
      "====> Epoch: 2479 Average loss: 1.4515  reconstruction loss:  1.4468685049361705 contractive_loss:  46.54119908027519\n",
      "====> Epoch: 2480 Average loss: 1.6546  reconstruction loss:  1.6499172094445682 contractive_loss:  46.7563436073492\n",
      "====> Epoch: 2481 Average loss: 1.6618  reconstruction loss:  1.657135766752259 contractive_loss:  46.450109767054954\n",
      "====> Epoch: 2482 Average loss: 1.6000  reconstruction loss:  1.5954013655132393 contractive_loss:  46.34447824188352\n",
      "====> Epoch: 2483 Average loss: 1.4246  reconstruction loss:  1.4200560412326866 contractive_loss:  45.74914935901598\n",
      "====> Epoch: 2484 Average loss: 1.7768  reconstruction loss:  1.7722219251659412 contractive_loss:  46.22552873980874\n",
      "====> Epoch: 2485 Average loss: 0.9702  reconstruction loss:  0.9654262160669415 contractive_loss:  47.308793964531986\n",
      "====> Epoch: 2486 Average loss: 1.0655  reconstruction loss:  1.0607606720957445 contractive_loss:  46.9306604970417\n",
      "====> Epoch: 2487 Average loss: 1.6167  reconstruction loss:  1.612139005884766 contractive_loss:  45.91916506018892\n",
      "====> Epoch: 2488 Average loss: 1.6811  reconstruction loss:  1.6765609613012478 contractive_loss:  45.428766225068586\n",
      "====> Epoch: 2489 Average loss: 1.5402  reconstruction loss:  1.5357113186587716 contractive_loss:  45.01799724877406\n",
      "====> Epoch: 2490 Average loss: 1.7509  reconstruction loss:  1.7463650743940875 contractive_loss:  45.222737368520384\n",
      "====> Epoch: 2491 Average loss: 1.7450  reconstruction loss:  1.7405639616988517 contractive_loss:  44.81714273239802\n",
      "====> Epoch: 2492 Average loss: 1.5141  reconstruction loss:  1.5095511036224527 contractive_loss:  45.52439812388492\n",
      "model saved!\n",
      "====> Epoch: 2493 Average loss: 0.1898  reconstruction loss:  0.18515065426978686 contractive_loss:  46.61255779886001\n",
      "====> Epoch: 2494 Average loss: 0.2779  reconstruction loss:  0.2732909659847854 contractive_loss:  46.39076691209746\n",
      "====> Epoch: 2495 Average loss: 0.4041  reconstruction loss:  0.3995043464028784 contractive_loss:  45.99310051665036\n",
      "====> Epoch: 2496 Average loss: 1.3280  reconstruction loss:  1.323524485609248 contractive_loss:  44.8474320214704\n",
      "====> Epoch: 2497 Average loss: 1.8550  reconstruction loss:  1.8506232182377156 contractive_loss:  44.182539305667866\n",
      "====> Epoch: 2498 Average loss: 1.6335  reconstruction loss:  1.629166310645252 contractive_loss:  43.58872800349641\n",
      "====> Epoch: 2499 Average loss: 1.7612  reconstruction loss:  1.7567696210539732 contractive_loss:  44.04464715345898\n",
      "====> Epoch: 2500 Average loss: 0.3586  reconstruction loss:  0.35411866437774864 contractive_loss:  45.07839187215548\n",
      "====> Epoch: 2501 Average loss: 0.5937  reconstruction loss:  0.5892368978491946 contractive_loss:  44.437473263621754\n",
      "====> Epoch: 2502 Average loss: 1.2113  reconstruction loss:  1.2068610652349339 contractive_loss:  43.89953101118452\n",
      "====> Epoch: 2503 Average loss: 1.5951  reconstruction loss:  1.5907361666604745 contractive_loss:  43.595961319315535\n",
      "====> Epoch: 2504 Average loss: 1.6792  reconstruction loss:  1.6749523839654534 contractive_loss:  42.78739219395345\n",
      "====> Epoch: 2505 Average loss: 1.7725  reconstruction loss:  1.7681925282246933 contractive_loss:  42.608783729736246\n",
      "====> Epoch: 2506 Average loss: 1.9968  reconstruction loss:  1.9924777784452372 contractive_loss:  42.90429669170195\n",
      "====> Epoch: 2507 Average loss: 0.9104  reconstruction loss:  0.9060808638669584 contractive_loss:  43.63284498825719\n",
      "====> Epoch: 2508 Average loss: 1.0446  reconstruction loss:  1.0402092794485358 contractive_loss:  43.493698930466266\n",
      "====> Epoch: 2509 Average loss: 1.7873  reconstruction loss:  1.7830766618375729 contractive_loss:  42.599603537408704\n",
      "====> Epoch: 2510 Average loss: 1.7018  reconstruction loss:  1.6975655702972414 contractive_loss:  42.16202467891139\n",
      "====> Epoch: 2511 Average loss: 1.6781  reconstruction loss:  1.6738684357953608 contractive_loss:  41.997491290074386\n",
      "====> Epoch: 2512 Average loss: 1.7132  reconstruction loss:  1.7089868956297642 contractive_loss:  42.03912622942029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2513 Average loss: 1.7025  reconstruction loss:  1.6983743658958925 contractive_loss:  41.70231255216823\n",
      "====> Epoch: 2514 Average loss: 1.7076  reconstruction loss:  1.7033977835078395 contractive_loss:  41.5475521865648\n",
      "====> Epoch: 2515 Average loss: 1.7620  reconstruction loss:  1.757853540408752 contractive_loss:  41.502819208143215\n",
      "====> Epoch: 2516 Average loss: 1.8877  reconstruction loss:  1.883624972886731 contractive_loss:  41.01220158773379\n",
      "====> Epoch: 2517 Average loss: 1.9378  reconstruction loss:  1.933693524724695 contractive_loss:  41.4301024782045\n",
      "====> Epoch: 2518 Average loss: 1.8049  reconstruction loss:  1.8008057678095923 contractive_loss:  40.99980103443\n",
      "====> Epoch: 2519 Average loss: 1.9000  reconstruction loss:  1.8958412980189345 contractive_loss:  41.41816339282266\n",
      "====> Epoch: 2520 Average loss: 1.7948  reconstruction loss:  1.7906377818892418 contractive_loss:  41.730424186134144\n",
      "====> Epoch: 2521 Average loss: 1.7979  reconstruction loss:  1.793716702918191 contractive_loss:  41.631099313783125\n",
      "====> Epoch: 2522 Average loss: 1.7426  reconstruction loss:  1.738496283884177 contractive_loss:  41.26498452734041\n",
      "====> Epoch: 2523 Average loss: 1.5756  reconstruction loss:  1.5713934726680885 contractive_loss:  41.697146209208746\n",
      "====> Epoch: 2524 Average loss: 0.4621  reconstruction loss:  0.45784105390613655 contractive_loss:  42.51589552778835\n",
      "====> Epoch: 2525 Average loss: 0.9154  reconstruction loss:  0.9112828400072311 contractive_loss:  41.604121800535104\n",
      "====> Epoch: 2526 Average loss: 1.3359  reconstruction loss:  1.331736485091039 contractive_loss:  41.45812849650431\n",
      "====> Epoch: 2527 Average loss: 1.8613  reconstruction loss:  1.857232449142069 contractive_loss:  40.67306315237628\n",
      "====> Epoch: 2528 Average loss: 1.7372  reconstruction loss:  1.7331081002239508 contractive_loss:  40.52027193021139\n",
      "====> Epoch: 2529 Average loss: 1.7003  reconstruction loss:  1.6963169748316498 contractive_loss:  40.27466584742522\n",
      "====> Epoch: 2530 Average loss: 1.7314  reconstruction loss:  1.7273921303901705 contractive_loss:  40.313489504086334\n",
      "====> Epoch: 2531 Average loss: 1.8091  reconstruction loss:  1.805071186711683 contractive_loss:  40.057149346803705\n",
      "====> Epoch: 2532 Average loss: 1.7564  reconstruction loss:  1.7524194903919794 contractive_loss:  40.24967683163444\n",
      "====> Epoch: 2533 Average loss: 1.7316  reconstruction loss:  1.7275462013628524 contractive_loss:  40.20967762197853\n",
      "====> Epoch: 2534 Average loss: 4.1447  reconstruction loss:  4.140664098866941 contractive_loss:  40.736590414279355\n",
      "====> Epoch: 2535 Average loss: 2.3658  reconstruction loss:  2.3617430954599707 contractive_loss:  40.43352389957991\n",
      "====> Epoch: 2536 Average loss: 1.7266  reconstruction loss:  1.7226067037243842 contractive_loss:  40.17447421015993\n",
      "====> Epoch: 2537 Average loss: 1.6971  reconstruction loss:  1.6930799530844225 contractive_loss:  40.185771970549844\n",
      "====> Epoch: 2538 Average loss: 1.5767  reconstruction loss:  1.5726330187856505 contractive_loss:  40.557555145322574\n",
      "====> Epoch: 2539 Average loss: 1.6921  reconstruction loss:  1.688149387693071 contractive_loss:  39.844281432645474\n",
      "====> Epoch: 2540 Average loss: 1.7109  reconstruction loss:  1.7068778042833 contractive_loss:  40.08462896684784\n",
      "====> Epoch: 2541 Average loss: 1.7641  reconstruction loss:  1.7601110996250822 contractive_loss:  39.756965526509326\n",
      "====> Epoch: 2542 Average loss: 1.6917  reconstruction loss:  1.687729521115421 contractive_loss:  39.44523648162095\n",
      "====> Epoch: 2543 Average loss: 1.7069  reconstruction loss:  1.7029351234726047 contractive_loss:  39.73723601308441\n",
      "====> Epoch: 2544 Average loss: 1.7537  reconstruction loss:  1.7497046923492505 contractive_loss:  39.47056385624671\n",
      "====> Epoch: 2545 Average loss: 2.0757  reconstruction loss:  2.0716783107848213 contractive_loss:  39.95348026313449\n",
      "====> Epoch: 2546 Average loss: 1.7911  reconstruction loss:  1.787117904514188 contractive_loss:  40.061749911214115\n",
      "====> Epoch: 2547 Average loss: 1.0299  reconstruction loss:  1.025815226932553 contractive_loss:  40.96310940039404\n",
      "====> Epoch: 2548 Average loss: 1.5827  reconstruction loss:  1.578705755265541 contractive_loss:  40.33114968281562\n",
      "====> Epoch: 2549 Average loss: 1.7619  reconstruction loss:  1.7579372493829524 contractive_loss:  39.600843735721426\n",
      "====> Epoch: 2550 Average loss: 1.7172  reconstruction loss:  1.7132719360575366 contractive_loss:  39.5764698512885\n",
      "====> Epoch: 2551 Average loss: 1.7044  reconstruction loss:  1.7004385900465777 contractive_loss:  39.66289106545429\n",
      "====> Epoch: 2552 Average loss: 1.6685  reconstruction loss:  1.6645116810931577 contractive_loss:  39.7727234171929\n",
      "====> Epoch: 2553 Average loss: 1.7029  reconstruction loss:  1.6989299218261826 contractive_loss:  39.48720987069545\n",
      "====> Epoch: 2554 Average loss: 1.7193  reconstruction loss:  1.7153304750740026 contractive_loss:  39.24438114294173\n",
      "====> Epoch: 2555 Average loss: 1.7868  reconstruction loss:  1.78290972322962 contractive_loss:  39.287639627161916\n",
      "====> Epoch: 2556 Average loss: 1.7567  reconstruction loss:  1.7527727868852452 contractive_loss:  39.212669994937\n",
      "====> Epoch: 2557 Average loss: 1.4711  reconstruction loss:  1.4671270864001216 contractive_loss:  39.83521059461193\n",
      "====> Epoch: 2558 Average loss: 1.7170  reconstruction loss:  1.7130877597787437 contractive_loss:  39.333150259159424\n",
      "====> Epoch: 2559 Average loss: 1.7011  reconstruction loss:  1.6971803227203721 contractive_loss:  39.13826493889826\n",
      "====> Epoch: 2560 Average loss: 1.7405  reconstruction loss:  1.7365460879385055 contractive_loss:  39.04152185674066\n",
      "====> Epoch: 2561 Average loss: 1.8016  reconstruction loss:  1.7976970933877499 contractive_loss:  39.3109255527111\n",
      "====> Epoch: 2562 Average loss: 1.8316  reconstruction loss:  1.8276493091313817 contractive_loss:  39.0215603064461\n",
      "====> Epoch: 2563 Average loss: 1.7184  reconstruction loss:  1.714437813262932 contractive_loss:  39.482056657953855\n",
      "====> Epoch: 2564 Average loss: 1.8386  reconstruction loss:  1.8347086451939671 contractive_loss:  38.95708333027984\n",
      "====> Epoch: 2565 Average loss: 1.7331  reconstruction loss:  1.7291527598229346 contractive_loss:  39.325685438328165\n",
      "====> Epoch: 2566 Average loss: 1.9893  reconstruction loss:  1.9852582711639275 contractive_loss:  40.04101023644284\n",
      "====> Epoch: 2567 Average loss: 1.9340  reconstruction loss:  1.9300233745588764 contractive_loss:  39.75535436793505\n",
      "====> Epoch: 2568 Average loss: 1.7813  reconstruction loss:  1.7773492092788459 contractive_loss:  39.30852274914982\n",
      "====> Epoch: 2569 Average loss: 1.8003  reconstruction loss:  1.7962770303165652 contractive_loss:  40.06486127614308\n",
      "====> Epoch: 2570 Average loss: 1.7342  reconstruction loss:  1.7302271109817595 contractive_loss:  39.39830892148327\n",
      "====> Epoch: 2571 Average loss: 1.8052  reconstruction loss:  1.8012050297076538 contractive_loss:  40.1030037170073\n",
      "====> Epoch: 2572 Average loss: 1.8083  reconstruction loss:  1.804249791076566 contractive_loss:  40.47250995484063\n",
      "====> Epoch: 2573 Average loss: 1.7344  reconstruction loss:  1.7304062371152353 contractive_loss:  39.937421044370275\n",
      "====> Epoch: 2574 Average loss: 1.7617  reconstruction loss:  1.7576697124993736 contractive_loss:  40.008037860090724\n",
      "====> Epoch: 2575 Average loss: 1.7125  reconstruction loss:  1.7084753977161764 contractive_loss:  40.195420574990344\n",
      "====> Epoch: 2576 Average loss: 1.6867  reconstruction loss:  1.6826829615192602 contractive_loss:  40.27030236330571\n",
      "====> Epoch: 2577 Average loss: 1.7483  reconstruction loss:  1.7442834105723548 contractive_loss:  39.792456123228355\n",
      "====> Epoch: 2578 Average loss: 1.8272  reconstruction loss:  1.823255756959215 contractive_loss:  39.75380663137116\n",
      "====> Epoch: 2579 Average loss: 1.6940  reconstruction loss:  1.689970039669025 contractive_loss:  40.102726002289444\n",
      "====> Epoch: 2580 Average loss: 1.6945  reconstruction loss:  1.690523627270569 contractive_loss:  39.96644429608247\n",
      "====> Epoch: 2581 Average loss: 1.7722  reconstruction loss:  1.768261625831279 contractive_loss:  39.74891638493068\n",
      "====> Epoch: 2582 Average loss: 1.7309  reconstruction loss:  1.7269151023590958 contractive_loss:  39.88088995583031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2583 Average loss: 1.7293  reconstruction loss:  1.7252754811664055 contractive_loss:  39.98770674797238\n",
      "====> Epoch: 2584 Average loss: 1.4616  reconstruction loss:  1.4575165374556998 contractive_loss:  40.4469501168483\n",
      "====> Epoch: 2585 Average loss: 1.7126  reconstruction loss:  1.70856093722392 contractive_loss:  40.06190383742215\n",
      "====> Epoch: 2586 Average loss: 1.8075  reconstruction loss:  1.8035255232020873 contractive_loss:  39.96408763937095\n",
      "====> Epoch: 2587 Average loss: 1.7674  reconstruction loss:  1.76350119047625 contractive_loss:  39.43100488882861\n",
      "====> Epoch: 2588 Average loss: 1.7824  reconstruction loss:  1.7783874870268657 contractive_loss:  40.10379851336948\n",
      "====> Epoch: 2589 Average loss: 1.7497  reconstruction loss:  1.7457602656422468 contractive_loss:  39.80002963686971\n",
      "====> Epoch: 2590 Average loss: 1.6130  reconstruction loss:  1.6089284180040448 contractive_loss:  40.38548399670762\n",
      "====> Epoch: 2591 Average loss: 1.6337  reconstruction loss:  1.6296732042705329 contractive_loss:  40.22448920254203\n",
      "====> Epoch: 2592 Average loss: 1.7722  reconstruction loss:  1.768221750983758 contractive_loss:  40.01899592966489\n",
      "====> Epoch: 2593 Average loss: 1.9841  reconstruction loss:  1.9801866369012933 contractive_loss:  39.495078906841954\n",
      "====> Epoch: 2594 Average loss: 1.8072  reconstruction loss:  1.8031470761467037 contractive_loss:  40.32395385498619\n",
      "====> Epoch: 2595 Average loss: 1.8380  reconstruction loss:  1.8339896453921916 contractive_loss:  40.27838467996913\n",
      "====> Epoch: 2596 Average loss: 1.7851  reconstruction loss:  1.7810650995256707 contractive_loss:  40.136817053295175\n",
      "====> Epoch: 2597 Average loss: 1.8484  reconstruction loss:  1.8444789992323698 contractive_loss:  39.607545807754825\n",
      "====> Epoch: 2598 Average loss: 1.4147  reconstruction loss:  1.4105180502365464 contractive_loss:  41.36802672843713\n",
      "====> Epoch: 2599 Average loss: 1.7775  reconstruction loss:  1.7734481876127375 contractive_loss:  40.703398501721615\n",
      "====> Epoch: 2600 Average loss: 1.8743  reconstruction loss:  1.8701787908721577 contractive_loss:  41.55662048781002\n",
      "====> Epoch: 2601 Average loss: 1.9314  reconstruction loss:  1.9273100233585527 contractive_loss:  40.954393918301804\n",
      "====> Epoch: 2602 Average loss: 1.8323  reconstruction loss:  1.8282742014326652 contractive_loss:  40.481981047633504\n",
      "====> Epoch: 2603 Average loss: 1.9091  reconstruction loss:  1.904990387478111 contractive_loss:  41.03744198393982\n",
      "====> Epoch: 2604 Average loss: 1.8193  reconstruction loss:  1.8151812841829393 contractive_loss:  41.2675328262786\n",
      "====> Epoch: 2605 Average loss: 1.7139  reconstruction loss:  1.7098136814650664 contractive_loss:  40.803743127714995\n",
      "====> Epoch: 2606 Average loss: 1.7422  reconstruction loss:  1.738145311818566 contractive_loss:  40.90903143847458\n",
      "====> Epoch: 2607 Average loss: 1.7590  reconstruction loss:  1.7549092427492763 contractive_loss:  40.76156601227081\n",
      "====> Epoch: 2608 Average loss: 1.6934  reconstruction loss:  1.6893425342649897 contractive_loss:  40.605164822565705\n",
      "====> Epoch: 2609 Average loss: 1.7566  reconstruction loss:  1.7524800964198903 contractive_loss:  40.71829050999437\n",
      "====> Epoch: 2610 Average loss: 1.7026  reconstruction loss:  1.6985005680714653 contractive_loss:  40.56550345721398\n",
      "====> Epoch: 2611 Average loss: 1.8461  reconstruction loss:  1.8420579079003239 contractive_loss:  40.369589540510674\n",
      "====> Epoch: 2612 Average loss: 1.7832  reconstruction loss:  1.7791235096425644 contractive_loss:  40.877239709196566\n",
      "====> Epoch: 2613 Average loss: 1.7297  reconstruction loss:  1.7256756088504905 contractive_loss:  40.693443320235176\n",
      "====> Epoch: 2614 Average loss: 1.7708  reconstruction loss:  1.7666885634012832 contractive_loss:  40.94280169695878\n",
      "====> Epoch: 2615 Average loss: 1.7249  reconstruction loss:  1.7207783097877833 contractive_loss:  40.91548813613487\n",
      "====> Epoch: 2616 Average loss: 1.6838  reconstruction loss:  1.6796890613712168 contractive_loss:  40.765880687302165\n",
      "====> Epoch: 2617 Average loss: 1.7962  reconstruction loss:  1.7921320823604419 contractive_loss:  40.43744066445345\n",
      "====> Epoch: 2618 Average loss: 1.7066  reconstruction loss:  1.7025946505567067 contractive_loss:  40.48263327794962\n",
      "====> Epoch: 2619 Average loss: 1.1759  reconstruction loss:  1.171769376052842 contractive_loss:  41.522294206055655\n",
      "====> Epoch: 2620 Average loss: 0.4213  reconstruction loss:  0.41710392635987903 contractive_loss:  41.74593708710607\n",
      "====> Epoch: 2621 Average loss: 0.9527  reconstruction loss:  0.9485526876251039 contractive_loss:  41.12206177662834\n",
      "====> Epoch: 2622 Average loss: 1.4545  reconstruction loss:  1.4503737974811812 contractive_loss:  40.977077113277275\n",
      "====> Epoch: 2623 Average loss: 1.7314  reconstruction loss:  1.7274088356053836 contractive_loss:  40.28738543970981\n",
      "====> Epoch: 2624 Average loss: 1.5545  reconstruction loss:  1.5503697861879788 contractive_loss:  41.18380221997645\n",
      "====> Epoch: 2625 Average loss: 1.4782  reconstruction loss:  1.4741229346825175 contractive_loss:  41.056532405005534\n",
      "====> Epoch: 2626 Average loss: 1.7034  reconstruction loss:  1.6993055924289475 contractive_loss:  40.76953436876466\n",
      "====> Epoch: 2627 Average loss: 1.7364  reconstruction loss:  1.7323647335432537 contractive_loss:  40.83344091053305\n",
      "====> Epoch: 2628 Average loss: 1.8477  reconstruction loss:  1.8435668478517855 contractive_loss:  41.08337760597959\n",
      "====> Epoch: 2629 Average loss: 1.7623  reconstruction loss:  1.7581870891455984 contractive_loss:  40.95965870344196\n",
      "====> Epoch: 2630 Average loss: 2.1442  reconstruction loss:  2.1400508031935384 contractive_loss:  41.52698534455718\n",
      "====> Epoch: 2631 Average loss: 1.1381  reconstruction loss:  1.1338557654622914 contractive_loss:  42.91012651170331\n",
      "====> Epoch: 2632 Average loss: 1.6959  reconstruction loss:  1.691726673823203 contractive_loss:  41.908688199163706\n",
      "====> Epoch: 2633 Average loss: 1.9767  reconstruction loss:  1.972606321068171 contractive_loss:  40.75235788935747\n",
      "====> Epoch: 2634 Average loss: 1.9435  reconstruction loss:  1.9393867835808158 contractive_loss:  41.22696045913457\n",
      "====> Epoch: 2635 Average loss: 2.1073  reconstruction loss:  2.1032322029676886 contractive_loss:  41.063554244624704\n",
      "====> Epoch: 2636 Average loss: 1.9107  reconstruction loss:  1.9065533888287272 contractive_loss:  41.933468883677946\n",
      "====> Epoch: 2637 Average loss: 0.8306  reconstruction loss:  0.8263278556825281 contractive_loss:  42.7919522356038\n",
      "====> Epoch: 2638 Average loss: 1.4189  reconstruction loss:  1.4147696324106023 contractive_loss:  41.74566758842764\n",
      "====> Epoch: 2639 Average loss: 1.6619  reconstruction loss:  1.6577589773800105 contractive_loss:  41.01438748342775\n",
      "====> Epoch: 2640 Average loss: 1.7021  reconstruction loss:  1.6979111904784083 contractive_loss:  41.469421676174164\n",
      "====> Epoch: 2641 Average loss: 1.6257  reconstruction loss:  1.6216575938797357 contractive_loss:  40.73948068567606\n",
      "====> Epoch: 2642 Average loss: 1.6109  reconstruction loss:  1.6067150044435567 contractive_loss:  41.696633895015495\n",
      "====> Epoch: 2643 Average loss: 1.5705  reconstruction loss:  1.5664473857951222 contractive_loss:  40.92400770254119\n",
      "====> Epoch: 2644 Average loss: 0.3441  reconstruction loss:  0.3395867528216015 contractive_loss:  45.3416909551955\n",
      "====> Epoch: 2645 Average loss: 1.6506  reconstruction loss:  1.639839035791629 contractive_loss:  107.42895753264882\n",
      "====> Epoch: 2646 Average loss: 1.7014  reconstruction loss:  1.6976807737405881 contractive_loss:  37.207318728323614\n",
      "====> Epoch: 2647 Average loss: 0.5239  reconstruction loss:  0.5214854665267976 contractive_loss:  24.44671505501678\n",
      "====> Epoch: 2648 Average loss: 0.8850  reconstruction loss:  0.8801873888628677 contractive_loss:  48.2997341005827\n",
      "====> Epoch: 2649 Average loss: 0.7487  reconstruction loss:  0.7380971653010571 contractive_loss:  106.29394495274866\n",
      "====> Epoch: 2650 Average loss: 0.6516  reconstruction loss:  0.6487769528491967 contractive_loss:  28.100797517698883\n",
      "====> Epoch: 2651 Average loss: 0.4946  reconstruction loss:  0.4915450064486476 contractive_loss:  30.17101358737822\n",
      "====> Epoch: 2652 Average loss: 0.4442  reconstruction loss:  0.4364151863996409 contractive_loss:  77.61918133759656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2653 Average loss: 1.0462  reconstruction loss:  1.0372648772260316 contractive_loss:  89.11598177765258\n",
      "====> Epoch: 2654 Average loss: 1.1388  reconstruction loss:  1.1342119061876452 contractive_loss:  46.15412641232769\n",
      "====> Epoch: 2655 Average loss: 0.4203  reconstruction loss:  0.4151141761694781 contractive_loss:  51.57778845919292\n",
      "====> Epoch: 2656 Average loss: 0.1967  reconstruction loss:  0.19161164087828944 contractive_loss:  50.59326237741206\n",
      "====> Epoch: 2657 Average loss: 1.5854  reconstruction loss:  1.5805986761184234 contractive_loss:  48.31747763357629\n",
      "====> Epoch: 2658 Average loss: 0.2197  reconstruction loss:  0.21471975580348585 contractive_loss:  50.05686307969683\n",
      "model saved!\n",
      "====> Epoch: 2659 Average loss: 0.1795  reconstruction loss:  0.1745101789910852 contractive_loss:  50.27968640746264\n",
      "model saved!\n",
      "====> Epoch: 2660 Average loss: 0.1615  reconstruction loss:  0.1565902078009885 contractive_loss:  49.215664914517845\n",
      "====> Epoch: 2661 Average loss: 0.2530  reconstruction loss:  0.24819886851561357 contractive_loss:  48.243366617513686\n",
      "====> Epoch: 2662 Average loss: 1.4034  reconstruction loss:  1.3987612046002362 contractive_loss:  46.75186623130361\n",
      "====> Epoch: 2663 Average loss: 1.3404  reconstruction loss:  1.3358270825529608 contractive_loss:  45.59525984284125\n",
      "====> Epoch: 2664 Average loss: 2.3302  reconstruction loss:  2.3256784650215723 contractive_loss:  45.52983833352298\n",
      "====> Epoch: 2665 Average loss: 1.6213  reconstruction loss:  1.616712007679169 contractive_loss:  45.76137673803067\n",
      "====> Epoch: 2666 Average loss: 1.6389  reconstruction loss:  1.6342901164429773 contractive_loss:  46.020600680899314\n",
      "====> Epoch: 2667 Average loss: 1.5299  reconstruction loss:  1.525378286433163 contractive_loss:  45.2642790825293\n",
      "====> Epoch: 2668 Average loss: 0.2011  reconstruction loss:  0.19631753839385127 contractive_loss:  47.77426214859419\n",
      "====> Epoch: 2669 Average loss: 0.1761  reconstruction loss:  0.17136862918040952 contractive_loss:  47.115272843214434\n",
      "====> Epoch: 2670 Average loss: 1.5900  reconstruction loss:  1.5855951585476473 contractive_loss:  44.338948045949564\n",
      "====> Epoch: 2671 Average loss: 1.1336  reconstruction loss:  1.1292751263593241 contractive_loss:  43.663424748140145\n",
      "====> Epoch: 2672 Average loss: 7.6894  reconstruction loss:  7.684547083214027 contractive_loss:  49.00832561667144\n",
      "====> Epoch: 2673 Average loss: 3.4542  reconstruction loss:  3.451702153275812 contractive_loss:  24.572298600442156\n",
      "====> Epoch: 2674 Average loss: 2.5742  reconstruction loss:  2.573179325807681 contractive_loss:  9.847285434875925\n",
      "====> Epoch: 2675 Average loss: 1.5573  reconstruction loss:  1.555135180415137 contractive_loss:  21.347823921741234\n",
      "====> Epoch: 2676 Average loss: 1.6884  reconstruction loss:  1.6836097477395457 contractive_loss:  47.720099242860904\n",
      "====> Epoch: 2677 Average loss: 1.3710  reconstruction loss:  1.3654859977459681 contractive_loss:  55.495408660712954\n",
      "====> Epoch: 2678 Average loss: 1.5311  reconstruction loss:  1.5257489786692695 contractive_loss:  53.30072400518669\n",
      "====> Epoch: 2679 Average loss: 0.6856  reconstruction loss:  0.6805568749002875 contractive_loss:  50.04903788732995\n",
      "====> Epoch: 2680 Average loss: 1.0433  reconstruction loss:  1.0383077671987837 contractive_loss:  49.99549881947976\n",
      "====> Epoch: 2681 Average loss: 0.8960  reconstruction loss:  0.8931334272300019 contractive_loss:  28.21465942225234\n",
      "====> Epoch: 2682 Average loss: 0.9839  reconstruction loss:  0.9826022217831989 contractive_loss:  13.440850250829016\n",
      "====> Epoch: 2683 Average loss: 1.8704  reconstruction loss:  1.8687215164027446 contractive_loss:  17.213618672960898\n",
      "====> Epoch: 2684 Average loss: 1.7929  reconstruction loss:  1.7908063141645372 contractive_loss:  20.890217369263556\n",
      "====> Epoch: 2685 Average loss: 1.1927  reconstruction loss:  1.1910031669244818 contractive_loss:  16.95274027161695\n",
      "====> Epoch: 2686 Average loss: 1.1005  reconstruction loss:  1.098848036942033 contractive_loss:  16.77620877838855\n",
      "====> Epoch: 2687 Average loss: 0.8778  reconstruction loss:  0.8763351998201075 contractive_loss:  14.674797178298341\n",
      "====> Epoch: 2688 Average loss: 0.7672  reconstruction loss:  0.7658055428858118 contractive_loss:  13.622994100382703\n",
      "====> Epoch: 2689 Average loss: 0.8822  reconstruction loss:  0.8807443943782315 contractive_loss:  14.753831234082957\n",
      "====> Epoch: 2690 Average loss: 0.7283  reconstruction loss:  0.7269136525280098 contractive_loss:  14.00871172420178\n",
      "====> Epoch: 2691 Average loss: 0.7522  reconstruction loss:  0.7507815695003239 contractive_loss:  14.044807303077175\n",
      "====> Epoch: 2692 Average loss: 0.9390  reconstruction loss:  0.9376901007240521 contractive_loss:  13.521781458912262\n",
      "====> Epoch: 2693 Average loss: 0.7315  reconstruction loss:  0.7300680137184863 contractive_loss:  14.24892951157079\n",
      "====> Epoch: 2694 Average loss: 0.7805  reconstruction loss:  0.7790639986610214 contractive_loss:  13.882423026241897\n",
      "====> Epoch: 2695 Average loss: 0.6943  reconstruction loss:  0.6929220787101699 contractive_loss:  13.804151143123375\n",
      "====> Epoch: 2696 Average loss: 1.3978  reconstruction loss:  1.3959612382886055 contractive_loss:  18.06321349255957\n",
      "====> Epoch: 2697 Average loss: 1.6378  reconstruction loss:  1.6353315564857847 contractive_loss:  25.079993969678473\n",
      "====> Epoch: 2698 Average loss: 1.5717  reconstruction loss:  1.5697919492351502 contractive_loss:  19.551968674192633\n",
      "====> Epoch: 2699 Average loss: 0.8882  reconstruction loss:  0.8865743991032842 contractive_loss:  16.739889730470473\n",
      "====> Epoch: 2700 Average loss: 0.6808  reconstruction loss:  0.6793732602212964 contractive_loss:  14.318935933056848\n",
      "====> Epoch: 2701 Average loss: 1.0626  reconstruction loss:  1.0609673306058376 contractive_loss:  16.61164579584646\n",
      "====> Epoch: 2702 Average loss: 1.1289  reconstruction loss:  1.1270300791570234 contractive_loss:  18.928194035006456\n",
      "====> Epoch: 2703 Average loss: 0.8462  reconstruction loss:  0.8446347825296471 contractive_loss:  15.936416809871524\n",
      "====> Epoch: 2704 Average loss: 0.7794  reconstruction loss:  0.7778976613470983 contractive_loss:  15.416150441556178\n",
      "====> Epoch: 2705 Average loss: 0.8098  reconstruction loss:  0.808462548391113 contractive_loss:  13.698573936249849\n",
      "====> Epoch: 2706 Average loss: 1.6870  reconstruction loss:  1.6848111363110565 contractive_loss:  21.496628275991547\n",
      "====> Epoch: 2707 Average loss: 1.8003  reconstruction loss:  1.797882247340913 contractive_loss:  24.383027565956766\n",
      "====> Epoch: 2708 Average loss: 1.2587  reconstruction loss:  1.2565602845730595 contractive_loss:  21.364273220921444\n",
      "====> Epoch: 2709 Average loss: 1.0315  reconstruction loss:  1.0296640377882054 contractive_loss:  18.789583602108916\n",
      "====> Epoch: 2710 Average loss: 0.6656  reconstruction loss:  0.6640755593749579 contractive_loss:  15.486720876928436\n",
      "====> Epoch: 2711 Average loss: 0.5970  reconstruction loss:  0.5955411939163626 contractive_loss:  14.52533701730598\n",
      "====> Epoch: 2712 Average loss: 0.7841  reconstruction loss:  0.7824403166285311 contractive_loss:  16.921854486303353\n",
      "====> Epoch: 2713 Average loss: 0.7960  reconstruction loss:  0.7943680157795109 contractive_loss:  16.360024888257406\n",
      "====> Epoch: 2714 Average loss: 0.7325  reconstruction loss:  0.7309527471808585 contractive_loss:  15.30520964599406\n",
      "====> Epoch: 2715 Average loss: 0.6383  reconstruction loss:  0.636853127851838 contractive_loss:  14.635130935705199\n",
      "====> Epoch: 2716 Average loss: 0.8579  reconstruction loss:  0.8563629892964392 contractive_loss:  15.255648119531942\n",
      "====> Epoch: 2717 Average loss: 0.7601  reconstruction loss:  0.7585362394317846 contractive_loss:  15.575534110687\n",
      "====> Epoch: 2718 Average loss: 0.9726  reconstruction loss:  0.9709125265145638 contractive_loss:  16.6255442007007\n",
      "====> Epoch: 2719 Average loss: 0.7183  reconstruction loss:  0.7167889692672972 contractive_loss:  15.374210235478971\n",
      "====> Epoch: 2720 Average loss: 0.6273  reconstruction loss:  0.6257916213485829 contractive_loss:  15.566256295612526\n",
      "====> Epoch: 2721 Average loss: 0.7482  reconstruction loss:  0.7466126422239893 contractive_loss:  15.979372217038883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2722 Average loss: 0.7997  reconstruction loss:  0.7981093732602739 contractive_loss:  15.75297863446548\n",
      "====> Epoch: 2723 Average loss: 0.9277  reconstruction loss:  0.9262269746203752 contractive_loss:  14.631331801914829\n",
      "====> Epoch: 2724 Average loss: 0.7194  reconstruction loss:  0.7178461118362381 contractive_loss:  15.416546783569217\n",
      "====> Epoch: 2725 Average loss: 0.6869  reconstruction loss:  0.6853356951134741 contractive_loss:  15.145709122606863\n",
      "====> Epoch: 2726 Average loss: 1.7317  reconstruction loss:  1.7287954288424845 contractive_loss:  28.83036554546189\n",
      "====> Epoch: 2727 Average loss: 0.4526  reconstruction loss:  0.44839387394459906 contractive_loss:  41.57954583279717\n",
      "====> Epoch: 2728 Average loss: 0.7843  reconstruction loss:  0.7791481654924636 contractive_loss:  51.130616525760935\n",
      "====> Epoch: 2729 Average loss: 0.5429  reconstruction loss:  0.5371031203577264 contractive_loss:  57.672060186258854\n",
      "====> Epoch: 2730 Average loss: 0.5885  reconstruction loss:  0.5831354364599836 contractive_loss:  53.90416894721648\n",
      "====> Epoch: 2731 Average loss: 0.8478  reconstruction loss:  0.8417797763881247 contractive_loss:  60.14772144862546\n",
      "====> Epoch: 2732 Average loss: 1.0096  reconstruction loss:  1.0036793967496427 contractive_loss:  59.17658480297702\n",
      "====> Epoch: 2733 Average loss: 0.8293  reconstruction loss:  0.8237257168640969 contractive_loss:  56.18759823604016\n",
      "====> Epoch: 2734 Average loss: 0.4132  reconstruction loss:  0.40808684134275636 contractive_loss:  50.65010405943688\n",
      "====> Epoch: 2735 Average loss: 0.5593  reconstruction loss:  0.5526608541720124 contractive_loss:  66.3854576370308\n",
      "====> Epoch: 2736 Average loss: 0.5197  reconstruction loss:  0.5134872553055139 contractive_loss:  61.86857827691026\n",
      "====> Epoch: 2737 Average loss: 0.7939  reconstruction loss:  0.7873220816671589 contractive_loss:  65.41320181290463\n",
      "====> Epoch: 2738 Average loss: 0.7619  reconstruction loss:  0.7558568812771033 contractive_loss:  59.998864334192405\n",
      "====> Epoch: 2739 Average loss: 0.6470  reconstruction loss:  0.6417180858413772 contractive_loss:  52.427242907760494\n",
      "====> Epoch: 2740 Average loss: 0.5355  reconstruction loss:  0.529746278940503 contractive_loss:  57.07703336230511\n",
      "====> Epoch: 2741 Average loss: 0.3922  reconstruction loss:  0.38629492863092635 contractive_loss:  58.556162196698395\n",
      "====> Epoch: 2742 Average loss: 0.4154  reconstruction loss:  0.4095148440265741 contractive_loss:  58.544082767386755\n",
      "====> Epoch: 2743 Average loss: 0.3708  reconstruction loss:  0.36497045100538483 contractive_loss:  58.12061708746119\n",
      "====> Epoch: 2744 Average loss: 0.7959  reconstruction loss:  0.7893527828528631 contractive_loss:  65.11969299814737\n",
      "====> Epoch: 2745 Average loss: 0.9790  reconstruction loss:  0.9727697133523182 contractive_loss:  62.63997577232048\n",
      "====> Epoch: 2746 Average loss: 1.6230  reconstruction loss:  1.619978105629924 contractive_loss:  30.10269326421736\n",
      "====> Epoch: 2747 Average loss: 0.6511  reconstruction loss:  0.649306997107259 contractive_loss:  17.755102264155855\n",
      "====> Epoch: 2748 Average loss: 0.3791  reconstruction loss:  0.3778740433359091 contractive_loss:  11.921483337136012\n",
      "====> Epoch: 2749 Average loss: 0.6395  reconstruction loss:  0.635445223559326 contractive_loss:  40.23814841118855\n",
      "====> Epoch: 2750 Average loss: 0.5086  reconstruction loss:  0.5032454731472701 contractive_loss:  53.560861008442714\n",
      "====> Epoch: 2751 Average loss: 0.5361  reconstruction loss:  0.5287182022861655 contractive_loss:  74.21477080908157\n",
      "====> Epoch: 2752 Average loss: 1.8670  reconstruction loss:  1.859682907556366 contractive_loss:  73.26084650092479\n",
      "====> Epoch: 2753 Average loss: 1.4540  reconstruction loss:  1.4466375247927596 contractive_loss:  73.57506797231356\n",
      "====> Epoch: 2754 Average loss: 0.5306  reconstruction loss:  0.5230741853360457 contractive_loss:  74.80963345067333\n",
      "====> Epoch: 2755 Average loss: 1.5776  reconstruction loss:  1.5702144497179538 contractive_loss:  73.71489601164001\n",
      "====> Epoch: 2756 Average loss: 1.2373  reconstruction loss:  1.2299107891207193 contractive_loss:  73.44489129454729\n",
      "====> Epoch: 2757 Average loss: 0.8475  reconstruction loss:  0.8401529466194287 contractive_loss:  73.5799552093608\n",
      "====> Epoch: 2758 Average loss: 0.6657  reconstruction loss:  0.6582383322224683 contractive_loss:  74.12043058236235\n",
      "====> Epoch: 2759 Average loss: 0.9776  reconstruction loss:  0.9703509027309429 contractive_loss:  72.90652105940708\n",
      "====> Epoch: 2760 Average loss: 0.9579  reconstruction loss:  0.9503384622162421 contractive_loss:  75.29962196951055\n",
      "====> Epoch: 2761 Average loss: 0.4692  reconstruction loss:  0.46163578839079716 contractive_loss:  75.54863975099124\n",
      "====> Epoch: 2762 Average loss: 0.4864  reconstruction loss:  0.4790044344975976 contractive_loss:  74.26694320357825\n",
      "====> Epoch: 2763 Average loss: 1.5511  reconstruction loss:  1.543764125791048 contractive_loss:  73.1253232995734\n",
      "====> Epoch: 2764 Average loss: 0.6532  reconstruction loss:  0.6458872091434047 contractive_loss:  72.9040004654221\n",
      "====> Epoch: 2765 Average loss: 0.4571  reconstruction loss:  0.4506205242163736 contractive_loss:  64.58525709729\n",
      "====> Epoch: 2766 Average loss: 0.3532  reconstruction loss:  0.347556417722614 contractive_loss:  56.249522938491154\n",
      "====> Epoch: 2767 Average loss: 1.4343  reconstruction loss:  1.4301574611227017 contractive_loss:  41.138116490308605\n",
      "====> Epoch: 2768 Average loss: 0.6732  reconstruction loss:  0.6702816508893674 contractive_loss:  28.738805530230692\n",
      "====> Epoch: 2769 Average loss: 0.4703  reconstruction loss:  0.46806214997586226 contractive_loss:  22.339398857260292\n",
      "====> Epoch: 2770 Average loss: 0.5095  reconstruction loss:  0.5075020600718257 contractive_loss:  19.637075580316164\n",
      "====> Epoch: 2771 Average loss: 3.7425  reconstruction loss:  3.7388823740765273 contractive_loss:  35.933685013921874\n",
      "====> Epoch: 2772 Average loss: 1.7297  reconstruction loss:  1.7221156272259228 contractive_loss:  76.12455759483629\n",
      "====> Epoch: 2773 Average loss: 3.3825  reconstruction loss:  3.3750909758135315 contractive_loss:  74.55683021177008\n",
      "====> Epoch: 2774 Average loss: 2.2695  reconstruction loss:  2.2622321171740576 contractive_loss:  72.18195685170878\n",
      "====> Epoch: 2775 Average loss: 3.5814  reconstruction loss:  3.57633347660303 contractive_loss:  50.29692909648966\n",
      "====> Epoch: 2776 Average loss: 1.5215  reconstruction loss:  1.516155235863076 contractive_loss:  53.22130268913042\n",
      "====> Epoch: 2777 Average loss: 1.7377  reconstruction loss:  1.7327073116156884 contractive_loss:  50.34545313657521\n",
      "====> Epoch: 2778 Average loss: 2.0326  reconstruction loss:  2.0276001143642675 contractive_loss:  50.12854587667828\n",
      "====> Epoch: 2779 Average loss: 0.9863  reconstruction loss:  0.9811374384002896 contractive_loss:  51.85167962176269\n",
      "====> Epoch: 2780 Average loss: 0.6649  reconstruction loss:  0.6595333773450713 contractive_loss:  53.25851279284907\n",
      "====> Epoch: 2781 Average loss: 1.2623  reconstruction loss:  1.2569007424372958 contractive_loss:  53.49295827930622\n",
      "====> Epoch: 2782 Average loss: 1.5551  reconstruction loss:  1.5500925411734476 contractive_loss:  50.27946472427249\n",
      "====> Epoch: 2783 Average loss: 1.2523  reconstruction loss:  1.2472949845800394 contractive_loss:  49.875819701297566\n",
      "====> Epoch: 2784 Average loss: 1.1208  reconstruction loss:  1.1158198772895545 contractive_loss:  49.96350915854281\n",
      "====> Epoch: 2785 Average loss: 0.7822  reconstruction loss:  0.7771680351186498 contractive_loss:  50.5605365302677\n",
      "====> Epoch: 2786 Average loss: 0.7578  reconstruction loss:  0.7526226064911878 contractive_loss:  51.67564564770696\n",
      "====> Epoch: 2787 Average loss: 0.6309  reconstruction loss:  0.625608231863555 contractive_loss:  53.290101739135494\n",
      "====> Epoch: 2788 Average loss: 0.6296  reconstruction loss:  0.6241175197110002 contractive_loss:  54.93975575964555\n",
      "====> Epoch: 2789 Average loss: 1.7159  reconstruction loss:  1.7108741749158625 contractive_loss:  50.12408150521205\n",
      "====> Epoch: 2790 Average loss: 1.6894  reconstruction loss:  1.6843427662068342 contractive_loss:  50.18608881330646\n",
      "====> Epoch: 2791 Average loss: 1.3990  reconstruction loss:  1.394048800957169 contractive_loss:  49.915906467960596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2792 Average loss: 1.1744  reconstruction loss:  1.1693967839046568 contractive_loss:  49.69986579302179\n",
      "====> Epoch: 2793 Average loss: 0.7126  reconstruction loss:  0.7075374686860526 contractive_loss:  50.22922189097995\n",
      "====> Epoch: 2794 Average loss: 0.5855  reconstruction loss:  0.5803856130913256 contractive_loss:  51.37744291619492\n",
      "====> Epoch: 2795 Average loss: 0.5484  reconstruction loss:  0.5430856463613426 contractive_loss:  53.30838034412082\n",
      "====> Epoch: 2796 Average loss: 0.5344  reconstruction loss:  0.5289941180560099 contractive_loss:  54.4703099428936\n",
      "====> Epoch: 2797 Average loss: 1.9361  reconstruction loss:  1.9311339802752951 contractive_loss:  49.699046660857526\n",
      "====> Epoch: 2798 Average loss: 1.4087  reconstruction loss:  1.4017896425066398 contractive_loss:  69.47924252197812\n",
      "====> Epoch: 2799 Average loss: 1.8072  reconstruction loss:  1.8005746715867774 contractive_loss:  66.6603948145572\n",
      "====> Epoch: 2800 Average loss: 2.8451  reconstruction loss:  2.838288316472187 contractive_loss:  67.78942380499069\n",
      "====> Epoch: 2801 Average loss: 1.2481  reconstruction loss:  1.241236833051635 contractive_loss:  68.74721973034342\n",
      "====> Epoch: 2802 Average loss: 0.5707  reconstruction loss:  0.5639692900906035 contractive_loss:  67.01321055415536\n",
      "====> Epoch: 2803 Average loss: 1.6447  reconstruction loss:  1.6380147326412524 contractive_loss:  66.44174158297524\n",
      "====> Epoch: 2804 Average loss: 1.7782  reconstruction loss:  1.7715190259320677 contractive_loss:  66.5894067911822\n",
      "====> Epoch: 2805 Average loss: 1.6539  reconstruction loss:  1.6472348144467983 contractive_loss:  67.02489895084065\n",
      "====> Epoch: 2806 Average loss: 1.3023  reconstruction loss:  1.2955021026777265 contractive_loss:  67.7108970094684\n",
      "====> Epoch: 2807 Average loss: 1.1456  reconstruction loss:  1.1388151542069767 contractive_loss:  68.23519674526315\n",
      "====> Epoch: 2808 Average loss: 0.8689  reconstruction loss:  0.8619835109770315 contractive_loss:  68.9440343490749\n",
      "====> Epoch: 2809 Average loss: 0.9260  reconstruction loss:  0.9189857240581624 contractive_loss:  69.92244874474568\n",
      "====> Epoch: 2810 Average loss: 0.6716  reconstruction loss:  0.6645892716396561 contractive_loss:  70.5485196617806\n",
      "====> Epoch: 2811 Average loss: 0.6602  reconstruction loss:  0.6531004841198615 contractive_loss:  71.08388888614341\n",
      "====> Epoch: 2812 Average loss: 0.6347  reconstruction loss:  0.6275486809457946 contractive_loss:  71.61526320694009\n",
      "====> Epoch: 2813 Average loss: 0.7212  reconstruction loss:  0.7139777059862967 contractive_loss:  72.0912935823658\n",
      "====> Epoch: 2814 Average loss: 0.6473  reconstruction loss:  0.6401117626789248 contractive_loss:  72.06944985289886\n",
      "====> Epoch: 2815 Average loss: 0.7392  reconstruction loss:  0.7320245137183927 contractive_loss:  71.58985794324701\n",
      "====> Epoch: 2816 Average loss: 1.0363  reconstruction loss:  1.029178818665979 contractive_loss:  70.77098943652491\n",
      "====> Epoch: 2817 Average loss: 1.0975  reconstruction loss:  1.0903504041120757 contractive_loss:  71.20708744017637\n",
      "====> Epoch: 2818 Average loss: 0.8940  reconstruction loss:  0.8868945864416691 contractive_loss:  71.45146209458733\n",
      "====> Epoch: 2819 Average loss: 0.5728  reconstruction loss:  0.5656599327733047 contractive_loss:  71.5504785982587\n",
      "====> Epoch: 2820 Average loss: 0.5586  reconstruction loss:  0.5514210580924689 contractive_loss:  71.58353618685159\n",
      "====> Epoch: 2821 Average loss: 0.5259  reconstruction loss:  0.5187162175951652 contractive_loss:  71.6116749498691\n",
      "====> Epoch: 2822 Average loss: 0.5514  reconstruction loss:  0.5442401029700596 contractive_loss:  71.54535023769608\n",
      "====> Epoch: 2823 Average loss: 0.5654  reconstruction loss:  0.5582515617606275 contractive_loss:  71.30798902703611\n",
      "====> Epoch: 2824 Average loss: 0.7125  reconstruction loss:  0.7053977656835835 contractive_loss:  71.10784853662106\n",
      "====> Epoch: 2825 Average loss: 0.5265  reconstruction loss:  0.5194045864019309 contractive_loss:  70.95330596155883\n",
      "====> Epoch: 2826 Average loss: 0.5426  reconstruction loss:  0.5355601822249247 contractive_loss:  70.81853782041289\n",
      "====> Epoch: 2827 Average loss: 0.5638  reconstruction loss:  0.5566877862936068 contractive_loss:  70.95596679265125\n",
      "====> Epoch: 2828 Average loss: 0.6141  reconstruction loss:  0.606968889100246 contractive_loss:  71.12754278655927\n",
      "====> Epoch: 2829 Average loss: 0.5923  reconstruction loss:  0.5852833182204388 contractive_loss:  70.54490713388438\n",
      "====> Epoch: 2830 Average loss: 0.7225  reconstruction loss:  0.7154456306980441 contractive_loss:  70.49406191124243\n",
      "====> Epoch: 2831 Average loss: 0.5995  reconstruction loss:  0.5924089684212184 contractive_loss:  70.81518442002816\n",
      "====> Epoch: 2832 Average loss: 0.5768  reconstruction loss:  0.5697366702320392 contractive_loss:  70.49366026235754\n",
      "====> Epoch: 2833 Average loss: 0.6230  reconstruction loss:  0.6159584726629727 contractive_loss:  70.0934074592779\n",
      "====> Epoch: 2834 Average loss: 0.5491  reconstruction loss:  0.5421210067624418 contractive_loss:  70.14558878929911\n",
      "====> Epoch: 2835 Average loss: 0.5466  reconstruction loss:  0.5395642639010423 contractive_loss:  70.35678993713609\n",
      "====> Epoch: 2836 Average loss: 1.2668  reconstruction loss:  1.2600103614899032 contractive_loss:  67.93371289120375\n",
      "====> Epoch: 2837 Average loss: 1.4345  reconstruction loss:  1.4279175111835247 contractive_loss:  65.93521296207443\n",
      "====> Epoch: 2838 Average loss: 1.4658  reconstruction loss:  1.4592749363191655 contractive_loss:  65.73083534271244\n",
      "====> Epoch: 2839 Average loss: 1.3535  reconstruction loss:  1.3468937279502464 contractive_loss:  65.93654706587856\n",
      "====> Epoch: 2840 Average loss: 0.9406  reconstruction loss:  0.9339326844574408 contractive_loss:  66.2817836699481\n",
      "====> Epoch: 2841 Average loss: 0.7232  reconstruction loss:  0.7164718069048023 contractive_loss:  66.87296549042085\n",
      "====> Epoch: 2842 Average loss: 0.6341  reconstruction loss:  0.6273195473827486 contractive_loss:  67.49777973103097\n",
      "====> Epoch: 2843 Average loss: 0.5669  reconstruction loss:  0.5601073583341392 contractive_loss:  68.31383916703611\n",
      "====> Epoch: 2844 Average loss: 0.5069  reconstruction loss:  0.5000660528705849 contractive_loss:  68.82784185551697\n",
      "====> Epoch: 2845 Average loss: 0.4550  reconstruction loss:  0.4480270035283347 contractive_loss:  69.38739349429055\n",
      "====> Epoch: 2846 Average loss: 0.5289  reconstruction loss:  0.5219909966167295 contractive_loss:  68.67774824670957\n",
      "====> Epoch: 2847 Average loss: 0.4929  reconstruction loss:  0.4860564623618103 contractive_loss:  68.5658048053766\n",
      "====> Epoch: 2848 Average loss: 0.4662  reconstruction loss:  0.45933894150751237 contractive_loss:  68.6721330212816\n",
      "====> Epoch: 2849 Average loss: 0.4651  reconstruction loss:  0.4582364506239324 contractive_loss:  68.60222308511044\n",
      "====> Epoch: 2850 Average loss: 0.4918  reconstruction loss:  0.4849299242915199 contractive_loss:  68.5057834295865\n",
      "====> Epoch: 2851 Average loss: 0.4718  reconstruction loss:  0.46496212674883813 contractive_loss:  68.41112706493757\n",
      "====> Epoch: 2852 Average loss: 0.4677  reconstruction loss:  0.46088302802157444 contractive_loss:  68.2765745156647\n",
      "====> Epoch: 2853 Average loss: 0.5825  reconstruction loss:  0.5756515784165296 contractive_loss:  68.01754503510651\n",
      "====> Epoch: 2854 Average loss: 0.6946  reconstruction loss:  0.6879440501247904 contractive_loss:  66.55379143287277\n",
      "====> Epoch: 2855 Average loss: 0.5637  reconstruction loss:  0.557025621456711 contractive_loss:  66.53911478221113\n",
      "====> Epoch: 2856 Average loss: 0.7312  reconstruction loss:  0.7245277547475325 contractive_loss:  66.630778365211\n",
      "====> Epoch: 2857 Average loss: 0.6040  reconstruction loss:  0.5972580598686311 contractive_loss:  67.0402653535119\n",
      "====> Epoch: 2858 Average loss: 0.8139  reconstruction loss:  0.8071525709939776 contractive_loss:  67.19795800164313\n",
      "====> Epoch: 2859 Average loss: 0.6229  reconstruction loss:  0.6163087066927929 contractive_loss:  65.90313395683242\n",
      "====> Epoch: 2860 Average loss: 0.5596  reconstruction loss:  0.5530479600715833 contractive_loss:  65.81926819545873\n",
      "====> Epoch: 2861 Average loss: 0.5043  reconstruction loss:  0.49772486323772386 contractive_loss:  66.20599649619355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2862 Average loss: 0.5734  reconstruction loss:  0.5667209903612218 contractive_loss:  66.53762615953691\n",
      "====> Epoch: 2863 Average loss: 0.4554  reconstruction loss:  0.4486861481609358 contractive_loss:  66.9260966130598\n",
      "====> Epoch: 2864 Average loss: 0.7676  reconstruction loss:  0.7611053466457119 contractive_loss:  64.9181462271392\n",
      "====> Epoch: 2865 Average loss: 0.9589  reconstruction loss:  0.9524168032674112 contractive_loss:  64.34285703127775\n",
      "====> Epoch: 2866 Average loss: 0.9509  reconstruction loss:  0.9444786897713863 contractive_loss:  64.22866858341199\n",
      "====> Epoch: 2867 Average loss: 0.8831  reconstruction loss:  0.8766555784647294 contractive_loss:  64.27101644659818\n",
      "====> Epoch: 2868 Average loss: 0.6357  reconstruction loss:  0.6292537180024061 contractive_loss:  64.38278131871853\n",
      "====> Epoch: 2869 Average loss: 0.5431  reconstruction loss:  0.5366541429225412 contractive_loss:  64.6163064750582\n",
      "====> Epoch: 2870 Average loss: 0.4716  reconstruction loss:  0.46508765803077595 contractive_loss:  64.96715458720628\n",
      "====> Epoch: 2871 Average loss: 0.4237  reconstruction loss:  0.41722081438919434 contractive_loss:  65.2202187646089\n",
      "====> Epoch: 2872 Average loss: 0.4669  reconstruction loss:  0.4604428911490658 contractive_loss:  64.90934986248672\n",
      "====> Epoch: 2873 Average loss: 0.5732  reconstruction loss:  0.5666795227819835 contractive_loss:  64.85752613942789\n",
      "====> Epoch: 2874 Average loss: 0.4544  reconstruction loss:  0.4479598767628869 contractive_loss:  64.49508135273557\n",
      "====> Epoch: 2875 Average loss: 0.4919  reconstruction loss:  0.4855095727155311 contractive_loss:  63.893130003072066\n",
      "====> Epoch: 2876 Average loss: 0.5058  reconstruction loss:  0.49938368541628847 contractive_loss:  63.81179277596146\n",
      "====> Epoch: 2877 Average loss: 0.5510  reconstruction loss:  0.5445964308438382 contractive_loss:  63.83111744685719\n",
      "====> Epoch: 2878 Average loss: 0.4124  reconstruction loss:  0.4059755942229246 contractive_loss:  64.10786628112128\n",
      "====> Epoch: 2879 Average loss: 0.4577  reconstruction loss:  0.4513161439001398 contractive_loss:  63.850727877734954\n",
      "====> Epoch: 2880 Average loss: 0.5020  reconstruction loss:  0.4956431350944607 contractive_loss:  63.38913518090227\n",
      "====> Epoch: 2881 Average loss: 0.5180  reconstruction loss:  0.5116595647977096 contractive_loss:  63.50582791252794\n",
      "====> Epoch: 2882 Average loss: 0.5633  reconstruction loss:  0.5569961163140181 contractive_loss:  63.46920213991227\n",
      "====> Epoch: 2883 Average loss: 0.6065  reconstruction loss:  0.6001902218097619 contractive_loss:  62.865608489599474\n",
      "====> Epoch: 2884 Average loss: 0.5840  reconstruction loss:  0.5777787138123683 contractive_loss:  62.465361286569696\n",
      "====> Epoch: 2885 Average loss: 1.1637  reconstruction loss:  1.1574497643643706 contractive_loss:  62.338848007289336\n",
      "====> Epoch: 2886 Average loss: 0.6878  reconstruction loss:  0.681524741112249 contractive_loss:  62.526695677835775\n",
      "====> Epoch: 2887 Average loss: 0.5728  reconstruction loss:  0.5665702023388387 contractive_loss:  62.39320649146886\n",
      "====> Epoch: 2888 Average loss: 0.4078  reconstruction loss:  0.40151858709497956 contractive_loss:  62.53427923741788\n",
      "====> Epoch: 2889 Average loss: 0.4169  reconstruction loss:  0.41060801463829627 contractive_loss:  62.570689996480276\n",
      "====> Epoch: 2890 Average loss: 0.4251  reconstruction loss:  0.4188618210835019 contractive_loss:  62.29923043898812\n",
      "====> Epoch: 2891 Average loss: 0.3989  reconstruction loss:  0.3927285797300018 contractive_loss:  62.098385035904364\n",
      "====> Epoch: 2892 Average loss: 0.4093  reconstruction loss:  0.40304833827008524 contractive_loss:  62.076795669753785\n",
      "====> Epoch: 2893 Average loss: 0.4968  reconstruction loss:  0.49064432615441994 contractive_loss:  61.98866838995185\n",
      "====> Epoch: 2894 Average loss: 0.4441  reconstruction loss:  0.43791369964818566 contractive_loss:  61.74656633174814\n",
      "====> Epoch: 2895 Average loss: 0.5114  reconstruction loss:  0.5052197684041729 contractive_loss:  61.84552699311046\n",
      "====> Epoch: 2896 Average loss: 0.4655  reconstruction loss:  0.45935132921738053 contractive_loss:  61.60047358428502\n",
      "====> Epoch: 2897 Average loss: 0.4125  reconstruction loss:  0.4063162631809334 contractive_loss:  61.58161168148157\n",
      "====> Epoch: 2898 Average loss: 0.7023  reconstruction loss:  0.6961122151514785 contractive_loss:  61.528068099671415\n",
      "====> Epoch: 2899 Average loss: 0.4663  reconstruction loss:  0.46020612600761474 contractive_loss:  61.35990969625386\n",
      "====> Epoch: 2900 Average loss: 0.4104  reconstruction loss:  0.40427092317322355 contractive_loss:  61.421162252917085\n",
      "====> Epoch: 2901 Average loss: 0.4963  reconstruction loss:  0.49013563622971473 contractive_loss:  61.352376634659954\n",
      "====> Epoch: 2902 Average loss: 0.4222  reconstruction loss:  0.4160860028826997 contractive_loss:  61.131116750747054\n",
      "====> Epoch: 2903 Average loss: 0.4377  reconstruction loss:  0.43160243804417786 contractive_loss:  60.9863726064097\n",
      "====> Epoch: 2904 Average loss: 0.4482  reconstruction loss:  0.44205058556888605 contractive_loss:  61.04044963926611\n",
      "====> Epoch: 2905 Average loss: 0.5856  reconstruction loss:  0.5794840038758314 contractive_loss:  60.94397577783373\n",
      "====> Epoch: 2906 Average loss: 0.6061  reconstruction loss:  0.6000638557661364 contractive_loss:  60.785015227376654\n",
      "====> Epoch: 2907 Average loss: 1.1114  reconstruction loss:  1.1052751280363942 contractive_loss:  61.01211412039218\n",
      "====> Epoch: 2908 Average loss: 0.8848  reconstruction loss:  0.8787456816437256 contractive_loss:  60.45435370206103\n",
      "====> Epoch: 2909 Average loss: 0.6600  reconstruction loss:  0.6539744267332901 contractive_loss:  60.22299686780692\n",
      "====> Epoch: 2910 Average loss: 0.5355  reconstruction loss:  0.5294706168392793 contractive_loss:  60.361656371987436\n",
      "====> Epoch: 2911 Average loss: 0.7053  reconstruction loss:  0.6992562206935472 contractive_loss:  60.32581454926728\n",
      "====> Epoch: 2912 Average loss: 0.4346  reconstruction loss:  0.4285661324844742 contractive_loss:  60.375107471724455\n",
      "====> Epoch: 2913 Average loss: 0.3862  reconstruction loss:  0.38016190398938526 contractive_loss:  60.222640482884934\n",
      "====> Epoch: 2914 Average loss: 0.4755  reconstruction loss:  0.46942320599760806 contractive_loss:  60.38980979647124\n",
      "====> Epoch: 2915 Average loss: 0.4874  reconstruction loss:  0.48138254005611164 contractive_loss:  60.1334429897424\n",
      "====> Epoch: 2916 Average loss: 0.4000  reconstruction loss:  0.39397744547092534 contractive_loss:  60.27020658611437\n",
      "====> Epoch: 2917 Average loss: 0.4377  reconstruction loss:  0.43163214765356417 contractive_loss:  60.28054710390137\n",
      "====> Epoch: 2918 Average loss: 0.4419  reconstruction loss:  0.4358834259123911 contractive_loss:  60.07585919912009\n",
      "====> Epoch: 2919 Average loss: 0.4047  reconstruction loss:  0.39866974371986885 contractive_loss:  60.347663065390066\n",
      "====> Epoch: 2920 Average loss: 0.6653  reconstruction loss:  0.6592529137363123 contractive_loss:  60.33200289768051\n",
      "====> Epoch: 2921 Average loss: 0.4234  reconstruction loss:  0.41740830722003686 contractive_loss:  60.20428072640469\n",
      "====> Epoch: 2922 Average loss: 0.4788  reconstruction loss:  0.4728151949825276 contractive_loss:  60.29212322315996\n",
      "====> Epoch: 2923 Average loss: 0.4986  reconstruction loss:  0.49256518156347595 contractive_loss:  60.237027904429\n",
      "====> Epoch: 2924 Average loss: 0.4171  reconstruction loss:  0.4110533009035702 contractive_loss:  60.259337549841256\n",
      "====> Epoch: 2925 Average loss: 0.3787  reconstruction loss:  0.3726512067352627 contractive_loss:  60.32390917189575\n",
      "====> Epoch: 2926 Average loss: 0.3906  reconstruction loss:  0.3845532736191785 contractive_loss:  60.29352585446825\n",
      "====> Epoch: 2927 Average loss: 0.3799  reconstruction loss:  0.373870498169945 contractive_loss:  60.219060078593365\n",
      "====> Epoch: 2928 Average loss: 0.4152  reconstruction loss:  0.4091623057975115 contractive_loss:  60.217149812681456\n",
      "====> Epoch: 2929 Average loss: 0.4116  reconstruction loss:  0.40554220570477584 contractive_loss:  60.43329051334028\n",
      "====> Epoch: 2930 Average loss: 0.3884  reconstruction loss:  0.382333639262217 contractive_loss:  60.365743200955116\n",
      "====> Epoch: 2931 Average loss: 0.4083  reconstruction loss:  0.4022722323787829 contractive_loss:  60.364673205253496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2932 Average loss: 0.6934  reconstruction loss:  0.6873975044967529 contractive_loss:  60.01336045580785\n",
      "====> Epoch: 2933 Average loss: 0.6050  reconstruction loss:  0.598951804688626 contractive_loss:  60.12801164213073\n",
      "====> Epoch: 2934 Average loss: 0.4635  reconstruction loss:  0.4574520004693072 contractive_loss:  60.151122675681734\n",
      "====> Epoch: 2935 Average loss: 0.5043  reconstruction loss:  0.49825674277192905 contractive_loss:  60.30495128170445\n",
      "====> Epoch: 2936 Average loss: 0.4345  reconstruction loss:  0.42841776453717206 contractive_loss:  60.637660338694204\n",
      "====> Epoch: 2937 Average loss: 0.5177  reconstruction loss:  0.5116108705085476 contractive_loss:  60.59638833917874\n",
      "====> Epoch: 2938 Average loss: 0.5383  reconstruction loss:  0.5322563847331151 contractive_loss:  60.64107482205548\n",
      "====> Epoch: 2939 Average loss: 0.5101  reconstruction loss:  0.5040005751374085 contractive_loss:  60.52386935275544\n",
      "====> Epoch: 2940 Average loss: 0.4339  reconstruction loss:  0.4277916184688529 contractive_loss:  60.63707130461662\n",
      "====> Epoch: 2941 Average loss: 0.4133  reconstruction loss:  0.40723686668658504 contractive_loss:  60.40627786132981\n",
      "====> Epoch: 2942 Average loss: 0.4196  reconstruction loss:  0.413604430540385 contractive_loss:  60.34714203781284\n",
      "====> Epoch: 2943 Average loss: 0.4057  reconstruction loss:  0.399684902022439 contractive_loss:  60.40681827879162\n",
      "====> Epoch: 2944 Average loss: 0.4115  reconstruction loss:  0.40543089247822306 contractive_loss:  60.23226458252203\n",
      "====> Epoch: 2945 Average loss: 0.3939  reconstruction loss:  0.387886623146168 contractive_loss:  60.22136383121216\n",
      "====> Epoch: 2946 Average loss: 0.4120  reconstruction loss:  0.40598326641918603 contractive_loss:  60.09140320102573\n",
      "====> Epoch: 2947 Average loss: 0.3933  reconstruction loss:  0.3872969421776756 contractive_loss:  60.153704406869906\n",
      "====> Epoch: 2948 Average loss: 0.4510  reconstruction loss:  0.4449600850217067 contractive_loss:  60.3160511908381\n",
      "====> Epoch: 2949 Average loss: 0.4218  reconstruction loss:  0.4157768470482381 contractive_loss:  60.23181583200547\n",
      "====> Epoch: 2950 Average loss: 0.5562  reconstruction loss:  0.5501542945346576 contractive_loss:  60.246899561857525\n",
      "====> Epoch: 2951 Average loss: 0.5691  reconstruction loss:  0.5630917847385359 contractive_loss:  60.1346980676881\n",
      "====> Epoch: 2952 Average loss: 0.4283  reconstruction loss:  0.42229567360103276 contractive_loss:  60.027465852900015\n",
      "====> Epoch: 2953 Average loss: 0.4539  reconstruction loss:  0.4479079399692701 contractive_loss:  60.32676874962375\n",
      "====> Epoch: 2954 Average loss: 0.3854  reconstruction loss:  0.37943601508493147 contractive_loss:  60.05774780441173\n",
      "====> Epoch: 2955 Average loss: 0.4029  reconstruction loss:  0.3968991948859573 contractive_loss:  60.22733991102724\n",
      "====> Epoch: 2956 Average loss: 0.5393  reconstruction loss:  0.5333225763945878 contractive_loss:  60.200523935861746\n",
      "====> Epoch: 2957 Average loss: 0.4398  reconstruction loss:  0.4337732831933836 contractive_loss:  60.154208864044115\n",
      "====> Epoch: 2958 Average loss: 0.4135  reconstruction loss:  0.40748574042181074 contractive_loss:  60.24660976222386\n",
      "====> Epoch: 2959 Average loss: 0.4615  reconstruction loss:  0.45548836156993716 contractive_loss:  60.347391792548045\n",
      "====> Epoch: 2960 Average loss: 0.4177  reconstruction loss:  0.4116810979641678 contractive_loss:  60.23323817599651\n",
      "====> Epoch: 2961 Average loss: 0.4531  reconstruction loss:  0.4471201071718853 contractive_loss:  60.103290461910575\n",
      "====> Epoch: 2962 Average loss: 0.4128  reconstruction loss:  0.4067613732882947 contractive_loss:  60.05170564827018\n",
      "====> Epoch: 2963 Average loss: 0.5611  reconstruction loss:  0.5551212394349553 contractive_loss:  60.21799695821289\n",
      "====> Epoch: 2964 Average loss: 0.6244  reconstruction loss:  0.6183703388635844 contractive_loss:  60.119042778994924\n",
      "====> Epoch: 2965 Average loss: 2.3049  reconstruction loss:  2.2985650632799564 contractive_loss:  63.673082177734784\n",
      "====> Epoch: 2966 Average loss: 0.8166  reconstruction loss:  0.808880063094637 contractive_loss:  77.45517949855703\n",
      "====> Epoch: 2967 Average loss: 1.4994  reconstruction loss:  1.4924467048050747 contractive_loss:  69.52875036142258\n",
      "====> Epoch: 2968 Average loss: 1.8362  reconstruction loss:  1.8293629790143724 contractive_loss:  68.53360038224771\n",
      "====> Epoch: 2969 Average loss: 0.9364  reconstruction loss:  0.9297645551917103 contractive_loss:  66.59547890149267\n",
      "====> Epoch: 2970 Average loss: 0.5280  reconstruction loss:  0.5213971530406029 contractive_loss:  66.02565818040834\n",
      "====> Epoch: 2971 Average loss: 0.5069  reconstruction loss:  0.5002219064478313 contractive_loss:  66.57581172378774\n",
      "====> Epoch: 2972 Average loss: 0.3878  reconstruction loss:  0.3811310274171488 contractive_loss:  66.48104663228146\n",
      "====> Epoch: 2973 Average loss: 0.4377  reconstruction loss:  0.43112435329540644 contractive_loss:  66.16660556181535\n",
      "====> Epoch: 2974 Average loss: 0.6703  reconstruction loss:  0.6636413684958284 contractive_loss:  66.16882071695639\n",
      "====> Epoch: 2975 Average loss: 0.6435  reconstruction loss:  0.6369419484102334 contractive_loss:  66.00660441490382\n",
      "====> Epoch: 2976 Average loss: 2.7560  reconstruction loss:  2.749421391528748 contractive_loss:  66.12053477380313\n",
      "====> Epoch: 2977 Average loss: 0.7639  reconstruction loss:  0.7573062669850443 contractive_loss:  65.55206056490823\n",
      "====> Epoch: 2978 Average loss: 0.7718  reconstruction loss:  0.7652931502490817 contractive_loss:  65.33861736809361\n",
      "====> Epoch: 2979 Average loss: 0.5433  reconstruction loss:  0.5366945771105978 contractive_loss:  65.59606753167942\n",
      "====> Epoch: 2980 Average loss: 0.5167  reconstruction loss:  0.5101196722914344 contractive_loss:  65.59299403346144\n",
      "====> Epoch: 2981 Average loss: 0.5532  reconstruction loss:  0.5466897165476626 contractive_loss:  65.44937735011116\n",
      "====> Epoch: 2982 Average loss: 0.4371  reconstruction loss:  0.4305846097040192 contractive_loss:  65.59517436154863\n",
      "====> Epoch: 2983 Average loss: 0.4222  reconstruction loss:  0.41562427306756106 contractive_loss:  65.53919915801679\n",
      "====> Epoch: 2984 Average loss: 0.4007  reconstruction loss:  0.3941067797200281 contractive_loss:  65.49110716473463\n",
      "====> Epoch: 2985 Average loss: 0.4151  reconstruction loss:  0.40854275305816684 contractive_loss:  65.38159692927115\n",
      "====> Epoch: 2986 Average loss: 0.4825  reconstruction loss:  0.4759194443552576 contractive_loss:  65.32314743949955\n",
      "====> Epoch: 2987 Average loss: 1.1636  reconstruction loss:  1.156927305961826 contractive_loss:  66.49884690492195\n",
      "====> Epoch: 2988 Average loss: 0.8101  reconstruction loss:  0.8035947498780162 contractive_loss:  65.21724261675574\n",
      "====> Epoch: 2989 Average loss: 0.6325  reconstruction loss:  0.6259374961334765 contractive_loss:  65.16378397035572\n",
      "====> Epoch: 2990 Average loss: 0.5461  reconstruction loss:  0.5396123132671561 contractive_loss:  64.81659423698302\n",
      "====> Epoch: 2991 Average loss: 0.4247  reconstruction loss:  0.41818480476615905 contractive_loss:  64.67180694689993\n",
      "====> Epoch: 2992 Average loss: 0.4087  reconstruction loss:  0.4022613058798312 contractive_loss:  64.86128341083644\n",
      "====> Epoch: 2993 Average loss: 0.5286  reconstruction loss:  0.5220558171481195 contractive_loss:  65.04447687596405\n",
      "====> Epoch: 2994 Average loss: 0.6107  reconstruction loss:  0.6042381095275393 contractive_loss:  64.8449114271644\n",
      "====> Epoch: 2995 Average loss: 0.5869  reconstruction loss:  0.5804065415885308 contractive_loss:  64.74247984014717\n",
      "====> Epoch: 2996 Average loss: 0.5262  reconstruction loss:  0.5197814217217942 contractive_loss:  64.44516802025906\n",
      "====> Epoch: 2997 Average loss: 0.5011  reconstruction loss:  0.4946499693989245 contractive_loss:  64.70151011663143\n",
      "====> Epoch: 2998 Average loss: 0.4446  reconstruction loss:  0.43814143020564716 contractive_loss:  64.77900196708214\n",
      "====> Epoch: 2999 Average loss: 1.1512  reconstruction loss:  1.1446475150029627 contractive_loss:  65.79972825303699\n"
     ]
    }
   ],
   "source": [
    "path = \"./models/manifold/CAE\"\n",
    "cae = CAE(x_dim, h_dim, h_dim2, 100)\n",
    "cae.fit(path, X, Xval, opt=torch.optim.Adam, opt_kwargs={\"lr\": (5e-2)}, batch_size=64, epochs=3000, verbose=True, comment='manifold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "B_SPANS = cae.get_spans(X)\n",
    "B_SPANSval = cae.get_spans(Xval)\n",
    "B_SPANStest = cae.get_spans(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- training non-strategically----------\n",
      "batch 001 / 019 | loss: 1.02394 | err: 0.32812\n",
      "batch 002 / 019 | loss: 0.63633 | err: 0.17969\n",
      "batch 003 / 019 | loss: 0.45570 | err: 0.11979\n",
      "batch 004 / 019 | loss: 0.38126 | err: 0.10938\n",
      "batch 005 / 019 | loss: 0.32444 | err: 0.09375\n",
      "batch 006 / 019 | loss: 0.28923 | err: 0.08333\n",
      "batch 007 / 019 | loss: 0.26239 | err: 0.07366\n",
      "batch 008 / 019 | loss: 0.23655 | err: 0.06445\n",
      "batch 009 / 019 | loss: 0.22352 | err: 0.06076\n",
      "batch 010 / 019 | loss: 0.21293 | err: 0.05625\n",
      "batch 011 / 019 | loss: 0.20388 | err: 0.05256\n",
      "batch 012 / 019 | loss: 0.19570 | err: 0.04948\n",
      "batch 013 / 019 | loss: 0.18636 | err: 0.04808\n",
      "batch 014 / 019 | loss: 0.17625 | err: 0.04464\n",
      "batch 015 / 019 | loss: 0.16851 | err: 0.04167\n",
      "batch 016 / 019 | loss: 0.15965 | err: 0.04004\n",
      "batch 017 / 019 | loss: 0.15724 | err: 0.03952\n",
      "batch 018 / 019 | loss: 0.15253 | err: 0.03733\n",
      "batch 019 / 019 | loss: 0.14838 | err: 0.03536\n",
      "model saved!\n",
      "----- epoch 001 / 006 | time: 000 sec | loss: 0.06115 | err: 0.00500\n",
      "batch 001 / 019 | loss: 0.06673 | err: 0.00000\n",
      "batch 002 / 019 | loss: 0.06654 | err: 0.00781\n",
      "batch 003 / 019 | loss: 0.06306 | err: 0.00521\n",
      "batch 004 / 019 | loss: 0.06076 | err: 0.00391\n",
      "batch 005 / 019 | loss: 0.05412 | err: 0.00313\n",
      "batch 006 / 019 | loss: 0.04896 | err: 0.00260\n",
      "batch 007 / 019 | loss: 0.05076 | err: 0.00223\n",
      "batch 008 / 019 | loss: 0.05115 | err: 0.00195\n",
      "batch 009 / 019 | loss: 0.05445 | err: 0.00521\n",
      "batch 010 / 019 | loss: 0.05349 | err: 0.00469\n",
      "batch 011 / 019 | loss: 0.05518 | err: 0.00710\n",
      "batch 012 / 019 | loss: 0.05149 | err: 0.00651\n",
      "batch 013 / 019 | loss: 0.05248 | err: 0.00601\n",
      "batch 014 / 019 | loss: 0.05148 | err: 0.00670\n",
      "batch 015 / 019 | loss: 0.05061 | err: 0.00625\n",
      "batch 016 / 019 | loss: 0.04992 | err: 0.00586\n",
      "batch 017 / 019 | loss: 0.04975 | err: 0.00551\n",
      "batch 018 / 019 | loss: 0.04960 | err: 0.00521\n",
      "batch 019 / 019 | loss: 0.05317 | err: 0.00713\n",
      "model saved!\n",
      "----- epoch 002 / 006 | time: 000 sec | loss: 0.05003 | err: 0.00250\n",
      "batch 001 / 019 | loss: 0.07292 | err: 0.03125\n",
      "batch 002 / 019 | loss: 0.07088 | err: 0.03125\n",
      "batch 003 / 019 | loss: 0.05897 | err: 0.02083\n",
      "batch 004 / 019 | loss: 0.06539 | err: 0.01953\n",
      "batch 005 / 019 | loss: 0.05805 | err: 0.01562\n",
      "batch 006 / 019 | loss: 0.05503 | err: 0.01302\n",
      "batch 007 / 019 | loss: 0.05091 | err: 0.01116\n",
      "batch 008 / 019 | loss: 0.05273 | err: 0.00977\n",
      "batch 009 / 019 | loss: 0.05190 | err: 0.00868\n",
      "batch 010 / 019 | loss: 0.05426 | err: 0.01094\n",
      "batch 011 / 019 | loss: 0.05183 | err: 0.00994\n",
      "batch 012 / 019 | loss: 0.04977 | err: 0.00911\n",
      "batch 013 / 019 | loss: 0.04941 | err: 0.00962\n",
      "batch 014 / 019 | loss: 0.04785 | err: 0.00893\n",
      "batch 015 / 019 | loss: 0.04585 | err: 0.00833\n",
      "batch 016 / 019 | loss: 0.04462 | err: 0.00781\n",
      "batch 017 / 019 | loss: 0.04612 | err: 0.00735\n",
      "batch 018 / 019 | loss: 0.04568 | err: 0.00694\n",
      "batch 019 / 019 | loss: 0.04538 | err: 0.00658\n",
      "----- epoch 003 / 006 | time: 000 sec | loss: 0.04501 | err: 0.00250\n",
      "batch 001 / 019 | loss: 0.01944 | err: 0.00000\n",
      "batch 002 / 019 | loss: 0.01807 | err: 0.00000\n",
      "batch 003 / 019 | loss: 0.02985 | err: 0.00000\n",
      "batch 004 / 019 | loss: 0.02853 | err: 0.00000\n",
      "batch 005 / 019 | loss: 0.03089 | err: 0.00000\n",
      "batch 006 / 019 | loss: 0.03176 | err: 0.00000\n",
      "batch 007 / 019 | loss: 0.03439 | err: 0.00000\n",
      "batch 008 / 019 | loss: 0.03538 | err: 0.00000\n",
      "batch 009 / 019 | loss: 0.03524 | err: 0.00174\n",
      "batch 010 / 019 | loss: 0.03766 | err: 0.00469\n",
      "batch 011 / 019 | loss: 0.03713 | err: 0.00426\n",
      "batch 012 / 019 | loss: 0.03994 | err: 0.00781\n",
      "batch 013 / 019 | loss: 0.04375 | err: 0.00721\n",
      "batch 014 / 019 | loss: 0.04381 | err: 0.00781\n",
      "batch 015 / 019 | loss: 0.04516 | err: 0.00729\n",
      "batch 016 / 019 | loss: 0.04286 | err: 0.00684\n",
      "batch 017 / 019 | loss: 0.04271 | err: 0.00643\n",
      "batch 018 / 019 | loss: 0.04294 | err: 0.00694\n",
      "batch 019 / 019 | loss: 0.04172 | err: 0.00658\n",
      "model saved!\n",
      "----- epoch 004 / 006 | time: 000 sec | loss: 0.04155 | err: 0.00000\n",
      "batch 001 / 019 | loss: 0.05168 | err: 0.00000\n",
      "batch 002 / 019 | loss: 0.04205 | err: 0.00000\n",
      "batch 003 / 019 | loss: 0.04563 | err: 0.00000\n",
      "batch 004 / 019 | loss: 0.04046 | err: 0.00000\n",
      "batch 005 / 019 | loss: 0.03793 | err: 0.00000\n",
      "batch 006 / 019 | loss: 0.03788 | err: 0.00000\n",
      "batch 007 / 019 | loss: 0.03902 | err: 0.00000\n",
      "batch 008 / 019 | loss: 0.03832 | err: 0.00000\n",
      "batch 009 / 019 | loss: 0.03874 | err: 0.00000\n",
      "batch 010 / 019 | loss: 0.03899 | err: 0.00000\n",
      "batch 011 / 019 | loss: 0.03804 | err: 0.00000\n",
      "batch 012 / 019 | loss: 0.03835 | err: 0.00260\n",
      "batch 013 / 019 | loss: 0.03942 | err: 0.00240\n",
      "batch 014 / 019 | loss: 0.03774 | err: 0.00223\n",
      "batch 015 / 019 | loss: 0.03647 | err: 0.00208\n",
      "batch 016 / 019 | loss: 0.03777 | err: 0.00195\n",
      "batch 017 / 019 | loss: 0.03993 | err: 0.00184\n",
      "batch 018 / 019 | loss: 0.04054 | err: 0.00174\n",
      "batch 019 / 019 | loss: 0.03864 | err: 0.00164\n",
      "----- epoch 005 / 006 | time: 000 sec | loss: 0.03897 | err: 0.00000\n",
      "batch 001 / 019 | loss: 0.04357 | err: 0.00000\n",
      "batch 002 / 019 | loss: 0.05756 | err: 0.00000\n",
      "batch 003 / 019 | loss: 0.04654 | err: 0.00000\n",
      "batch 004 / 019 | loss: 0.05819 | err: 0.00391\n",
      "batch 005 / 019 | loss: 0.05088 | err: 0.00313\n",
      "batch 006 / 019 | loss: 0.04698 | err: 0.00260\n",
      "batch 007 / 019 | loss: 0.04321 | err: 0.00223\n",
      "batch 008 / 019 | loss: 0.04026 | err: 0.00195\n",
      "batch 009 / 019 | loss: 0.03914 | err: 0.00174\n",
      "batch 010 / 019 | loss: 0.03836 | err: 0.00156\n",
      "batch 011 / 019 | loss: 0.03600 | err: 0.00142\n",
      "batch 012 / 019 | loss: 0.03519 | err: 0.00130\n",
      "batch 013 / 019 | loss: 0.03398 | err: 0.00120\n",
      "batch 014 / 019 | loss: 0.03365 | err: 0.00112\n",
      "batch 015 / 019 | loss: 0.03444 | err: 0.00104\n",
      "batch 016 / 019 | loss: 0.03427 | err: 0.00098\n",
      "batch 017 / 019 | loss: 0.03610 | err: 0.00092\n",
      "batch 018 / 019 | loss: 0.03634 | err: 0.00087\n",
      "batch 019 / 019 | loss: 0.03687 | err: 0.00082\n",
      "----- epoch 006 / 006 | time: 000 sec | loss: 0.03691 | err: 0.00000\n",
      "training time: 0.3876159191131592 seconds\n",
      "---------- training strategically----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:163: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 019 | loss: 1.82980 | err: 0.53125\n",
      "batch 002 / 019 | loss: 1.50959 | err: 0.47656\n",
      "batch 003 / 019 | loss: 1.44019 | err: 0.48958\n",
      "batch 004 / 019 | loss: 1.36033 | err: 0.50000\n",
      "batch 005 / 019 | loss: 1.19798 | err: 0.46250\n",
      "batch 006 / 019 | loss: 1.13006 | err: 0.44271\n",
      "batch 007 / 019 | loss: 1.16374 | err: 0.45089\n",
      "batch 008 / 019 | loss: 1.16693 | err: 0.44922\n",
      "batch 009 / 019 | loss: 1.14414 | err: 0.43750\n",
      "batch 010 / 019 | loss: 1.07604 | err: 0.42031\n",
      "batch 011 / 019 | loss: 1.05423 | err: 0.42472\n",
      "batch 012 / 019 | loss: 1.04167 | err: 0.42969\n",
      "batch 013 / 019 | loss: 1.02954 | err: 0.43149\n",
      "batch 014 / 019 | loss: 1.04164 | err: 0.44308\n",
      "batch 015 / 019 | loss: 1.04621 | err: 0.45104\n",
      "batch 016 / 019 | loss: 1.04000 | err: 0.45508\n",
      "batch 017 / 019 | loss: 1.01621 | err: 0.45129\n",
      "batch 018 / 019 | loss: 0.99747 | err: 0.44705\n",
      "batch 019 / 019 | loss: 0.97704 | err: 0.42681\n",
      "model saved!\n",
      "----- epoch 001 / 006 | time: 159 sec | loss: 0.62526 | err: 0.16250\n",
      "batch 001 / 019 | loss: 0.70196 | err: 0.23438\n",
      "batch 002 / 019 | loss: 0.66375 | err: 0.19531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\problems\\problem.py:1055: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 003 / 019 | loss: 0.60803 | err: 0.16146\n",
      "batch 004 / 019 | loss: 0.59704 | err: 0.18359\n",
      "batch 005 / 019 | loss: 0.60636 | err: 0.22812\n",
      "batch 006 / 019 | loss: 0.63670 | err: 0.27083\n",
      "batch 007 / 019 | loss: 0.62479 | err: 0.27902\n",
      "batch 008 / 019 | loss: 0.59606 | err: 0.26953\n",
      "batch 009 / 019 | loss: 0.58091 | err: 0.26736\n",
      "batch 010 / 019 | loss: 0.56805 | err: 0.26719\n",
      "batch 011 / 019 | loss: 0.56243 | err: 0.26989\n",
      "batch 012 / 019 | loss: 0.53599 | err: 0.25911\n",
      "batch 013 / 019 | loss: 0.52968 | err: 0.25962\n",
      "batch 014 / 019 | loss: 0.52959 | err: 0.26451\n",
      "batch 015 / 019 | loss: 0.51846 | err: 0.26146\n",
      "batch 016 / 019 | loss: 0.50895 | err: 0.25879\n",
      "batch 017 / 019 | loss: 0.50996 | err: 0.26287\n",
      "batch 018 / 019 | loss: 0.51292 | err: 0.26736\n",
      "batch 019 / 019 | loss: 0.51049 | err: 0.26754\n",
      "----- epoch 002 / 006 | time: 198 sec | loss: 0.43507 | err: 0.25500\n",
      "batch 001 / 019 | loss: 0.50044 | err: 0.32812\n",
      "batch 002 / 019 | loss: 0.48181 | err: 0.28906\n",
      "batch 003 / 019 | loss: 0.50231 | err: 0.31250\n",
      "batch 004 / 019 | loss: 0.49850 | err: 0.30469\n",
      "batch 005 / 019 | loss: 0.47377 | err: 0.27187\n",
      "batch 006 / 019 | loss: 0.45227 | err: 0.23438\n",
      "batch 007 / 019 | loss: 0.45495 | err: 0.21652\n",
      "batch 008 / 019 | loss: 0.47419 | err: 0.22070\n",
      "batch 009 / 019 | loss: 0.46343 | err: 0.20139\n",
      "batch 010 / 019 | loss: 0.45119 | err: 0.18594\n",
      "batch 011 / 019 | loss: 0.44084 | err: 0.18466\n",
      "batch 012 / 019 | loss: 0.44320 | err: 0.19141\n",
      "batch 013 / 019 | loss: 0.44365 | err: 0.19591\n",
      "batch 014 / 019 | loss: 0.43301 | err: 0.18973\n",
      "batch 015 / 019 | loss: 0.42486 | err: 0.18854\n",
      "batch 016 / 019 | loss: 0.43119 | err: 0.19922\n",
      "batch 017 / 019 | loss: 0.42230 | err: 0.19393\n",
      "batch 018 / 019 | loss: 0.41680 | err: 0.19358\n",
      "batch 019 / 019 | loss: 0.41442 | err: 0.19545\n",
      "----- epoch 003 / 006 | time: 233 sec | loss: 0.33314 | err: 0.18000\n",
      "batch 001 / 019 | loss: 0.36433 | err: 0.20312\n",
      "batch 002 / 019 | loss: 0.34753 | err: 0.19531\n",
      "batch 003 / 019 | loss: 0.34529 | err: 0.19271\n",
      "batch 004 / 019 | loss: 0.36348 | err: 0.20703\n",
      "batch 005 / 019 | loss: 0.36343 | err: 0.20625\n",
      "batch 006 / 019 | loss: 0.36021 | err: 0.20312\n",
      "batch 007 / 019 | loss: 0.36858 | err: 0.20982\n",
      "batch 008 / 019 | loss: 0.36397 | err: 0.20898\n",
      "batch 009 / 019 | loss: 0.35136 | err: 0.19965\n",
      "batch 010 / 019 | loss: 0.35422 | err: 0.20156\n",
      "batch 011 / 019 | loss: 0.34852 | err: 0.19886\n",
      "batch 012 / 019 | loss: 0.34436 | err: 0.19661\n",
      "batch 013 / 019 | loss: 0.34628 | err: 0.19712\n",
      "batch 014 / 019 | loss: 0.33449 | err: 0.18638\n",
      "batch 015 / 019 | loss: 0.33146 | err: 0.18438\n",
      "batch 016 / 019 | loss: 0.32500 | err: 0.18066\n",
      "batch 017 / 019 | loss: 0.31879 | err: 0.17739\n",
      "batch 018 / 019 | loss: 0.32375 | err: 0.18056\n",
      "batch 019 / 019 | loss: 0.32062 | err: 0.17873\n",
      "model saved!\n",
      "----- epoch 004 / 006 | time: 233 sec | loss: 0.27167 | err: 0.14750\n",
      "batch 001 / 019 | loss: 0.30911 | err: 0.17188\n",
      "batch 002 / 019 | loss: 0.36188 | err: 0.20312\n",
      "batch 003 / 019 | loss: 0.30310 | err: 0.16667\n",
      "batch 004 / 019 | loss: 0.25017 | err: 0.13281\n",
      "batch 005 / 019 | loss: 0.22469 | err: 0.11875\n",
      "batch 006 / 019 | loss: 0.22817 | err: 0.11979\n",
      "batch 007 / 019 | loss: 0.24030 | err: 0.12723\n",
      "batch 008 / 019 | loss: 0.24517 | err: 0.13086\n",
      "batch 009 / 019 | loss: 0.24351 | err: 0.13021\n",
      "batch 010 / 019 | loss: 0.24768 | err: 0.13281\n",
      "batch 011 / 019 | loss: 0.23894 | err: 0.12784\n",
      "batch 012 / 019 | loss: 0.23652 | err: 0.12630\n",
      "batch 013 / 019 | loss: 0.23813 | err: 0.12740\n",
      "batch 014 / 019 | loss: 0.22981 | err: 0.12277\n",
      "batch 015 / 019 | loss: 0.22866 | err: 0.12187\n",
      "batch 016 / 019 | loss: 0.23082 | err: 0.12305\n",
      "batch 017 / 019 | loss: 0.23291 | err: 0.12408\n",
      "batch 018 / 019 | loss: 0.23358 | err: 0.12413\n",
      "batch 019 / 019 | loss: 0.23773 | err: 0.12637\n",
      "model saved!\n",
      "----- epoch 005 / 006 | time: 219 sec | loss: 0.20737 | err: 0.11000\n",
      "batch 001 / 019 | loss: 0.17503 | err: 0.09375\n",
      "batch 002 / 019 | loss: 0.19194 | err: 0.10156\n",
      "batch 003 / 019 | loss: 0.17533 | err: 0.09375\n",
      "batch 004 / 019 | loss: 0.18387 | err: 0.09766\n",
      "batch 005 / 019 | loss: 0.18767 | err: 0.10000\n",
      "batch 006 / 019 | loss: 0.18630 | err: 0.09896\n",
      "batch 007 / 019 | loss: 0.18949 | err: 0.10045\n",
      "batch 008 / 019 | loss: 0.18364 | err: 0.09766\n",
      "batch 009 / 019 | loss: 0.17428 | err: 0.09201\n",
      "batch 010 / 019 | loss: 0.17175 | err: 0.09062\n",
      "batch 011 / 019 | loss: 0.16935 | err: 0.08949\n",
      "batch 012 / 019 | loss: 0.17013 | err: 0.08984\n",
      "batch 013 / 019 | loss: 0.17904 | err: 0.09495\n",
      "batch 014 / 019 | loss: 0.17689 | err: 0.09375\n",
      "batch 015 / 019 | loss: 0.17717 | err: 0.09375\n",
      "batch 016 / 019 | loss: 0.18270 | err: 0.09668\n",
      "batch 017 / 019 | loss: 0.18088 | err: 0.09559\n",
      "batch 018 / 019 | loss: 0.18397 | err: 0.09722\n",
      "batch 019 / 019 | loss: 0.19273 | err: 0.10197\n",
      "model saved!\n",
      "----- epoch 006 / 006 | time: 220 sec | loss: 0.15748 | err: 0.08000\n",
      "training time: 1266.445344209671 seconds\n",
      "---------- training strategically----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:163: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 019 | loss: 1.24259 | err: 0.45312\n",
      "batch 002 / 019 | loss: 0.79267 | err: 0.28125\n",
      "batch 003 / 019 | loss: 0.61963 | err: 0.22396\n",
      "batch 004 / 019 | loss: 0.55348 | err: 0.20312\n",
      "batch 005 / 019 | loss: 0.50085 | err: 0.18750\n",
      "batch 006 / 019 | loss: 0.51183 | err: 0.19531\n",
      "batch 007 / 019 | loss: 0.50604 | err: 0.19420\n",
      "batch 008 / 019 | loss: 0.51882 | err: 0.19922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\problems\\problem.py:1055: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 009 / 019 | loss: 0.48798 | err: 0.18924\n",
      "batch 010 / 019 | loss: 0.46330 | err: 0.17969\n",
      "batch 011 / 019 | loss: 0.47971 | err: 0.18608\n",
      "batch 012 / 019 | loss: 0.47257 | err: 0.18359\n",
      "batch 013 / 019 | loss: 0.44631 | err: 0.17308\n",
      "batch 014 / 019 | loss: 0.44622 | err: 0.17299\n",
      "batch 015 / 019 | loss: 0.44610 | err: 0.17396\n",
      "batch 016 / 019 | loss: 0.44242 | err: 0.17383\n",
      "batch 017 / 019 | loss: 0.43662 | err: 0.17279\n",
      "batch 018 / 019 | loss: 0.44271 | err: 0.17708\n",
      "batch 019 / 019 | loss: 0.42697 | err: 0.17105\n",
      "model saved!\n",
      "----- epoch 001 / 006 | time: 156 sec | loss: 0.25900 | err: 0.12250\n",
      "batch 001 / 019 | loss: 0.38842 | err: 0.18750\n",
      "batch 002 / 019 | loss: 0.32219 | err: 0.14844\n",
      "batch 003 / 019 | loss: 0.23275 | err: 0.10417\n",
      "batch 004 / 019 | loss: 0.20951 | err: 0.09375\n",
      "batch 005 / 019 | loss: 0.19923 | err: 0.09062\n",
      "batch 006 / 019 | loss: 0.20041 | err: 0.09115\n",
      "batch 007 / 019 | loss: 0.18474 | err: 0.08259\n",
      "batch 008 / 019 | loss: 0.17545 | err: 0.07812\n",
      "batch 009 / 019 | loss: 0.18408 | err: 0.07986\n",
      "batch 010 / 019 | loss: 0.18224 | err: 0.07500\n",
      "batch 011 / 019 | loss: 0.19056 | err: 0.07670\n",
      "batch 012 / 019 | loss: 0.17814 | err: 0.07031\n",
      "batch 013 / 019 | loss: 0.17950 | err: 0.07212\n",
      "batch 014 / 019 | loss: 0.17663 | err: 0.07143\n",
      "batch 015 / 019 | loss: 0.17133 | err: 0.06979\n",
      "batch 016 / 019 | loss: 0.17245 | err: 0.07031\n",
      "batch 017 / 019 | loss: 0.17308 | err: 0.07169\n",
      "batch 018 / 019 | loss: 0.17885 | err: 0.07378\n",
      "batch 019 / 019 | loss: 0.18597 | err: 0.07648\n",
      "model saved!\n",
      "----- epoch 002 / 006 | time: 162 sec | loss: 0.16497 | err: 0.07000\n",
      "batch 001 / 019 | loss: 0.20537 | err: 0.07812\n",
      "batch 002 / 019 | loss: 0.21814 | err: 0.08594\n",
      "batch 003 / 019 | loss: 0.23410 | err: 0.09896\n",
      "batch 004 / 019 | loss: 0.23279 | err: 0.09766\n",
      "batch 005 / 019 | loss: 0.19364 | err: 0.08125\n",
      "batch 006 / 019 | loss: 0.17451 | err: 0.07292\n",
      "batch 007 / 019 | loss: 0.16456 | err: 0.06920\n",
      "batch 008 / 019 | loss: 0.18754 | err: 0.08008\n",
      "batch 009 / 019 | loss: 0.17929 | err: 0.07639\n",
      "batch 010 / 019 | loss: 0.17200 | err: 0.07344\n",
      "batch 011 / 019 | loss: 0.16356 | err: 0.06960\n",
      "batch 012 / 019 | loss: 0.16200 | err: 0.06901\n",
      "batch 013 / 019 | loss: 0.15805 | err: 0.06851\n",
      "batch 014 / 019 | loss: 0.15306 | err: 0.06696\n",
      "batch 015 / 019 | loss: 0.14968 | err: 0.06667\n",
      "batch 016 / 019 | loss: 0.14813 | err: 0.06738\n",
      "batch 017 / 019 | loss: 0.14700 | err: 0.06618\n",
      "batch 018 / 019 | loss: 0.14567 | err: 0.06510\n",
      "batch 019 / 019 | loss: 0.14547 | err: 0.06497\n",
      "model saved!\n",
      "----- epoch 003 / 006 | time: 160 sec | loss: 0.13644 | err: 0.05750\n",
      "batch 001 / 019 | loss: 0.07463 | err: 0.03125\n",
      "batch 002 / 019 | loss: 0.12010 | err: 0.05469\n",
      "batch 003 / 019 | loss: 0.11467 | err: 0.05729\n",
      "batch 004 / 019 | loss: 0.10252 | err: 0.05078\n",
      "batch 005 / 019 | loss: 0.10453 | err: 0.05312\n",
      "batch 006 / 019 | loss: 0.10998 | err: 0.05469\n",
      "batch 007 / 019 | loss: 0.12240 | err: 0.06027\n",
      "batch 008 / 019 | loss: 0.11946 | err: 0.05859\n",
      "batch 009 / 019 | loss: 0.11715 | err: 0.05729\n",
      "batch 010 / 019 | loss: 0.12398 | err: 0.05937\n",
      "batch 011 / 019 | loss: 0.12203 | err: 0.05824\n",
      "batch 012 / 019 | loss: 0.12722 | err: 0.05990\n",
      "batch 013 / 019 | loss: 0.14131 | err: 0.06490\n",
      "batch 014 / 019 | loss: 0.13902 | err: 0.06362\n",
      "batch 015 / 019 | loss: 0.14306 | err: 0.06458\n",
      "batch 016 / 019 | loss: 0.13617 | err: 0.06152\n",
      "batch 017 / 019 | loss: 0.13389 | err: 0.06066\n",
      "batch 018 / 019 | loss: 0.13341 | err: 0.05990\n",
      "batch 019 / 019 | loss: 0.13307 | err: 0.06003\n",
      "model saved!\n",
      "----- epoch 004 / 006 | time: 175 sec | loss: 0.14116 | err: 0.05500\n",
      "batch 001 / 019 | loss: 0.14983 | err: 0.06250\n",
      "batch 002 / 019 | loss: 0.13107 | err: 0.05469\n",
      "batch 003 / 019 | loss: 0.11682 | err: 0.05208\n",
      "batch 004 / 019 | loss: 0.09469 | err: 0.04297\n",
      "batch 005 / 019 | loss: 0.08803 | err: 0.04063\n",
      "batch 006 / 019 | loss: 0.09639 | err: 0.04427\n",
      "batch 007 / 019 | loss: 0.11169 | err: 0.05134\n",
      "batch 008 / 019 | loss: 0.11227 | err: 0.05273\n",
      "batch 009 / 019 | loss: 0.11962 | err: 0.05556\n",
      "batch 010 / 019 | loss: 0.12268 | err: 0.05625\n",
      "batch 011 / 019 | loss: 0.11891 | err: 0.05398\n",
      "batch 012 / 019 | loss: 0.12571 | err: 0.05599\n",
      "batch 013 / 019 | loss: 0.13610 | err: 0.06010\n",
      "batch 014 / 019 | loss: 0.13109 | err: 0.05804\n",
      "batch 015 / 019 | loss: 0.12930 | err: 0.05729\n",
      "batch 016 / 019 | loss: 0.13117 | err: 0.05762\n",
      "batch 017 / 019 | loss: 0.13525 | err: 0.05882\n",
      "batch 018 / 019 | loss: 0.13617 | err: 0.05903\n",
      "batch 019 / 019 | loss: 0.14114 | err: 0.06140\n",
      "----- epoch 005 / 006 | time: 180 sec | loss: 0.13934 | err: 0.05750\n",
      "batch 001 / 019 | loss: 0.12084 | err: 0.04688\n",
      "batch 002 / 019 | loss: 0.13903 | err: 0.05469\n",
      "batch 003 / 019 | loss: 0.14997 | err: 0.06250\n",
      "batch 004 / 019 | loss: 0.14164 | err: 0.05859\n",
      "batch 005 / 019 | loss: 0.14300 | err: 0.05937\n",
      "batch 006 / 019 | loss: 0.13743 | err: 0.05729\n",
      "batch 007 / 019 | loss: 0.13672 | err: 0.05804\n",
      "batch 008 / 019 | loss: 0.12852 | err: 0.05469\n",
      "batch 009 / 019 | loss: 0.12376 | err: 0.05208\n",
      "batch 010 / 019 | loss: 0.12769 | err: 0.05469\n",
      "batch 011 / 019 | loss: 0.11982 | err: 0.05114\n",
      "batch 012 / 019 | loss: 0.12057 | err: 0.05208\n",
      "batch 013 / 019 | loss: 0.11706 | err: 0.05048\n",
      "batch 014 / 019 | loss: 0.11550 | err: 0.05022\n",
      "batch 015 / 019 | loss: 0.11774 | err: 0.05208\n",
      "batch 016 / 019 | loss: 0.11869 | err: 0.05371\n",
      "batch 017 / 019 | loss: 0.11973 | err: 0.05423\n",
      "batch 018 / 019 | loss: 0.12091 | err: 0.05469\n",
      "batch 019 / 019 | loss: 0.11906 | err: 0.05400\n",
      "model saved!\n",
      "----- epoch 006 / 006 | time: 199 sec | loss: 0.08924 | err: 0.04000\n",
      "training time: 1034.7573926448822 seconds\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "PATH = \"./models/manifold/\" + now.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "\n",
    "EPOCHS = 6\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# non-strategic classification\n",
    "print(\"---------- training non-strategically----------\")\n",
    "non_strategic_model = MyStrategicModel(x_dim, h_dim, funcs, TRAIN_SLOPE, EVAL_SLOPE, strategic=False)\n",
    "\n",
    "non_strategic_model.fit(PATH, X, B_SPANS, Y, Xval, B_SPANSval, Yval,\n",
    "                                opt=torch.optim.Adam, opt_kwargs={\"lr\": 5*(1e-1)},\n",
    "                                batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=True,\n",
    "                                comment=\"non_strategic\")\n",
    "\n",
    "# strategic classification\n",
    "print(\"---------- training strategically----------\")\n",
    "strategic_model_naive = MyStrategicModel(x_dim, h_dim, funcs, TRAIN_SLOPE, EVAL_SLOPE, strategic=True, manifold=False)\n",
    "\n",
    "strategic_model_naive.fit(PATH, X, B_SPANS, Y, Xval, B_SPANSval, Yval,\n",
    "                                opt=torch.optim.Adam, opt_kwargs={\"lr\": 5*(1e-1)},\n",
    "                                batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=True,\n",
    "                                comment=\"strategic_naive\")\n",
    "\n",
    "# strategic classification\n",
    "print(\"---------- training strategically----------\")\n",
    "strategic_model_man = MyStrategicModel(x_dim, h_dim, funcs, TRAIN_SLOPE, EVAL_SLOPE, strategic=True, manifold=True)\n",
    "\n",
    "strategic_model_man.fit(PATH, X, B_SPANS, Y, Xval, B_SPANSval, Yval,\n",
    "                                opt=torch.optim.Adam, opt_kwargs={\"lr\": 5*(1e-1)},\n",
    "                                batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=True,\n",
    "                                comment=\"strategic_man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:163: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\problems\\problem.py:1055: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    }
   ],
   "source": [
    "non_strategic_model = MyStrategicModel(x_dim, funcs, TRAIN_SLOPE, EVAL_SLOPE, strategic=False)\n",
    "non_strategic_model.load_model(PATH + \"/non_strategic/model.pt\")\n",
    "\n",
    "strategic_model_naive = MyStrategicModel(x_dim, funcs, TRAIN_SLOPE, EVAL_SLOPE, strategic=True, manifold=False)\n",
    "strategic_model_naive.load_model(PATH + \"/strategic_naive/model.pt\")\n",
    "\n",
    "strategic_model_man = MyStrategicModel(x_dim, funcs, TRAIN_SLOPE, EVAL_SLOPE, strategic=True, manifold=True)\n",
    "strategic_model_man.load_model(PATH + \"/strategic_man/model.pt\")\n",
    "\n",
    "# calculate results\n",
    "accuracies = np.zeros(4)\n",
    "\n",
    "# non strategic model & non strategic data\n",
    "accuracies[0] = (non_strategic_model.evaluate(Xtest, B_SPANStest, Ytest))\n",
    "\n",
    "# naive strategic model & strategic data\n",
    "Xtest_opt = strategic_model_naive.optimize_X(Xtest, B_SPANStest)\n",
    "test_scores = strategic_model_naive.score(Xtest_opt)\n",
    "accuracies[1] = (strategic_model_naive.calc_accuracy(Ytest, test_scores))\n",
    "\n",
    "# manifold strategic model & strategic data\n",
    "Xtest_opt = strategic_model_man.optimize_X(Xtest, B_SPANStest)\n",
    "test_scores = strategic_model_man.score(Xtest_opt)\n",
    "accuracies[2] = (strategic_model_man.calc_accuracy(Ytest, test_scores))\n",
    "\n",
    "# non strategic model & strategic data\n",
    "Xtest_opt = non_strategic_model.optimize_X(Xtest, B_SPANStest)\n",
    "accuracies[3] = (non_strategic_model.evaluate(Xtest_opt, B_SPANStest, Ytest))\n",
    "\n",
    "pd.DataFrame(accuracies).to_csv(PATH + '/results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "funcPred",
   "language": "python",
   "name": "funcpred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
