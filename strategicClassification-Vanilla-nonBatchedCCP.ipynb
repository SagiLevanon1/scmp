{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import cvxpy as cp\n",
    "import dccp\n",
    "import torch\n",
    "import numpy as np\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import zero_one_loss, confusion_matrix\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.patches as mpatches\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import os, psutil\n",
    "from datetime import datetime\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "TRAIN_SLOPE = 2\n",
    "EVAL_SLOPE = 5\n",
    "X_LOWER_BOUND = -10\n",
    "X_UPPER_BOUND = 10\n",
    "SEED = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, Y, percentage):\n",
    "    num_val = int(len(X)*percentage)\n",
    "    return X[num_val:], Y[num_val:], X[:num_val], Y[:num_val]\n",
    "\n",
    "def shuffle(X, Y):\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    data = torch.cat((Y, X), 1)\n",
    "    data = data[torch.randperm(data.size()[0])]\n",
    "    X = data[:, 1:]\n",
    "    Y = data[:, 0]\n",
    "    return X, Y\n",
    "\n",
    "def conf_mat(Y1, Y2):\n",
    "    num_of_samples = len(Y1)\n",
    "    mat = confusion_matrix(Y1, Y2, labels=[-1, 1])*100/num_of_samples\n",
    "    acc = np.trace(mat)\n",
    "    return mat, acc\n",
    "\n",
    "def calc_accuracy(Y, Ypred):\n",
    "    num = len(Y)\n",
    "    temp = Y - Ypred\n",
    "    acc = len(temp[temp == 0])*1./num\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCP classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCP:\n",
    "    def __init__(self, x_dim, funcs, scale):\n",
    "        self.f_derivative = funcs[\"f_derivative\"]\n",
    "        self.g = funcs[\"g\"]\n",
    "        self.c = funcs[\"c\"]\n",
    "        \n",
    "        self.x = cp.Variable(x_dim)\n",
    "        self.xt = cp.Parameter(x_dim)\n",
    "        self.r = cp.Parameter(x_dim)\n",
    "        self.w = cp.Parameter(x_dim)\n",
    "        self.b = cp.Parameter(1)\n",
    "        self.slope = cp.Parameter(1)\n",
    "\n",
    "        target = self.x@self.f_derivative(self.xt, self.w, self.b, self.slope)-self.g(self.x, self.w, self.b, self.slope)-self.c(self.x, self.r, x_dim, scale)\n",
    "        constraints = [self.x >= X_LOWER_BOUND,\n",
    "                       self.x <= X_UPPER_BOUND]\n",
    "        self.prob = cp.Problem(cp.Maximize(target), constraints)\n",
    "        \n",
    "    def ccp(self, r):\n",
    "        \"\"\"\n",
    "        numpy to numpy\n",
    "        \"\"\"\n",
    "        self.xt.value = r\n",
    "        self.r.value = r\n",
    "        result = self.prob.solve()\n",
    "        diff = np.linalg.norm(self.xt.value - self.x.value)\n",
    "        cnt = 0\n",
    "        while diff > 0.001 and cnt < 100:\n",
    "            cnt += 1\n",
    "            self.xt.value = self.x.value\n",
    "            result = self.prob.solve()\n",
    "            diff = np.linalg.norm(self.x.value - self.xt.value)\n",
    "        return self.x.value\n",
    "    \n",
    "    def optimize_X(self, X, w, b, slope):\n",
    "        \"\"\"\n",
    "        tensor to tensor\n",
    "        \"\"\"\n",
    "        w = w.detach().numpy()\n",
    "        b = b.detach().numpy()\n",
    "        slope = np.full(1, slope)\n",
    "        X = X.numpy()\n",
    "        \n",
    "        self.w.value = w\n",
    "        self.b.value = b\n",
    "        self.slope.value = slope\n",
    "        \n",
    "        return torch.stack([torch.from_numpy(self.ccp(x)) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DELTA():\n",
    "    \n",
    "    def __init__(self, x_dim, funcs, scale):\n",
    "        self.g = funcs[\"g\"]\n",
    "        self.c = funcs[\"c\"]\n",
    "        \n",
    "        self.x = cp.Variable(x_dim)\n",
    "        self.r = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.w = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.b = cp.Parameter(1, value = np.random.randn(1))\n",
    "        self.f_der = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "\n",
    "        target = self.x@self.f_der-self.g(self.x, self.w, self.b, TRAIN_SLOPE)-self.c(self.x, self.r, x_dim, scale)\n",
    "        constraints = [self.x >= X_LOWER_BOUND,\n",
    "                       self.x <= X_UPPER_BOUND]\n",
    "        objective = cp.Maximize(target)\n",
    "        problem = cp.Problem(objective, constraints)\n",
    "        self.layer = CvxpyLayer(problem, parameters=[self.r, self.w, self.b, self.f_der],\n",
    "                                variables=[self.x])\n",
    "        \n",
    "    def optimize_X(self, X, w, b, F_DER):\n",
    "        return self.layer(X, w, b, F_DER)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gain & Cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(x, w, b):\n",
    "    return x@w + b\n",
    "\n",
    "def f(x, w, b, slope):\n",
    "    return 0.5*cp.norm(cp.hstack([1, (slope*score(x, w, b) + 1)]), 2)\n",
    "\n",
    "def g(x, w, b, slope):\n",
    "    return 0.5*cp.norm(cp.hstack([1, (slope*score(x, w, b) - 1)]), 2)\n",
    "\n",
    "def c(x, r, x_dim, scale):\n",
    "    return (scale)*cp.sum_squares(x-r)\n",
    "\n",
    "def f_derivative(x, w, b, slope):\n",
    "    return 0.5*cp.multiply(slope*((slope*score(x, w, b) + 1)/cp.sqrt((slope*score(x, w, b) + 1)**2 + 1)), w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyStrategicModel(torch.nn.Module):\n",
    "    def __init__(self, x_dim, funcs, train_slope, eval_slope, scale, strategic=False):\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "\n",
    "        super(MyStrategicModel, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.train_slope, self.eval_slope = train_slope, eval_slope\n",
    "        self.w = torch.nn.parameter.Parameter(math.sqrt(1/x_dim)*(1-2*torch.rand(x_dim, dtype=torch.float64, requires_grad=True)))\n",
    "        self.b = torch.nn.parameter.Parameter(math.sqrt(1/x_dim)*(1-2*torch.rand(1, dtype=torch.float64, requires_grad=True)))\n",
    "#         self.w = torch.nn.parameter.Parameter(torch.zeros(x_dim, dtype=torch.float64, requires_grad=True))\n",
    "#         self.b = torch.nn.parameter.Parameter(torch.zeros(1, dtype=torch.float64, requires_grad=True))\n",
    "        self.strategic = strategic\n",
    "        self.ccp = CCP(x_dim, funcs, scale)\n",
    "        self.delta = DELTA(x_dim, funcs, scale)\n",
    "        self.ccp_time = 0\n",
    "\n",
    "    def forward(self, X, evaluation=False):\n",
    "        if self.strategic:\n",
    "            if evaluation:\n",
    "                t1 = time.time()\n",
    "                XT = self.ccp.optimize_X(X, self.w, self.b, self.eval_slope)\n",
    "                self.ccp_time += time.time()-t1\n",
    "                X_opt = XT\n",
    "            else:\n",
    "                t1 = time.time()\n",
    "                XT = self.ccp.optimize_X(X, self.w, self.b, self.train_slope)\n",
    "                self.ccp_time += time.time()-t1\n",
    "                F_DER = self.get_f_ders(XT, self.train_slope)\n",
    "                X_opt = self.delta.optimize_X(X, self.w, self.b, F_DER) # Xopt should be equal to XT but we do it again for the gradients\n",
    "            output = self.score(X_opt)\n",
    "        else:\n",
    "            output = self.score(X)        \n",
    "        return output\n",
    "    \n",
    "    def optimize_X(self, X, evaluation=False):\n",
    "        slope = self.eval_slope if evaluation else self.train_slope\n",
    "        return self.ccp.optimize_X(X, self.w, self.b, slope)\n",
    "    \n",
    "    def normalize_weights(self):\n",
    "        with torch.no_grad():\n",
    "            norm = torch.sqrt(torch.sum(self.w**2) + self.b**2)\n",
    "            self.w /= norm\n",
    "            self.b /= norm\n",
    "\n",
    "    def score(self, x):\n",
    "        return x@self.w + self.b\n",
    "    \n",
    "    def get_f_ders(self, XT, slope):\n",
    "        return torch.stack([0.5*slope*((slope*self.score(xt) + 1)/torch.sqrt((slope*self.score(xt) + 1)**2 + 1))*self.w for xt in XT])\n",
    "\n",
    "    def calc_accuracy(self, Y, Y_pred):\n",
    "        Y_pred = torch.sign(Y_pred)\n",
    "        num = len(Y)\n",
    "        temp = Y - Y_pred\n",
    "        acc = len(temp[temp == 0])*1./num        \n",
    "        return acc\n",
    "    \n",
    "    def evaluate(self, X, Y):      \n",
    "        return self.calc_accuracy(Y, self.forward(X, evaluation=True))\n",
    "    \n",
    "    def loss(self, Y, Y_pred):\n",
    "        return torch.mean(torch.clamp(1 - Y_pred * Y, min=0))\n",
    "    \n",
    "    def save_model(self, train_errors, val_errors, train_losses, val_losses, info, path, comment=None):\n",
    "        if comment is not None:\n",
    "            path += \"/\" + comment\n",
    "            \n",
    "        filename = path + \"/model.pt\"\n",
    "        if not os.path.exists(os.path.dirname(filename)):\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        torch.save(self.state_dict(), filename)\n",
    "                \n",
    "        pd.DataFrame(np.array(train_errors)).to_csv(path + '/train_errors.csv')\n",
    "        pd.DataFrame(np.array(val_errors)).to_csv(path + '/val_errors.csv')\n",
    "        pd.DataFrame(np.array(train_losses)).to_csv(path + '/train_losses.csv')\n",
    "        pd.DataFrame(np.array(val_losses)).to_csv(path + '/val_losses.csv')\n",
    "        \n",
    "        with open(path + \"/info.txt\", \"w\") as f:\n",
    "            f.write(info)\n",
    "    \n",
    "    def load_model(self, filename):\n",
    "        self.load_state_dict(torch.load(filename))\n",
    "        self.eval()\n",
    "    \n",
    "    def fit(self, path, X, Y, Xval, Yval, opt, opt_kwargs={\"lr\":1e-3}, batch_size=128, epochs=100, verbose=False, callback=None, comment=None):\n",
    "        train_dset = TensorDataset(X, Y)\n",
    "        train_loader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "        opt = opt(self.parameters(), **opt_kwargs)\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_errors = []\n",
    "        val_errors = []\n",
    "        \n",
    "        best_val_error = 1\n",
    "        consecutive_no_improvement = 0\n",
    "\n",
    "        total_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            t1 = time.time()\n",
    "            batch = 1\n",
    "            train_losses.append([])\n",
    "            train_errors.append([])\n",
    "            for Xbatch, Ybatch in train_loader:\n",
    "#                 try:\n",
    "                opt.zero_grad()\n",
    "                Ybatch_pred = self.forward(Xbatch)\n",
    "                l = self.loss(Ybatch, Ybatch_pred)\n",
    "                l.backward()\n",
    "                opt.step()\n",
    "                train_losses[-1].append(l.item())\n",
    "                with torch.no_grad():\n",
    "                    e = self.calc_accuracy(Ybatch, Ybatch_pred)\n",
    "                    train_errors[-1].append(1-e)\n",
    "                if verbose:\n",
    "                    print(\"batch %03d / %03d | loss: %3.5f | err: %3.5f\" %\n",
    "                          (batch, len(train_loader), np.mean(train_losses[-1]), np.mean(train_errors[-1])))\n",
    "                batch += 1\n",
    "                if callback is not None:\n",
    "                    callback()\n",
    "#                 except:\n",
    "#                     print(\"failed\")\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    Yval_pred = self.forward(Xval, evaluation=True)\n",
    "                    val_loss = self.loss(Yval, Yval_pred).item()\n",
    "                    val_losses.append(val_loss)\n",
    "                    val_error = 1-self.calc_accuracy(Yval, Yval_pred)\n",
    "                    val_errors.append(val_error)\n",
    "                    if val_error < best_val_error:\n",
    "                        consecutive_no_improvement = 0\n",
    "                        best_val_error = val_error\n",
    "                        info = \"training time in seconds: {}\\nepoch: {}\\nbatch size: {}\\ntrain slope: {}\\neval slope: {}\\nlearning rate: {}\\nvalidation loss: {}\\nvalidation error: {}\\n\".format(\n",
    "                        time.time()-total_time, epoch, batch_size, self.train_slope, self.eval_slope, opt_kwargs[\"lr\"], val_loss, val_error)\n",
    "                        self.save_model(train_errors, val_errors, train_losses, val_losses, info, path, comment)\n",
    "                        print(\"model saved!\")\n",
    "\n",
    "                    else:\n",
    "                        consecutive_no_improvement += 1\n",
    "                        if consecutive_no_improvement >= 4:\n",
    "                            break\n",
    "                except:\n",
    "                    print(\"failed\")\n",
    "                    \n",
    "            t2 = time.time()\n",
    "            if verbose:\n",
    "                print(\"------------- epoch %03d / %03d | time: %03d sec | loss: %3.5f | err: %3.5f\" % (epoch + 1, epochs, t2-t1, val_losses[-1], val_errors[-1]))\n",
    "        print(\"training time: {} seconds\".format(time.time()-total_time)) \n",
    "        return train_errors, val_errors, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sklearn_data(x_dim, N, informative_frac=1, shift_range=1, scale_range=1, noise_frac=0.01):\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    n_informative = int(informative_frac*x_dim)\n",
    "    n_redundant = x_dim - n_informative\n",
    "    shift_arr = shift_range*np.random.randn(x_dim)\n",
    "    scale_arr = scale_range*np.random.randn(x_dim)\n",
    "    X, Y = make_classification(n_samples=N, n_features=x_dim, n_informative=n_informative, n_redundant=n_redundant,\n",
    "                               flip_y=noise_frac, shift=shift_arr, scale=scale_arr, random_state=0)\n",
    "    Y[Y == 0] = -1\n",
    "    X -= np.mean(X, axis=0)\n",
    "    X /= np.std(X, axis=0)\n",
    "    return torch.from_numpy(X), torch.from_numpy(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 5])\n",
      "percent of positive samples: 50.5%\n",
      "batch 001 / 032 | loss: 0.68683 | err: 0.32000\n",
      "batch 002 / 032 | loss: 0.58017 | err: 0.22000\n",
      "batch 003 / 032 | loss: 0.57247 | err: 0.22667\n",
      "batch 004 / 032 | loss: 0.56509 | err: 0.20000\n",
      "batch 005 / 032 | loss: 0.54002 | err: 0.20000\n",
      "batch 006 / 032 | loss: 0.54862 | err: 0.20667\n",
      "batch 007 / 032 | loss: 0.52235 | err: 0.19429\n",
      "batch 008 / 032 | loss: 0.52171 | err: 0.20000\n",
      "batch 009 / 032 | loss: 0.48656 | err: 0.19111\n",
      "batch 010 / 032 | loss: 0.47813 | err: 0.19200\n",
      "batch 011 / 032 | loss: 0.47424 | err: 0.19273\n",
      "batch 012 / 032 | loss: 0.45730 | err: 0.18667\n",
      "batch 013 / 032 | loss: 0.44732 | err: 0.17538\n",
      "batch 014 / 032 | loss: 0.42668 | err: 0.16571\n",
      "batch 015 / 032 | loss: 0.42845 | err: 0.16800\n",
      "batch 016 / 032 | loss: 0.41320 | err: 0.16000\n",
      "batch 017 / 032 | loss: 0.41266 | err: 0.16235\n",
      "batch 018 / 032 | loss: 0.40357 | err: 0.15778\n",
      "batch 019 / 032 | loss: 0.39325 | err: 0.15158\n",
      "batch 020 / 032 | loss: 0.38556 | err: 0.15000\n",
      "batch 021 / 032 | loss: 0.38006 | err: 0.14667\n",
      "batch 022 / 032 | loss: 0.37856 | err: 0.14727\n",
      "batch 023 / 032 | loss: 0.36959 | err: 0.14435\n",
      "batch 024 / 032 | loss: 0.36196 | err: 0.14167\n",
      "batch 025 / 032 | loss: 0.35468 | err: 0.13920\n",
      "batch 026 / 032 | loss: 0.34305 | err: 0.13385\n",
      "batch 027 / 032 | loss: 0.34005 | err: 0.13185\n",
      "batch 028 / 032 | loss: 0.34554 | err: 0.13429\n",
      "batch 029 / 032 | loss: 0.35146 | err: 0.13655\n",
      "batch 030 / 032 | loss: 0.34444 | err: 0.13333\n",
      "batch 031 / 032 | loss: 0.33645 | err: 0.12903\n",
      "batch 032 / 032 | loss: 0.33444 | err: 0.12875\n",
      "model saved!\n",
      "------------- epoch 001 / 005 | time: 000 sec | loss: 0.27503 | err: 0.10000\n",
      "batch 001 / 032 | loss: 0.29207 | err: 0.12000\n",
      "batch 002 / 032 | loss: 0.31215 | err: 0.12000\n",
      "batch 003 / 032 | loss: 0.31634 | err: 0.12000\n",
      "batch 004 / 032 | loss: 0.28728 | err: 0.10000\n",
      "batch 005 / 032 | loss: 0.28356 | err: 0.09600\n",
      "batch 006 / 032 | loss: 0.28057 | err: 0.10000\n",
      "batch 007 / 032 | loss: 0.32116 | err: 0.11429\n",
      "batch 008 / 032 | loss: 0.30178 | err: 0.10500\n",
      "batch 009 / 032 | loss: 0.30634 | err: 0.11556\n",
      "batch 010 / 032 | loss: 0.29842 | err: 0.11200\n",
      "batch 011 / 032 | loss: 0.30617 | err: 0.11273\n",
      "batch 012 / 032 | loss: 0.29651 | err: 0.10667\n",
      "batch 013 / 032 | loss: 0.28322 | err: 0.10154\n",
      "batch 014 / 032 | loss: 0.27292 | err: 0.10000\n",
      "batch 015 / 032 | loss: 0.27182 | err: 0.10133\n",
      "batch 016 / 032 | loss: 0.27014 | err: 0.10500\n",
      "batch 017 / 032 | loss: 0.26940 | err: 0.10588\n",
      "batch 018 / 032 | loss: 0.26160 | err: 0.10222\n",
      "batch 019 / 032 | loss: 0.25437 | err: 0.09895\n",
      "batch 020 / 032 | loss: 0.25508 | err: 0.09800\n",
      "batch 021 / 032 | loss: 0.26161 | err: 0.10095\n",
      "batch 022 / 032 | loss: 0.25850 | err: 0.10000\n",
      "batch 023 / 032 | loss: 0.25132 | err: 0.09565\n",
      "batch 024 / 032 | loss: 0.24714 | err: 0.09500\n",
      "batch 025 / 032 | loss: 0.24397 | err: 0.09440\n",
      "batch 026 / 032 | loss: 0.24595 | err: 0.09692\n",
      "batch 027 / 032 | loss: 0.23967 | err: 0.09481\n",
      "batch 028 / 032 | loss: 0.23706 | err: 0.09429\n",
      "batch 029 / 032 | loss: 0.23291 | err: 0.09241\n",
      "batch 030 / 032 | loss: 0.23099 | err: 0.09067\n",
      "batch 031 / 032 | loss: 0.23430 | err: 0.09032\n",
      "batch 032 / 032 | loss: 0.23448 | err: 0.09125\n",
      "------------- epoch 002 / 005 | time: 000 sec | loss: 0.26855 | err: 0.10000\n",
      "batch 001 / 032 | loss: 0.21752 | err: 0.08000\n",
      "batch 002 / 032 | loss: 0.20085 | err: 0.08000\n",
      "batch 003 / 032 | loss: 0.17865 | err: 0.06667\n",
      "batch 004 / 032 | loss: 0.16648 | err: 0.06000\n",
      "batch 005 / 032 | loss: 0.15677 | err: 0.05600\n",
      "batch 006 / 032 | loss: 0.15683 | err: 0.06000\n",
      "batch 007 / 032 | loss: 0.21673 | err: 0.08571\n",
      "batch 008 / 032 | loss: 0.19723 | err: 0.08000\n",
      "batch 009 / 032 | loss: 0.20353 | err: 0.08000\n",
      "batch 010 / 032 | loss: 0.19793 | err: 0.07600\n",
      "batch 011 / 032 | loss: 0.19805 | err: 0.08000\n",
      "batch 012 / 032 | loss: 0.19516 | err: 0.08000\n",
      "batch 013 / 032 | loss: 0.18622 | err: 0.07692\n",
      "batch 014 / 032 | loss: 0.18024 | err: 0.07429\n",
      "batch 015 / 032 | loss: 0.20203 | err: 0.08267\n",
      "batch 016 / 032 | loss: 0.21221 | err: 0.09000\n",
      "batch 017 / 032 | loss: 0.20750 | err: 0.08706\n",
      "batch 018 / 032 | loss: 0.20637 | err: 0.08444\n",
      "batch 019 / 032 | loss: 0.20072 | err: 0.08421\n",
      "batch 020 / 032 | loss: 0.20941 | err: 0.08800\n",
      "batch 021 / 032 | loss: 0.21045 | err: 0.08952\n",
      "batch 022 / 032 | loss: 0.20990 | err: 0.08909\n",
      "batch 023 / 032 | loss: 0.20874 | err: 0.08696\n",
      "batch 024 / 032 | loss: 0.21435 | err: 0.08833\n",
      "batch 025 / 032 | loss: 0.21943 | err: 0.08960\n",
      "batch 026 / 032 | loss: 0.21991 | err: 0.08923\n",
      "batch 027 / 032 | loss: 0.21395 | err: 0.08741\n",
      "batch 028 / 032 | loss: 0.23329 | err: 0.09429\n",
      "batch 029 / 032 | loss: 0.22983 | err: 0.09241\n",
      "batch 030 / 032 | loss: 0.22875 | err: 0.09200\n",
      "batch 031 / 032 | loss: 0.23701 | err: 0.09677\n",
      "batch 032 / 032 | loss: 0.23201 | err: 0.09375\n",
      "------------- epoch 003 / 005 | time: 000 sec | loss: 0.24464 | err: 0.10000\n",
      "batch 001 / 032 | loss: 0.16577 | err: 0.04000\n",
      "batch 002 / 032 | loss: 0.17999 | err: 0.06000\n",
      "batch 003 / 032 | loss: 0.15971 | err: 0.05333\n",
      "batch 004 / 032 | loss: 0.15745 | err: 0.05000\n",
      "batch 005 / 032 | loss: 0.17184 | err: 0.05600\n",
      "batch 006 / 032 | loss: 0.16290 | err: 0.05333\n",
      "batch 007 / 032 | loss: 0.17078 | err: 0.05714\n",
      "batch 008 / 032 | loss: 0.16815 | err: 0.05500\n",
      "batch 009 / 032 | loss: 0.17521 | err: 0.05778\n",
      "batch 010 / 032 | loss: 0.18563 | err: 0.06800\n",
      "batch 011 / 032 | loss: 0.22787 | err: 0.08364\n",
      "batch 012 / 032 | loss: 0.21715 | err: 0.08000\n",
      "batch 013 / 032 | loss: 0.21776 | err: 0.08000\n",
      "batch 014 / 032 | loss: 0.22171 | err: 0.07714\n",
      "batch 015 / 032 | loss: 0.23048 | err: 0.08267\n",
      "batch 016 / 032 | loss: 0.22116 | err: 0.07750\n",
      "batch 017 / 032 | loss: 0.21299 | err: 0.07529\n",
      "batch 018 / 032 | loss: 0.20973 | err: 0.07556\n",
      "batch 019 / 032 | loss: 0.22274 | err: 0.08211\n",
      "batch 020 / 032 | loss: 0.23181 | err: 0.08400\n",
      "batch 021 / 032 | loss: 0.22711 | err: 0.08190\n",
      "batch 022 / 032 | loss: 0.22992 | err: 0.08364\n",
      "batch 023 / 032 | loss: 0.23008 | err: 0.08348\n",
      "batch 024 / 032 | loss: 0.22540 | err: 0.08333\n",
      "batch 025 / 032 | loss: 0.22074 | err: 0.08160\n",
      "batch 026 / 032 | loss: 0.22102 | err: 0.08308\n",
      "batch 027 / 032 | loss: 0.22358 | err: 0.08444\n",
      "batch 028 / 032 | loss: 0.22611 | err: 0.08571\n",
      "batch 029 / 032 | loss: 0.22793 | err: 0.08552\n",
      "batch 030 / 032 | loss: 0.23223 | err: 0.08667\n",
      "batch 031 / 032 | loss: 0.23451 | err: 0.08903\n",
      "batch 032 / 032 | loss: 0.23695 | err: 0.09125\n",
      "model saved!\n",
      "------------- epoch 004 / 005 | time: 000 sec | loss: 0.25825 | err: 0.09000\n",
      "batch 001 / 032 | loss: 0.20472 | err: 0.12000\n",
      "batch 002 / 032 | loss: 0.20747 | err: 0.10000\n",
      "batch 003 / 032 | loss: 0.18436 | err: 0.09333\n",
      "batch 004 / 032 | loss: 0.18244 | err: 0.08000\n",
      "batch 005 / 032 | loss: 0.18949 | err: 0.07200\n",
      "batch 006 / 032 | loss: 0.20822 | err: 0.08000\n",
      "batch 007 / 032 | loss: 0.23619 | err: 0.08000\n",
      "batch 008 / 032 | loss: 0.24336 | err: 0.08500\n",
      "batch 009 / 032 | loss: 0.23729 | err: 0.08444\n",
      "batch 010 / 032 | loss: 0.22398 | err: 0.08000\n",
      "batch 011 / 032 | loss: 0.22799 | err: 0.08000\n",
      "batch 012 / 032 | loss: 0.23118 | err: 0.08000\n",
      "batch 013 / 032 | loss: 0.23030 | err: 0.08000\n",
      "batch 014 / 032 | loss: 0.23769 | err: 0.08857\n",
      "batch 015 / 032 | loss: 0.22371 | err: 0.08267\n",
      "batch 016 / 032 | loss: 0.23226 | err: 0.08500\n",
      "batch 017 / 032 | loss: 0.22832 | err: 0.08471\n",
      "batch 018 / 032 | loss: 0.23293 | err: 0.08667\n",
      "batch 019 / 032 | loss: 0.23442 | err: 0.08842\n",
      "batch 020 / 032 | loss: 0.22829 | err: 0.08600\n",
      "batch 021 / 032 | loss: 0.22552 | err: 0.08571\n",
      "batch 022 / 032 | loss: 0.21958 | err: 0.08364\n",
      "batch 023 / 032 | loss: 0.21573 | err: 0.08174\n",
      "batch 024 / 032 | loss: 0.21740 | err: 0.08167\n",
      "batch 025 / 032 | loss: 0.21822 | err: 0.08160\n",
      "batch 026 / 032 | loss: 0.21404 | err: 0.08000\n",
      "batch 027 / 032 | loss: 0.21011 | err: 0.07852\n",
      "batch 028 / 032 | loss: 0.21341 | err: 0.07857\n",
      "batch 029 / 032 | loss: 0.21720 | err: 0.08000\n",
      "batch 030 / 032 | loss: 0.22132 | err: 0.08133\n",
      "batch 031 / 032 | loss: 0.23399 | err: 0.08516\n",
      "batch 032 / 032 | loss: 0.23090 | err: 0.08375\n",
      "------------- epoch 005 / 005 | time: 000 sec | loss: 0.27161 | err: 0.09500\n",
      "training time: 0.35079264640808105 seconds\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/sagil/Desktop/nir_project/models/runtime\"\n",
    "epochs = 5\n",
    "batch_size = 25\n",
    "x_dim = 5\n",
    "scale = 1\n",
    "X, Y = gen_sklearn_data(x_dim, 1000) #gen_custom_normal_data(x_dim, 422, torch.full((x_dim,), 0.5), torch.full((x_dim,), 0.5), torch.full((x_dim,), -0.5), torch.full((x_dim,), 0.5)) #gen_sklearn_data(x_dim, 422, scale_range=3)\n",
    "X, Y, Xval, Yval = split_data(X, Y, 0.2)\n",
    "print(Xval.size())\n",
    "print(\"percent of positive samples: {}%\".format(100 * len(Y[Y == 1]) / len(Y)))\n",
    "\n",
    "funcs = {\"f\": f, \"g\": g, \"f_derivative\": f_derivative, \"c\": c, \"score\": score}\n",
    "\n",
    "\n",
    "strategic_model = MyStrategicModel(x_dim, funcs, TRAIN_SLOPE, EVAL_SLOPE, scale=scale, strategic=False)\n",
    "strategic_model.fit(path, X, Y, Xval, Yval,\n",
    "                    opt=torch.optim.Adam, opt_kwargs={\"lr\": (1e-1)},\n",
    "                    batch_size=batch_size, epochs=epochs, verbose=True, \n",
    "                   comment=\"non_batched\") \n",
    "\n",
    "print(strategic_model.ccp_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "funcPred",
   "language": "python",
   "name": "funcpred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
