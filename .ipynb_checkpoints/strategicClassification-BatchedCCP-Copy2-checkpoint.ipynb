{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import cvxpy as cp\n",
    "import dccp\n",
    "import torch\n",
    "import numpy as np\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import zero_one_loss, confusion_matrix\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.patches as mpatches\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import os, psutil\n",
    "from datetime import datetime\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "TRAIN_SLOPE = 2\n",
    "EVAL_SLOPE = 5\n",
    "X_LOWER_BOUND = -10\n",
    "X_UPPER_BOUND = 10\n",
    "SEED = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, Y, percentage):\n",
    "    num_val = int(len(X)*percentage)\n",
    "    return X[num_val:], Y[num_val:], X[:num_val], Y[:num_val]\n",
    "\n",
    "def shuffle(X, Y):\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    data = torch.cat((Y, X), 1)\n",
    "    data = data[torch.randperm(data.size()[0])]\n",
    "    X = data[:, 1:]\n",
    "    Y = data[:, 0]\n",
    "    return X, Y\n",
    "\n",
    "def conf_mat(Y1, Y2):\n",
    "    num_of_samples = len(Y1)\n",
    "    mat = confusion_matrix(Y1, Y2, labels=[-1, 1])*100/num_of_samples\n",
    "    acc = np.trace(mat)\n",
    "    return mat, acc\n",
    "\n",
    "def calc_accuracy(Y, Ypred):\n",
    "    num = len(Y)\n",
    "    temp = Y - Ypred\n",
    "    acc = len(temp[temp == 0])*1./num\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCP classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCP:\n",
    "    def __init__(self, x_dim, batch_size, funcs, scale):\n",
    "        self.f_derivative = funcs[\"f_derivative\"]\n",
    "        self.g = funcs[\"g\"]\n",
    "        self.c = funcs[\"c\"]\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.x = cp.Variable((batch_size, x_dim))\n",
    "        self.xt = cp.Parameter((batch_size, x_dim))\n",
    "        self.r = cp.Parameter((batch_size, x_dim))\n",
    "        self.w = cp.Parameter(x_dim)\n",
    "        self.b = cp.Parameter(1)\n",
    "        self.slope = cp.Parameter(1)\n",
    "\n",
    "        target = cp.diag(self.x@(self.f_derivative(self.xt, self.w, self.b, self.slope).T))-self.g(self.x, self.w, self.b, self.slope)-self.c(self.x, self.r, x_dim, scale)\n",
    "        constraints = [self.x >= X_LOWER_BOUND,\n",
    "                       self.x <= X_UPPER_BOUND]\n",
    "        self.prob = cp.Problem(cp.Maximize(cp.sum(target)), constraints)\n",
    "        \n",
    "    def ccp(self, r):\n",
    "        \"\"\"\n",
    "        numpy to numpy\n",
    "        \"\"\"\n",
    "        self.xt.value = r\n",
    "        self.r.value = r\n",
    "        result = self.prob.solve()\n",
    "        diff = np.linalg.norm(self.xt.value - self.x.value)\n",
    "        cnt = 0\n",
    "        while diff > 0.001 and cnt < 100:\n",
    "            cnt += 1\n",
    "            self.xt.value = self.x.value\n",
    "            result = self.prob.solve()\n",
    "            diff = np.linalg.norm(self.x.value - self.xt.value)/self.batch_size\n",
    "        return self.x.value\n",
    "    \n",
    "    def optimize_X(self, X, w, b, slope):\n",
    "        \"\"\"\n",
    "        tensor to tensor\n",
    "        \"\"\"\n",
    "        w = w.detach().numpy()\n",
    "        b = b.detach().numpy()\n",
    "        slope = np.full(1, slope)\n",
    "        X = X.numpy()\n",
    "        \n",
    "        self.w.value = w\n",
    "        self.b.value = b\n",
    "        self.slope.value = slope\n",
    "        return torch.from_numpy(self.ccp(X))\n",
    "        # return torch.stack([torch.from_numpy(self.ccp(x)) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DELTA():\n",
    "    \n",
    "    def __init__(self, x_dim, funcs, scale):\n",
    "        self.g = funcs[\"g\"]\n",
    "        self.c = funcs[\"c\"]\n",
    "        \n",
    "        self.x = cp.Variable(x_dim)\n",
    "        self.r = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.w = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.b = cp.Parameter(1, value = np.random.randn(1))\n",
    "        self.f_der = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "\n",
    "        target = self.x@self.f_der-self.g(self.x, self.w, self.b, TRAIN_SLOPE)-self.c(self.x, self.r, x_dim, scale)\n",
    "        constraints = [self.x >= X_LOWER_BOUND,\n",
    "                       self.x <= X_UPPER_BOUND]\n",
    "        objective = cp.Maximize(target)\n",
    "        problem = cp.Problem(objective, constraints)\n",
    "        self.layer = CvxpyLayer(problem, parameters=[self.r, self.w, self.b, self.f_der],\n",
    "                                variables=[self.x])\n",
    "        \n",
    "    def optimize_X(self, X, w, b, F_DER):\n",
    "        return self.layer(X, w, b, F_DER)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gain & Cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(x, w, b):\n",
    "    return x@w + b\n",
    "\n",
    "def f(x, w, b, slope):\n",
    "    return 0.5*cp.norm(cp.hstack([1, (slope*score(x, w, b) + 1)]), 2)\n",
    "\n",
    "def g(x, w, b, slope):\n",
    "    return 0.5*cp.norm(cp.hstack([1, (slope*score(x, w, b) - 1)]), 2)\n",
    "\n",
    "def c(x, r, x_dim, scale):\n",
    "    return (scale)*cp.sum_squares(x-r)\n",
    "\n",
    "def f_derivative(x, w, b, slope):\n",
    "    return 0.5*cp.multiply(slope*((slope*score(x, w, b) + 1)/cp.sqrt((slope*score(x, w, b) + 1)**2 + 1)), w)\n",
    "    \n",
    "def f_batch(x, w, b, slope):\n",
    "    return 0.5*cp.norm(cp.vstack([np.ones(x.shape[0]), (slope*score(x, w, b) + 1)]), 2, axis=0)\n",
    "\n",
    "def g_batch(x, w, b, slope):\n",
    "    return 0.5*cp.norm(cp.vstack([np.ones((1, x.shape[0])), cp.reshape((slope*score(x, w, b) - 1), (1, x.shape[0]))]), 2, axis=0)\n",
    "\n",
    "def c_batch(x, r, x_dim, scale):\n",
    "    return (scale)*cp.square(cp.norm(x-r, 2, axis=1))\n",
    "\n",
    "def f_derivative_batch(x, w, b, slope):\n",
    "    nablas = 0.5*slope*((slope*score(x, w, b) + 1)/cp.sqrt((slope*score(x, w, b) + 1)**2 + 1))\n",
    "    return cp.reshape(nablas, (nablas.shape[0], 1))@cp.reshape(w, (1, x.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyStrategicModel(torch.nn.Module):\n",
    "    def __init__(self, x_dim, batch_size, funcs, funcs_batch, train_slope, eval_slope, scale, strategic=False):\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "\n",
    "        super(MyStrategicModel, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.train_slope, self.eval_slope = train_slope, eval_slope\n",
    "        self.w = torch.nn.parameter.Parameter(math.sqrt(1/x_dim)*(1-2*torch.rand(x_dim, dtype=torch.float64, requires_grad=True)))\n",
    "        self.b = torch.nn.parameter.Parameter(math.sqrt(1/x_dim)*(1-2*torch.rand(1, dtype=torch.float64, requires_grad=True)))\n",
    "#         self.w = torch.nn.parameter.Parameter(torch.zeros(x_dim, dtype=torch.float64, requires_grad=True))\n",
    "#         self.b = torch.nn.parameter.Parameter(torch.zeros(1, dtype=torch.float64, requires_grad=True))\n",
    "        self.strategic = strategic\n",
    "        self.ccp = CCP(x_dim, batch_size, funcs_batch, scale)\n",
    "        self.delta = DELTA(x_dim, funcs, scale)\n",
    "        self.ccp_time = 0\n",
    "        self.total_time = 0\n",
    "\n",
    "    def forward(self, X, evaluation=False):\n",
    "        if self.strategic:\n",
    "            if evaluation:\n",
    "                t1 = time.time()\n",
    "                XT = self.ccp.optimize_X(X, self.w, self.b, self.eval_slope)\n",
    "                self.ccp_time += time.time()-t1\n",
    "                X_opt = XT\n",
    "            else:\n",
    "                t1 = time.time()\n",
    "                XT = self.ccp.optimize_X(X, self.w, self.b, self.train_slope)\n",
    "                self.ccp_time += time.time()-t1\n",
    "                F_DER = self.get_f_ders(XT, self.train_slope)\n",
    "                X_opt = self.delta.optimize_X(X, self.w, self.b, F_DER) # Xopt should be equal to XT but we do it again for the gradients\n",
    "            output = self.score(X_opt)\n",
    "        else:\n",
    "            output = self.score(X)        \n",
    "        return output\n",
    "    \n",
    "    def optimize_X(self, X, evaluation=False):\n",
    "        slope = self.eval_slope if evaluation else self.train_slope\n",
    "        return self.ccp.optimize_X(X, self.w, self.b, slope)\n",
    "    \n",
    "    def normalize_weights(self):\n",
    "        with torch.no_grad():\n",
    "            norm = torch.sqrt(torch.sum(self.w**2) + self.b**2)\n",
    "            self.w /= norm\n",
    "            self.b /= norm\n",
    "\n",
    "    def score(self, x):\n",
    "        return x@self.w + self.b\n",
    "    \n",
    "    def get_f_ders(self, XT, slope):\n",
    "        # return torch.stack([0.5*slope*((slope*self.score(xt) + 1)/torch.sqrt((slope*self.score(xt) + 1)**2 + 1))*self.w for xt in XT])\n",
    "        nablas = 0.5*slope*((slope*self.score(XT) + 1)/torch.sqrt((slope*self.score(XT) + 1)**2 + 1))\n",
    "        return torch.reshape(nablas, (len(nablas), 1))@torch.reshape(self.w, (1, len(self.w)))\n",
    "\n",
    "    def calc_accuracy(self, Y, Y_pred):\n",
    "        Y_pred = torch.sign(Y_pred)\n",
    "        num = len(Y)\n",
    "        temp = Y - Y_pred\n",
    "        acc = len(temp[temp == 0])*1./num        \n",
    "        return acc\n",
    "    \n",
    "    def evaluate(self, X, Y):      \n",
    "        return self.calc_accuracy(Y, self.forward(X, evaluation=True))\n",
    "    \n",
    "    def loss(self, Y, Y_pred):\n",
    "        return torch.mean(torch.clamp(1 - Y_pred * Y, min=0))\n",
    "    \n",
    "    def save_model(self, train_errors, val_errors, train_losses, val_losses, info, path, comment=None):\n",
    "        if comment is not None:\n",
    "            path += \"/\" + comment\n",
    "            \n",
    "        filename = path + \"/model.pt\"\n",
    "        if not os.path.exists(os.path.dirname(filename)):\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        torch.save(self.state_dict(), filename)\n",
    "                \n",
    "        pd.DataFrame(np.array(train_errors)).to_csv(path + '/train_errors.csv')\n",
    "        pd.DataFrame(np.array(val_errors)).to_csv(path + '/val_errors.csv')\n",
    "        pd.DataFrame(np.array(train_losses)).to_csv(path + '/train_losses.csv')\n",
    "        pd.DataFrame(np.array(val_losses)).to_csv(path + '/val_losses.csv')\n",
    "        \n",
    "        with open(path + \"/info.txt\", \"w\") as f:\n",
    "            f.write(info)\n",
    "    \n",
    "    def load_model(self, filename):\n",
    "        self.load_state_dict(torch.load(filename))\n",
    "        self.eval()\n",
    "    \n",
    "    def fit(self, path, X, Y, Xval, Yval, opt, opt_kwargs={\"lr\":1e-3}, batch_size=128, epochs=100, verbose=False, callback=None, comment=None):\n",
    "        train_dset = TensorDataset(X, Y)\n",
    "        train_loader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "        test_dset = TensorDataset(Xval, Yval)\n",
    "        test_loader = DataLoader(test_dset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        opt = opt(self.parameters(), **opt_kwargs)\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_errors = []\n",
    "        val_errors = []\n",
    "        \n",
    "        best_val_error = 1\n",
    "        consecutive_no_improvement = 0\n",
    "\n",
    "        total_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            t1 = time.time()\n",
    "            batch = 1\n",
    "            train_losses.append([])\n",
    "            train_errors.append([])\n",
    "            for Xbatch, Ybatch in train_loader:\n",
    "#                 try:\n",
    "                opt.zero_grad()\n",
    "                Ybatch_pred = self.forward(Xbatch)\n",
    "                l = self.loss(Ybatch, Ybatch_pred)\n",
    "                l.backward()\n",
    "                opt.step()\n",
    "                train_losses[-1].append(l.item())\n",
    "                with torch.no_grad():\n",
    "                    e = self.calc_accuracy(Ybatch, Ybatch_pred)\n",
    "                    train_errors[-1].append(1-e)\n",
    "                if verbose:\n",
    "                    print(\"batch %03d / %03d | loss: %3.5f | err: %3.5f\" %\n",
    "                          (batch, len(train_loader), np.mean(train_losses[-1]), np.mean(train_errors[-1])))\n",
    "                batch += 1\n",
    "                if callback is not None:\n",
    "                    callback()\n",
    "#                 except:\n",
    "#                     print(\"failed\")\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                total_loss = 0\n",
    "                total_error = 0\n",
    "                batch = 0\n",
    "                for Xbatch, Ybatch in test_loader:\n",
    "#                     try:\n",
    "                    Yval_pred = self.forward(Xbatch, evaluation=True)\n",
    "                    val_loss = self.loss(Ybatch, Yval_pred).item()\n",
    "                    total_loss += val_loss\n",
    "                    val_error = 1-self.calc_accuracy(Ybatch, Yval_pred)\n",
    "                    total_error += val_error\n",
    "                    batch += 1\n",
    "#                     except:\n",
    "#                         print(\"failed\")\n",
    "                        \n",
    "                avg_loss = total_loss/batch\n",
    "                avg_error = total_error/batch\n",
    "                val_losses.append(avg_loss)\n",
    "                val_errors.append(avg_error)\n",
    "                if avg_error < best_val_error:\n",
    "                        consecutive_no_improvement = 0\n",
    "                        best_val_error = avg_error\n",
    "                        info = \"training time in seconds: {}\\nepoch: {}\\nbatch size: {}\\ntrain slope: {}\\neval slope: {}\\nlearning rate: {}\\nvalidation loss: {}\\nvalidation error: {}\\n\".format(\n",
    "                        time.time()-total_time, epoch, batch_size, self.train_slope, self.eval_slope, opt_kwargs[\"lr\"], avg_loss, avg_error)\n",
    "                        self.save_model(train_errors, val_errors, train_losses, val_losses, info, path, comment)\n",
    "                        print(\"model saved!\")\n",
    "\n",
    "                else:\n",
    "                    consecutive_no_improvement += 1\n",
    "                    if consecutive_no_improvement >= 4:\n",
    "                        break\n",
    "                    \n",
    "            t2 = time.time()\n",
    "            if verbose:\n",
    "                print(\"------------- epoch %03d / %03d | time: %03d sec | loss: %3.5f | err: %3.5f\" % (epoch + 1, epochs, t2-t1, val_losses[-1], val_errors[-1]))\n",
    "        \n",
    "        self.total_time = time.time()-total_time\n",
    "        print(\"training time: {} seconds\".format(self.total_time)) \n",
    "        return train_errors, val_errors, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sklearn_data(x_dim, N, informative_frac=1, shift_range=1, scale_range=1, noise_frac=0.01):\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    n_informative = int(informative_frac*x_dim)\n",
    "    n_redundant = x_dim - n_informative\n",
    "    shift_arr = shift_range*np.random.randn(x_dim)\n",
    "    scale_arr = scale_range*np.random.randn(x_dim)\n",
    "    X, Y = make_classification(n_samples=N, n_features=x_dim, n_informative=n_informative, n_redundant=n_redundant,\n",
    "                               flip_y=noise_frac, shift=shift_arr, scale=scale_arr, random_state=0)\n",
    "    Y[Y == 0] = -1\n",
    "    X -= np.mean(X, axis=0)\n",
    "    X /= np.std(X, axis=0)\n",
    "    return torch.from_numpy(X), torch.from_numpy(Y)\n",
    "\n",
    "def load_spam_data():\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    path = r\"C:\\Users\\sagil\\Desktop\\nir_project\\tip_spam_data\\IS_journal_tip_spam.arff\"\n",
    "    data, meta = arff.loadarff(path)\n",
    "    df = pd.DataFrame(data)\n",
    "    most_disc = ['qTips_plc', 'rating_plc', 'qEmail_tip', 'qContacts_tip', 'qURL_tip', 'qPhone_tip', 'qNumeriChar_tip', 'sentistrength_tip', 'combined_tip', 'qWords_tip', 'followers_followees_gph', 'qunigram_avg_tip', 'qTips_usr', 'indeg_gph', 'qCapitalChar_tip', 'class1']\n",
    "    df = df[most_disc]\n",
    "    df[\"class1\"].replace({b'spam': -1, b'notspam': 1}, inplace=True)\n",
    "    df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "    Y = df['class1'].values\n",
    "    X = df.drop('class1', axis = 1).values\n",
    "    x_dim = len(X[0])\n",
    "    X -= np.mean(X, axis=0)\n",
    "    X /= np.std(X, axis=0)\n",
    "    X /= math.sqrt(x_dim)\n",
    "    return torch.from_numpy(X), torch.from_numpy(Y)\n",
    "\n",
    "def load_card_fraud_data():\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    df = pd.read_csv('C:/Users/sagil/Desktop/nir_project/card_fraud_dataset/creditcard.csv')\n",
    "\n",
    "    rob_scaler = RobustScaler()\n",
    "\n",
    "    df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "    df.drop(['Time','Amount'], axis=1, inplace=True)\n",
    "    scaled_amount = df['scaled_amount']\n",
    "    df.drop(['scaled_amount'], axis=1, inplace=True)\n",
    "    df.insert(0, 'scaled_amount', scaled_amount)\n",
    "\n",
    "    df[\"Class\"].replace({1: -1, 0: 1}, inplace=True)\n",
    "    df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "    # amount of fraud classes 492 rows.\n",
    "    fraud_df = df.loc[df['Class'] == -1]\n",
    "    non_fraud_df = df.loc[df['Class'] == 1][:492]\n",
    "\n",
    "    normal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n",
    "\n",
    "    # Shuffle dataframe rows\n",
    "    df = normal_distributed_df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "    Y = df['Class'].values\n",
    "    X = df.drop('Class', axis = 1).values\n",
    "    x_dim = len(X[0])\n",
    "    X -= np.mean(X, axis=0)\n",
    "    X /= np.std(X, axis=0)\n",
    "    X /= math.sqrt(x_dim)\n",
    "    return torch.from_numpy(X), torch.from_numpy(Y)\n",
    "\n",
    "def load_credit_default_data():\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    url = 'https://raw.githubusercontent.com/ustunb/actionable-recourse/master/examples/paper/data/credit_processed.csv'\n",
    "    df = pd.read_csv(url)\n",
    "    df[\"NoDefaultNextMonth\"].replace({0: -1}, inplace=True)\n",
    "    df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "    df = df.drop(['Married', 'Single', 'Age_lt_25', 'Age_in_25_to_40', 'Age_in_40_to_59', 'Age_geq_60'], axis = 1)\n",
    "\n",
    "    fraud_df = df.loc[df[\"NoDefaultNextMonth\"] == -1]\n",
    "    non_fraud_df = df.loc[df[\"NoDefaultNextMonth\"] == 1][:6636]\n",
    "\n",
    "    normal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n",
    "\n",
    "    # Shuffle dataframe rows\n",
    "    df = normal_distributed_df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df.loc[:, df.columns != \"NoDefaultNextMonth\"] = scaler.fit_transform(df.drop(\"NoDefaultNextMonth\", axis=1)) \n",
    "    Y, X = df.iloc[:, 0].values, df.iloc[:, 1:].values\n",
    "    x_dim = len(X[0])\n",
    "    X -= np.mean(X, axis=0)\n",
    "    X /= np.std(X, axis=0)\n",
    "    X /= math.sqrt(x_dim)\n",
    "    return torch.from_numpy(X), torch.from_numpy(Y)\n",
    "\n",
    "def load_financial_distress_data():\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    data = pd.read_csv(\"C:/Users/sagil/Desktop/nir_project/financial_distress_data/Financial Distress.csv\")\n",
    "\n",
    "    data = data[data.columns.drop(list(data.filter(regex='x80')))] # Since it is a categorical feature with 37 features.\n",
    "    x_dim = len(data.columns) - 3\n",
    "    data.drop(['Time'], axis=1, inplace=True)\n",
    "\n",
    "    data_grouped = data.groupby(['Company']).last()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    data_grouped.loc[:, data_grouped.columns != \"Financial Distress\"] = scaler.fit_transform(data_grouped.drop(\"Financial Distress\", axis=1))\n",
    "\n",
    "    # Shuffle dataframe rows\n",
    "    data_grouped = data_grouped.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "    Y, X = data_grouped.iloc[:, 0].values, data_grouped.iloc[:, 1:].values\n",
    "    for y in range(0,len(Y)): # Coverting target variable from continuous to binary form\n",
    "        if Y[y] < -0.5:\n",
    "              Y[y] = -1\n",
    "        else:\n",
    "              Y[y] = 1\n",
    "    x_dim = len(X[0])\n",
    "    X -= np.mean(X, axis=0)\n",
    "    X /= np.std(X, axis=0)\n",
    "    X /= math.sqrt(x_dim)\n",
    "    return torch.from_numpy(X), torch.from_numpy(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 5])\n",
      "percent of positive samples: 50.78125%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\expressions\\expression.py:550: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:163: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 001 / 768 | loss: 0.31228 | err: 0.00000\n",
      "batch 002 / 768 | loss: 0.67294 | err: 0.50000\n",
      "batch 003 / 768 | loss: 0.44863 | err: 0.33333\n",
      "batch 004 / 768 | loss: 0.81932 | err: 0.50000\n",
      "batch 005 / 768 | loss: 0.65546 | err: 0.40000\n",
      "batch 006 / 768 | loss: 0.83054 | err: 0.50000\n",
      "batch 007 / 768 | loss: 0.71189 | err: 0.42857\n",
      "batch 008 / 768 | loss: 0.77489 | err: 0.50000\n",
      "batch 009 / 768 | loss: 0.68879 | err: 0.44444\n",
      "batch 010 / 768 | loss: 0.61991 | err: 0.40000\n",
      "batch 011 / 768 | loss: 0.59401 | err: 0.36364\n",
      "batch 012 / 768 | loss: 0.57693 | err: 0.33333\n",
      "batch 013 / 768 | loss: 0.53255 | err: 0.30769\n",
      "batch 014 / 768 | loss: 0.57349 | err: 0.35714\n",
      "batch 015 / 768 | loss: 0.56817 | err: 0.33333\n",
      "batch 016 / 768 | loss: 0.53266 | err: 0.31250\n",
      "batch 017 / 768 | loss: 0.51330 | err: 0.29412\n",
      "batch 018 / 768 | loss: 0.50176 | err: 0.27778\n",
      "batch 019 / 768 | loss: 0.56159 | err: 0.31579\n",
      "batch 020 / 768 | loss: 0.53351 | err: 0.30000\n",
      "batch 021 / 768 | loss: 0.65893 | err: 0.33333\n",
      "batch 022 / 768 | loss: 0.80188 | err: 0.36364\n",
      "batch 023 / 768 | loss: 0.76701 | err: 0.34783\n",
      "batch 024 / 768 | loss: 0.73506 | err: 0.33333\n",
      "batch 025 / 768 | loss: 0.75654 | err: 0.36000\n",
      "batch 026 / 768 | loss: 0.72744 | err: 0.34615\n",
      "batch 027 / 768 | loss: 0.70050 | err: 0.33333\n",
      "batch 028 / 768 | loss: 0.77841 | err: 0.35714\n",
      "batch 029 / 768 | loss: 0.76809 | err: 0.34483\n",
      "batch 030 / 768 | loss: 0.88046 | err: 0.36667\n",
      "batch 031 / 768 | loss: 0.89405 | err: 0.38710\n",
      "batch 032 / 768 | loss: 0.86611 | err: 0.37500\n",
      "batch 033 / 768 | loss: 0.84660 | err: 0.36364\n",
      "batch 034 / 768 | loss: 0.85837 | err: 0.38235\n",
      "batch 035 / 768 | loss: 0.84948 | err: 0.37143\n",
      "batch 036 / 768 | loss: 0.82588 | err: 0.36111\n",
      "batch 037 / 768 | loss: 0.85735 | err: 0.37838\n",
      "batch 038 / 768 | loss: 0.83549 | err: 0.36842\n",
      "batch 039 / 768 | loss: 0.85614 | err: 0.38462\n",
      "batch 040 / 768 | loss: 0.83474 | err: 0.37500\n",
      "batch 041 / 768 | loss: 0.82739 | err: 0.36585\n",
      "batch 042 / 768 | loss: 0.80769 | err: 0.35714\n",
      "batch 043 / 768 | loss: 0.78986 | err: 0.34884\n",
      "batch 044 / 768 | loss: 0.77190 | err: 0.34091\n",
      "batch 045 / 768 | loss: 0.76733 | err: 0.33333\n",
      "batch 046 / 768 | loss: 0.79450 | err: 0.34783\n",
      "batch 047 / 768 | loss: 0.78542 | err: 0.34043\n",
      "batch 048 / 768 | loss: 0.76906 | err: 0.33333\n",
      "batch 049 / 768 | loss: 0.75337 | err: 0.32653\n",
      "batch 050 / 768 | loss: 0.73830 | err: 0.32000\n",
      "batch 051 / 768 | loss: 0.73362 | err: 0.31373\n",
      "batch 052 / 768 | loss: 0.71951 | err: 0.30769\n",
      "batch 053 / 768 | loss: 0.70594 | err: 0.30189\n",
      "batch 054 / 768 | loss: 0.69286 | err: 0.29630\n",
      "batch 055 / 768 | loss: 0.68375 | err: 0.29091\n",
      "batch 056 / 768 | loss: 0.70056 | err: 0.30357\n",
      "batch 057 / 768 | loss: 0.69281 | err: 0.29825\n",
      "batch 058 / 768 | loss: 0.68087 | err: 0.29310\n",
      "batch 059 / 768 | loss: 0.66933 | err: 0.28814\n",
      "batch 060 / 768 | loss: 0.65829 | err: 0.28333\n",
      "batch 061 / 768 | loss: 0.64750 | err: 0.27869\n",
      "batch 062 / 768 | loss: 0.66255 | err: 0.29032\n",
      "batch 063 / 768 | loss: 0.65203 | err: 0.28571\n",
      "batch 064 / 768 | loss: 0.64184 | err: 0.28125\n",
      "batch 065 / 768 | loss: 0.65654 | err: 0.29231\n",
      "batch 066 / 768 | loss: 0.64660 | err: 0.28788\n",
      "batch 067 / 768 | loss: 0.63694 | err: 0.28358\n",
      "batch 068 / 768 | loss: 0.63074 | err: 0.27941\n",
      "batch 069 / 768 | loss: 0.62647 | err: 0.27536\n",
      "batch 070 / 768 | loss: 0.69750 | err: 0.28571\n",
      "batch 071 / 768 | loss: 0.71096 | err: 0.29577\n",
      "batch 072 / 768 | loss: 0.76271 | err: 0.30556\n",
      "batch 073 / 768 | loss: 0.75226 | err: 0.30137\n",
      "batch 074 / 768 | loss: 0.74210 | err: 0.29730\n",
      "batch 075 / 768 | loss: 0.76348 | err: 0.30667\n",
      "batch 076 / 768 | loss: 0.75343 | err: 0.30263\n",
      "batch 077 / 768 | loss: 0.74508 | err: 0.29870\n",
      "batch 078 / 768 | loss: 0.73553 | err: 0.29487\n",
      "batch 079 / 768 | loss: 0.74555 | err: 0.30380\n",
      "batch 080 / 768 | loss: 0.75683 | err: 0.31250\n",
      "batch 081 / 768 | loss: 0.74944 | err: 0.30864\n",
      "batch 082 / 768 | loss: 0.74030 | err: 0.30488\n",
      "batch 083 / 768 | loss: 0.73138 | err: 0.30120\n",
      "batch 084 / 768 | loss: 0.74513 | err: 0.30952\n",
      "batch 085 / 768 | loss: 0.73637 | err: 0.30588\n",
      "batch 086 / 768 | loss: 0.73039 | err: 0.30233\n",
      "batch 087 / 768 | loss: 0.73980 | err: 0.31034\n",
      "batch 088 / 768 | loss: 0.73391 | err: 0.30682\n",
      "batch 089 / 768 | loss: 0.72785 | err: 0.30337\n",
      "batch 090 / 768 | loss: 0.71976 | err: 0.30000\n",
      "batch 091 / 768 | loss: 0.73114 | err: 0.30769\n",
      "batch 092 / 768 | loss: 0.72319 | err: 0.30435\n",
      "batch 093 / 768 | loss: 0.71542 | err: 0.30108\n",
      "batch 094 / 768 | loss: 0.70780 | err: 0.29787\n",
      "batch 095 / 768 | loss: 0.70035 | err: 0.29474\n",
      "batch 096 / 768 | loss: 0.69306 | err: 0.29167\n",
      "batch 097 / 768 | loss: 0.68591 | err: 0.28866\n",
      "batch 098 / 768 | loss: 0.67891 | err: 0.28571\n",
      "batch 099 / 768 | loss: 0.67495 | err: 0.28283\n",
      "batch 100 / 768 | loss: 0.66820 | err: 0.28000\n",
      "batch 101 / 768 | loss: 0.67681 | err: 0.28713\n",
      "batch 102 / 768 | loss: 0.67017 | err: 0.28431\n",
      "batch 103 / 768 | loss: 0.66379 | err: 0.28155\n",
      "batch 104 / 768 | loss: 0.67156 | err: 0.28846\n",
      "batch 105 / 768 | loss: 0.66911 | err: 0.28571\n",
      "batch 106 / 768 | loss: 0.68249 | err: 0.29245\n",
      "batch 107 / 768 | loss: 0.67611 | err: 0.28972\n",
      "batch 108 / 768 | loss: 0.68827 | err: 0.29630\n",
      "batch 109 / 768 | loss: 0.68195 | err: 0.29358\n",
      "batch 110 / 768 | loss: 0.67575 | err: 0.29091\n",
      "batch 111 / 768 | loss: 0.66967 | err: 0.28829\n",
      "batch 112 / 768 | loss: 0.66369 | err: 0.28571\n",
      "batch 113 / 768 | loss: 0.67475 | err: 0.29204\n",
      "batch 114 / 768 | loss: 0.66883 | err: 0.28947\n",
      "batch 115 / 768 | loss: 0.66302 | err: 0.28696\n",
      "batch 116 / 768 | loss: 0.65730 | err: 0.28448\n",
      "batch 117 / 768 | loss: 0.65193 | err: 0.28205\n",
      "batch 118 / 768 | loss: 0.64989 | err: 0.27966\n",
      "batch 119 / 768 | loss: 0.64443 | err: 0.27731\n",
      "batch 120 / 768 | loss: 0.64281 | err: 0.27500\n",
      "batch 121 / 768 | loss: 0.63750 | err: 0.27273\n",
      "batch 122 / 768 | loss: 0.63227 | err: 0.27049\n",
      "batch 123 / 768 | loss: 0.62713 | err: 0.26829\n",
      "batch 124 / 768 | loss: 0.63788 | err: 0.27419\n",
      "batch 125 / 768 | loss: 0.63277 | err: 0.27200\n",
      "batch 126 / 768 | loss: 0.62775 | err: 0.26984\n",
      "batch 127 / 768 | loss: 0.62281 | err: 0.26772\n",
      "batch 128 / 768 | loss: 0.61794 | err: 0.26562\n",
      "batch 129 / 768 | loss: 0.61315 | err: 0.26357\n",
      "batch 130 / 768 | loss: 0.60844 | err: 0.26154\n",
      "batch 131 / 768 | loss: 0.60379 | err: 0.25954\n",
      "batch 132 / 768 | loss: 0.59922 | err: 0.25758\n",
      "batch 133 / 768 | loss: 0.61166 | err: 0.26316\n",
      "batch 134 / 768 | loss: 0.60710 | err: 0.26119\n",
      "batch 135 / 768 | loss: 0.60298 | err: 0.25926\n",
      "batch 136 / 768 | loss: 0.59855 | err: 0.25735\n",
      "batch 137 / 768 | loss: 0.59418 | err: 0.25547\n",
      "batch 138 / 768 | loss: 0.60458 | err: 0.26087\n",
      "batch 139 / 768 | loss: 0.61684 | err: 0.26619\n",
      "batch 140 / 768 | loss: 0.61243 | err: 0.26429\n",
      "batch 141 / 768 | loss: 0.60809 | err: 0.26241\n",
      "batch 142 / 768 | loss: 0.60381 | err: 0.26056\n",
      "batch 143 / 768 | loss: 0.59958 | err: 0.25874\n",
      "batch 144 / 768 | loss: 0.60587 | err: 0.26389\n",
      "batch 145 / 768 | loss: 0.60169 | err: 0.26207\n",
      "batch 146 / 768 | loss: 0.59904 | err: 0.26027\n",
      "batch 147 / 768 | loss: 0.60573 | err: 0.26531\n",
      "batch 148 / 768 | loss: 0.60164 | err: 0.26351\n",
      "batch 149 / 768 | loss: 0.59760 | err: 0.26174\n",
      "batch 150 / 768 | loss: 0.59362 | err: 0.26000\n",
      "batch 151 / 768 | loss: 0.58969 | err: 0.25828\n",
      "batch 152 / 768 | loss: 0.59710 | err: 0.26316\n",
      "batch 153 / 768 | loss: 0.59320 | err: 0.26144\n",
      "batch 154 / 768 | loss: 0.59042 | err: 0.25974\n",
      "batch 155 / 768 | loss: 0.58661 | err: 0.25806\n",
      "batch 156 / 768 | loss: 0.58551 | err: 0.25641\n",
      "batch 157 / 768 | loss: 0.58179 | err: 0.25478\n",
      "batch 158 / 768 | loss: 0.58140 | err: 0.25316\n",
      "batch 159 / 768 | loss: 0.57774 | err: 0.25157\n",
      "batch 160 / 768 | loss: 0.57413 | err: 0.25000\n",
      "batch 161 / 768 | loss: 0.57057 | err: 0.24845\n",
      "batch 162 / 768 | loss: 0.56736 | err: 0.24691\n",
      "batch 163 / 768 | loss: 0.56601 | err: 0.24540\n",
      "batch 164 / 768 | loss: 0.56256 | err: 0.24390\n",
      "batch 165 / 768 | loss: 0.55915 | err: 0.24242\n",
      "batch 166 / 768 | loss: 0.55578 | err: 0.24096\n",
      "batch 167 / 768 | loss: 0.55245 | err: 0.23952\n",
      "batch 168 / 768 | loss: 0.54917 | err: 0.23810\n",
      "batch 169 / 768 | loss: 0.54592 | err: 0.23669\n",
      "batch 170 / 768 | loss: 0.55356 | err: 0.24118\n",
      "batch 171 / 768 | loss: 0.55032 | err: 0.23977\n",
      "batch 172 / 768 | loss: 0.54712 | err: 0.23837\n",
      "batch 173 / 768 | loss: 0.54396 | err: 0.23699\n",
      "batch 174 / 768 | loss: 0.54084 | err: 0.23563\n",
      "batch 175 / 768 | loss: 0.53774 | err: 0.23429\n",
      "batch 176 / 768 | loss: 0.53469 | err: 0.23295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 177 / 768 | loss: 0.53265 | err: 0.23164\n",
      "batch 178 / 768 | loss: 0.52966 | err: 0.23034\n",
      "batch 179 / 768 | loss: 0.52670 | err: 0.22905\n",
      "batch 180 / 768 | loss: 0.53261 | err: 0.23333\n",
      "batch 181 / 768 | loss: 0.53856 | err: 0.23757\n",
      "batch 182 / 768 | loss: 0.53560 | err: 0.23626\n",
      "batch 183 / 768 | loss: 0.53267 | err: 0.23497\n",
      "batch 184 / 768 | loss: 0.52978 | err: 0.23370\n",
      "batch 185 / 768 | loss: 0.52691 | err: 0.23243\n",
      "batch 186 / 768 | loss: 0.52408 | err: 0.23118\n",
      "batch 187 / 768 | loss: 0.52128 | err: 0.22995\n",
      "batch 188 / 768 | loss: 0.51851 | err: 0.22872\n",
      "batch 189 / 768 | loss: 0.51576 | err: 0.22751\n",
      "batch 190 / 768 | loss: 0.51305 | err: 0.22632\n",
      "batch 191 / 768 | loss: 0.51036 | err: 0.22513\n",
      "batch 192 / 768 | loss: 0.50770 | err: 0.22396\n",
      "batch 193 / 768 | loss: 0.50507 | err: 0.22280\n",
      "batch 194 / 768 | loss: 0.51253 | err: 0.22680\n",
      "batch 195 / 768 | loss: 0.51064 | err: 0.22564\n",
      "batch 196 / 768 | loss: 0.50803 | err: 0.22449\n",
      "batch 197 / 768 | loss: 0.50545 | err: 0.22335\n",
      "batch 198 / 768 | loss: 0.50502 | err: 0.22222\n",
      "batch 199 / 768 | loss: 0.50248 | err: 0.22111\n",
      "batch 200 / 768 | loss: 0.49997 | err: 0.22000\n",
      "batch 201 / 768 | loss: 0.50962 | err: 0.22388\n",
      "batch 202 / 768 | loss: 0.50710 | err: 0.22277\n",
      "batch 203 / 768 | loss: 0.51651 | err: 0.22660\n",
      "batch 204 / 768 | loss: 0.52336 | err: 0.23039\n",
      "batch 205 / 768 | loss: 0.52080 | err: 0.22927\n",
      "batch 206 / 768 | loss: 0.51828 | err: 0.22816\n",
      "batch 207 / 768 | loss: 0.51577 | err: 0.22705\n",
      "batch 208 / 768 | loss: 0.51329 | err: 0.22596\n",
      "batch 209 / 768 | loss: 0.51084 | err: 0.22488\n",
      "batch 210 / 768 | loss: 0.50840 | err: 0.22381\n",
      "batch 211 / 768 | loss: 0.50599 | err: 0.22275\n",
      "batch 212 / 768 | loss: 0.51132 | err: 0.22642\n",
      "batch 213 / 768 | loss: 0.50892 | err: 0.22535\n",
      "batch 214 / 768 | loss: 0.50654 | err: 0.22430\n",
      "batch 215 / 768 | loss: 0.50418 | err: 0.22326\n",
      "batch 216 / 768 | loss: 0.50896 | err: 0.22685\n",
      "batch 217 / 768 | loss: 0.50661 | err: 0.22581\n",
      "batch 218 / 768 | loss: 0.50550 | err: 0.22477\n",
      "batch 219 / 768 | loss: 0.50319 | err: 0.22374\n",
      "batch 220 / 768 | loss: 0.50178 | err: 0.22273\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/sagil/Desktop/nir_project/models/runtime_varying_batch_size_dim5_avg_tol\"\n",
    "epochs = 5\n",
    "x_dim = 5\n",
    "scale = 1\n",
    "X, Y = gen_sklearn_data(x_dim, 1024)\n",
    "X, Y, Xval, Yval = split_data(X, Y, 0.25)\n",
    "print(Xval.size())\n",
    "print(\"percent of positive samples: {}%\".format(100 * len(Y[Y == 1]) / len(Y)))\n",
    "\n",
    "funcs = {\"f\": f, \"g\": g, \"f_derivative\": f_derivative, \"c\": c, \"score\": score}\n",
    "funcs_batch = {\"f\": f_batch, \"g\": g_batch, \"f_derivative\": f_derivative_batch, \"c\": c_batch, \"score\": score}\n",
    "\n",
    "total = []\n",
    "ccp = []\n",
    "for batch_size in (2**np.arange(9)).tolist():\n",
    "    strategic_model = MyStrategicModel(x_dim, batch_size, funcs, funcs_batch, TRAIN_SLOPE, EVAL_SLOPE, scale=scale, strategic=True)\n",
    "    strategic_model.fit(path, X, Y, Xval, Yval,\n",
    "                        opt=torch.optim.Adam, opt_kwargs={\"lr\": (1e-1)},\n",
    "                        batch_size=batch_size, epochs=epochs, verbose=True,\n",
    "                       comment=\"batched\")\n",
    "    \n",
    "    total_time = strategic_model.total_time\n",
    "    ccp_time = strategic_model.ccp_time\n",
    "    total.append(total_time)\n",
    "    ccp.append(ccp_time)\n",
    "    pd.DataFrame(np.array(total)).to_csv(path + '/total_timing_results.csv')\n",
    "    pd.DataFrame(np.array(ccp)).to_csv(path + '/ccp_timing_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.61718821525574\n"
     ]
    }
   ],
   "source": [
    "print(ccp_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "funcPred",
   "language": "python",
   "name": "funcpred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
