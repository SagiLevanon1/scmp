{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import cvxpy as cp\n",
    "import dccp\n",
    "import torch\n",
    "import numpy as np\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import zero_one_loss, confusion_matrix\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.patches as mpatches\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import os, psutil\n",
    "from datetime import datetime\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "TRAIN_SLOPE = 2\n",
    "EVAL_SLOPE = 5\n",
    "X_LOWER_BOUND = -10\n",
    "X_UPPER_BOUND = 10\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, Y, percentage):\n",
    "    num_val = int(len(X)*percentage)\n",
    "    return X[num_val:], Y[num_val:], X[:num_val], Y[:num_val]\n",
    "\n",
    "def shuffle(X, Y):\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    data = torch.cat((Y, X), 1)\n",
    "    data = data[torch.randperm(data.size()[0])]\n",
    "    X = data[:, 1:]\n",
    "    Y = data[:, 0]\n",
    "    return X, Y\n",
    "\n",
    "def conf_mat(Y1, Y2):\n",
    "    num_of_samples = len(Y1)\n",
    "    mat = confusion_matrix(Y1, Y2, labels=[-1, 1])*100/num_of_samples\n",
    "    acc = np.trace(mat)\n",
    "    return mat, acc\n",
    "\n",
    "def calc_accuracy(Y, Ypred):\n",
    "    num = len(Y)\n",
    "    temp = Y - Ypred\n",
    "    acc = len(temp[temp == 0])*1./num\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spam_data():\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    path = r\"C:\\Users\\sagil\\Desktop\\nir_project\\tip_spam_data\\IS_journal_tip_spam.arff\"\n",
    "    data, meta = arff.loadarff(path)\n",
    "    df = pd.DataFrame(data)\n",
    "    most_disc = ['qTips_plc', 'rating_plc', 'qEmail_tip', 'qContacts_tip', 'qURL_tip', 'qPhone_tip', 'qNumeriChar_tip', 'sentistrength_tip', 'combined_tip', 'qWords_tip', 'followers_followees_gph', 'qunigram_avg_tip', 'qTips_usr', 'indeg_gph', 'qCapitalChar_tip', 'class1']\n",
    "    df = df[most_disc]\n",
    "    df[\"class1\"].replace({b'spam': -1, b'notspam': 1}, inplace=True)\n",
    "    df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "    Y = df['class1'].values\n",
    "    X = df.drop('class1', axis = 1).values\n",
    "    x_dim = len(X[0])\n",
    "    X -= np.mean(X, axis=0)\n",
    "    X /= np.std(X, axis=0)\n",
    "    X /= math.sqrt(x_dim)\n",
    "    return torch.from_numpy(X), torch.from_numpy(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCP classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCP:\n",
    "    def __init__(self, x_dim, funcs, v, eps):\n",
    "        self.f_derivative = funcs[\"f_derivative\"]\n",
    "        self.g = funcs[\"g\"]\n",
    "        self.c = funcs[\"c\"]\n",
    "        \n",
    "        self.x = cp.Variable(x_dim)\n",
    "        self.xt = cp.Parameter(x_dim)\n",
    "        self.r = cp.Parameter(x_dim)\n",
    "        self.w = cp.Parameter(x_dim)\n",
    "        self.b = cp.Parameter(1)\n",
    "        self.slope = cp.Parameter(1)\n",
    "\n",
    "        target = self.x@self.f_derivative(self.xt, self.w, self.b, self.slope)-self.g(self.x, self.w, self.b, self.slope)-self.c(self.x, self.r, v, eps)\n",
    "        constraints = [self.x >= X_LOWER_BOUND,\n",
    "                       self.x <= X_UPPER_BOUND]\n",
    "        self.prob = cp.Problem(cp.Maximize(target), constraints)\n",
    "        \n",
    "    def ccp(self, r):\n",
    "        \"\"\"\n",
    "        numpy to numpy\n",
    "        \"\"\"\n",
    "        self.xt.value = r\n",
    "        self.r.value = r\n",
    "        result = self.prob.solve()\n",
    "        diff = np.linalg.norm(self.xt.value - self.x.value)\n",
    "        cnt = 0\n",
    "        while diff > 0.001 and cnt < 10:\n",
    "            cnt += 1\n",
    "            self.xt.value = self.x.value\n",
    "            result = self.prob.solve()\n",
    "            diff = np.linalg.norm(self.x.value - self.xt.value)\n",
    "        return self.x.value\n",
    "    \n",
    "    def optimize_X(self, X, w, b, slope):\n",
    "        \"\"\"\n",
    "        tensor to tensor\n",
    "        \"\"\"\n",
    "        w = w.detach().numpy()\n",
    "        b = b.detach().numpy()\n",
    "        slope = np.full(1, slope)\n",
    "        X = X.numpy()\n",
    "        \n",
    "        self.w.value = w\n",
    "        self.b.value = b\n",
    "        self.slope.value = slope\n",
    "        \n",
    "        return torch.stack([torch.from_numpy(self.ccp(x)) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DELTA():\n",
    "    \n",
    "    def __init__(self, x_dim, funcs, v, eps):\n",
    "        self.g = funcs[\"g\"]\n",
    "        self.c = funcs[\"c\"]\n",
    "        \n",
    "        self.x = cp.Variable(x_dim)\n",
    "        self.r = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.w = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "        self.b = cp.Parameter(1, value = np.random.randn(1))\n",
    "        self.f_der = cp.Parameter(x_dim, value = np.random.randn(x_dim))\n",
    "\n",
    "        target = self.x@self.f_der-self.g(self.x, self.w, self.b, TRAIN_SLOPE)-self.c(self.x, self.r, v, eps)\n",
    "        constraints = [self.x >= X_LOWER_BOUND,\n",
    "                       self.x <= X_UPPER_BOUND]\n",
    "        objective = cp.Maximize(target)\n",
    "        problem = cp.Problem(objective, constraints)\n",
    "        self.layer = CvxpyLayer(problem, parameters=[self.r, self.w, self.b, self.f_der],\n",
    "                                variables=[self.x])\n",
    "        \n",
    "    def optimize_X(self, X, w, b, F_DER):\n",
    "        return self.layer(X, w, b, F_DER)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gain & Cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(x, w, b):\n",
    "    return x@w + b\n",
    "\n",
    "def f(x, w, b, slope):\n",
    "    return 0.5*cp.norm(cp.hstack([1, (slope*score(x, w, b) + 1)]), 2)\n",
    "\n",
    "def g(x, w, b, slope):\n",
    "    return 0.5*cp.norm(cp.hstack([1, (slope*score(x, w, b) - 1)]), 2)\n",
    "\n",
    "def c(x, r, v, eps):\n",
    "    return (eps*cp.sum_squares(x-r) + (1-eps)*cp.pos((x-r)@v))\n",
    "\n",
    "def f_derivative(x, w, b, slope):\n",
    "    return 0.5*cp.multiply(slope*((slope*score(x, w, b) + 1)/cp.sqrt((slope*score(x, w, b) + 1)**2 + 1)), w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyStrategicModel(torch.nn.Module):\n",
    "    def __init__(self, x_dim, funcs, train_slope, eval_slope, v_true, eps_train, eps_true, strategic=False):\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "\n",
    "        super(MyStrategicModel, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.train_slope, self.eval_slope = train_slope, eval_slope\n",
    "        self.v_true = v_true\n",
    "        self.w = torch.nn.parameter.Parameter(math.sqrt(1/x_dim)*(1-2*torch.rand(x_dim, dtype=torch.float64, requires_grad=True)))\n",
    "        self.b = torch.nn.parameter.Parameter(math.sqrt(1/x_dim)*(1-2*torch.rand(1, dtype=torch.float64, requires_grad=True)))\n",
    "        self.strategic = strategic\n",
    "        self.eps_train = eps_train\n",
    "        self.eps_true = eps_true\n",
    "        self.ccp = CCP(x_dim, funcs, self.v_true, eps_train)\n",
    "        self.ccp_true = CCP(x_dim, funcs, self.v_true, eps_true)\n",
    "        self.delta = DELTA(x_dim, funcs, self.v_true, eps_train)\n",
    "\n",
    "    def forward(self, X, evaluation=False):\n",
    "        if self.strategic:\n",
    "            if evaluation:\n",
    "                XT = self.ccp.optimize_X(X, self.w, self.b, self.eval_slope)\n",
    "                X_opt = XT\n",
    "            else:\n",
    "                XT = self.ccp.optimize_X(X, self.w, self.b, self.train_slope)\n",
    "                F_DER = self.get_f_ders(XT, self.train_slope)\n",
    "                X_opt = self.delta.optimize_X(X, self.w, self.b, F_DER) # Xopt should be equal to XT but we do it again for the gradients\n",
    "            output = self.score(X_opt)\n",
    "        else:\n",
    "            output = self.score(X)        \n",
    "        return output\n",
    "    \n",
    "    def optimize_X(self, X):\n",
    "        return self.ccp_true.optimize_X(X, self.w, self.b, self.eval_slope)\n",
    "    \n",
    "    def normalize_weights(self):\n",
    "        with torch.no_grad():\n",
    "            norm = torch.sqrt(torch.sum(self.w**2) + self.b**2)\n",
    "            self.w /= norm\n",
    "            self.b /= norm\n",
    "\n",
    "    def score(self, x):\n",
    "        return x@self.w + self.b\n",
    "    \n",
    "    def get_f_ders(self, XT, slope):\n",
    "        return torch.stack([0.5*slope*((slope*self.score(xt) + 1)/torch.sqrt((slope*self.score(xt) + 1)**2 + 1))*self.w for xt in XT])\n",
    "\n",
    "    def calc_accuracy(self, Y, Y_pred):\n",
    "        Y_pred = torch.sign(Y_pred)\n",
    "        num = len(Y)\n",
    "        temp = Y - Y_pred\n",
    "        acc = len(temp[temp == 0])*1./num        \n",
    "        return acc\n",
    "    \n",
    "    def evaluate(self, X, Y):      \n",
    "        return self.calc_accuracy(Y, self.forward(X, evaluation=True))\n",
    "    \n",
    "    def loss(self, Y, Y_pred):\n",
    "        return torch.mean(torch.clamp(1 - Y_pred * Y, min=0))\n",
    "    \n",
    "    def save_model(self, train_errors, val_errors, train_losses, val_losses, info, path, comment=None):\n",
    "        if comment is not None:\n",
    "            path += \"/\" + comment\n",
    "            \n",
    "        filename = path + \"/model.pt\"\n",
    "        if not os.path.exists(os.path.dirname(filename)):\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        torch.save(self.state_dict(), filename)\n",
    "        \n",
    "        pd.DataFrame(self.v_true).to_csv(path + '/v_true.csv')\n",
    "        pd.DataFrame(np.array(train_errors)).to_csv(path + '/train_errors.csv')\n",
    "        pd.DataFrame(np.array(val_errors)).to_csv(path + '/val_errors.csv')\n",
    "        pd.DataFrame(np.array(train_losses)).to_csv(path + '/train_losses.csv')\n",
    "        pd.DataFrame(np.array(val_losses)).to_csv(path + '/val_losses.csv')\n",
    "        \n",
    "        with open(path + \"/info.txt\", \"w\") as f:\n",
    "            f.write(info)\n",
    "    \n",
    "    def load_model(self, filename):\n",
    "        self.load_state_dict(torch.load(filename))\n",
    "        self.eval()\n",
    "    \n",
    "    def fit(self, path, X, Y, Xval, Yval, opt, opt_kwargs={\"lr\":1e-3}, batch_size=128, epochs=100, verbose=False, callback=None, comment=None):\n",
    "        train_dset = TensorDataset(X, Y)\n",
    "        train_loader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "        opt = opt(self.parameters(), **opt_kwargs)\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_errors = []\n",
    "        val_errors = []\n",
    "        \n",
    "        best_val_error = 1\n",
    "        consecutive_no_improvement = 0\n",
    "\n",
    "        total_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            t1 = time.time()\n",
    "            batch = 1\n",
    "            train_losses.append([])\n",
    "            train_errors.append([])\n",
    "            for Xbatch, Ybatch in train_loader:\n",
    "                try:\n",
    "                    opt.zero_grad()\n",
    "                    Ybatch_pred = self.forward(Xbatch)\n",
    "                    l = self.loss(Ybatch, Ybatch_pred)\n",
    "                    l.backward()\n",
    "                    opt.step()\n",
    "                    train_losses[-1].append(l.item())\n",
    "                    with torch.no_grad():\n",
    "                        e = self.calc_accuracy(Ybatch, Ybatch_pred)\n",
    "                        train_errors[-1].append(1-e)\n",
    "                    if verbose:\n",
    "                        print(\"batch %03d / %03d | loss: %3.5f | err: %3.5f\" %\n",
    "                              (batch, len(train_loader), np.mean(train_losses[-1]), np.mean(train_errors[-1])))\n",
    "                    batch += 1\n",
    "                    if callback is not None:\n",
    "                        callback()\n",
    "                except:\n",
    "                    print(\"failed\")\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    Yval_pred = self.forward(Xval, evaluation=True)\n",
    "                    val_loss = self.loss(Yval, Yval_pred).item()\n",
    "                    val_losses.append(val_loss)\n",
    "                    val_error = 1-self.calc_accuracy(Yval, Yval_pred)\n",
    "                    val_errors.append(val_error)\n",
    "                    if val_error < best_val_error:\n",
    "                        consecutive_no_improvement = 0\n",
    "                        best_val_error = val_error\n",
    "                        info = \"training time in seconds: {}\\nepoch: {}\\nbatch size: {}\\ntrain slope: {}\\neval slope: {}\\nlearning rate: {}\\nvalidation loss: {}\\nvalidation error: {}\\n\".format(\n",
    "                        time.time()-total_time, epoch, batch_size, self.train_slope, self.eval_slope, opt_kwargs[\"lr\"], val_loss, val_error)\n",
    "                        self.save_model(train_errors, val_errors, train_losses, val_losses, info, path, comment)\n",
    "                        print(\"model saved!\")\n",
    "\n",
    "                    else:\n",
    "                        consecutive_no_improvement += 1\n",
    "                        if consecutive_no_improvement >= 4:\n",
    "                            break\n",
    "                except:\n",
    "                    print(\"failed\")\n",
    "                    \n",
    "            t2 = time.time()\n",
    "            if verbose:\n",
    "                print(\"------------- epoch %03d / %03d | time: %03d sec | loss: %3.5f | err: %3.5f\" % (epoch + 1, epochs, t2-t1, val_losses[-1], val_errors[-1]))\n",
    "        print(\"training time: {} seconds\".format(time.time()-total_time)) \n",
    "        return train_errors, val_errors, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gen_sklearn_data(x_dim, N, informative_frac=1, shift_range=1, scale_range=1, noise_frac=0.01):\n",
    "#     torch.manual_seed(0)\n",
    "#     np.random.seed(0)\n",
    "#     n_informative = int(informative_frac*x_dim)\n",
    "#     n_redundant = x_dim - n_informative\n",
    "#     shift_arr = shift_range*np.random.randn(x_dim)\n",
    "#     scale_arr = scale_range*np.random.randn(x_dim)\n",
    "#     X, Y = make_classification(n_samples=N, n_features=x_dim, n_informative=n_informative, n_redundant=n_redundant,\n",
    "#                                flip_y=noise_frac, shift=shift_arr, scale=scale_arr, random_state=0)\n",
    "#     Y[Y == 0] = -1\n",
    "#     X -= np.mean(X, axis=0)\n",
    "#     X /= np.std(X, axis=0)\n",
    "#     return torch.from_numpy(X), torch.from_numpy(Y)\n",
    "\n",
    "# def gen_custom_normal_data(x_dim, N, pos_mean, pos_std, neg_mean, neg_std):\n",
    "#     torch.manual_seed(0)\n",
    "#     np.random.seed(0)\n",
    "#     pos_samples_num = N//2\n",
    "#     neg_samples_num = N - pos_samples_num\n",
    "#     posX = torch.randn((pos_samples_num, x_dim))*pos_std + pos_mean\n",
    "#     negX = torch.randn((neg_samples_num, x_dim))*neg_std + neg_mean\n",
    "    \n",
    "#     X = torch.cat((posX, negX), 0)\n",
    "#     Y = torch.unsqueeze(torch.cat((torch.ones(len(posX)), -torch.ones(len(negX))), 0), 1)\n",
    "\n",
    "#     X, Y = shuffle(X, Y)\n",
    "#     X = X.numpy()\n",
    "#     X -= np.mean(X, axis=0)\n",
    "#     X /= np.std(X, axis=0)\n",
    "#     X /= math.sqrt(x_dim)\n",
    "#     return torch.Tensor(X), Y\n",
    "\n",
    "# def c(x, r, x_dim, scale):\n",
    "#     return (scale)*cp.sum_squares(x-r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"C:/Users/sagil/Desktop/nir_project/models/vanilla___\" + str(SEED)\n",
    "# epochs = 10\n",
    "# batch_size = 24\n",
    "# x_dim = 15\n",
    "# scale = 1\n",
    "# X, Y = load_spam_data() #gen_custom_normal_data(x_dim, 422, torch.full((x_dim,), 0.5), torch.full((x_dim,), 0.5), torch.full((x_dim,), -0.5), torch.full((x_dim,), 0.5)) #gen_sklearn_data(x_dim, 422, scale_range=3)\n",
    "# X, Y = X[:1500], Y[:1500]\n",
    "# X, Y, Xval, Yval = split_data(X, Y, 0.1)\n",
    "# Xval, Yval, Xtest, Ytest = split_data(Xval, Yval, 0.5)\n",
    "# print(X.size())\n",
    "# print(\"percent of positive samples: {}%\".format(100 * len(Y[Y == 1]) / len(Y)))\n",
    "\n",
    "# funcs = {\"f\": f, \"g\": g, \"f_derivative\": f_derivative, \"c\": c, \"score\": score}\n",
    "\n",
    "\n",
    "# non_strategic_model = MyStrategicModel(x_dim, funcs, TRAIN_SLOPE, EVAL_SLOPE, scale=scale, strategic=False)\n",
    "# non_strategic_model.fit(path, X, Y, Xval, Yval,\n",
    "#                         opt=torch.optim.Adam, opt_kwargs={\"lr\": (1e-1)},\n",
    "#                         batch_size=batch_size, epochs=epochs, verbose=True,\n",
    "#                        comment=\"non_strategic\")\n",
    "\n",
    "# non_strategic_model = MyStrategicModel(x_dim, funcs, TRAIN_SLOPE, EVAL_SLOPE, scale=scale, strategic=False)\n",
    "# non_strategic_model.load_model(path + \"/non_strategic/model.pt\")\n",
    "\n",
    "\n",
    "# # Xtest_opt = non_strategic_model.optimize_X(Xtest, evaluation=True)\n",
    "# # print(non_strategic_model.evaluate(Xtest, Ytest))\n",
    "# # print(non_strategic_model.evaluate(Xtest_opt, Ytest))\n",
    "    \n",
    "    \n",
    "# with torch.no_grad():\n",
    "#     Xtest_opt = non_strategic_model.optimize_X(Xtest, evaluation=True)\n",
    "#     print(non_strategic_model.evaluate(Xtest, Ytest))\n",
    "#     print(non_strategic_model.evaluate(Xtest_opt, Ytest))\n",
    "\n",
    "#     norm = torch.sqrt(torch.sum(non_strategic_model.w**2) + non_strategic_model.b**2)\n",
    "#     print(torch.norm(non_strategic_model.w))\n",
    "#     non_strategic_model.w /= norm\n",
    "#     non_strategic_model.w /= 1\n",
    "#     non_strategic_model.b /= norm\n",
    "#     non_strategic_model.b /= 1\n",
    "\n",
    "#     Xtest_opt = non_strategic_model.optimize_X(Xtest, evaluation=True)\n",
    "#     print(non_strategic_model.evaluate(Xtest, Ytest))\n",
    "#     print(non_strategic_model.evaluate(Xtest_opt, Ytest))\n",
    "#     print(torch.norm(non_strategic_model.w))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# strategic_model = MyStrategicModel(x_dim, funcs, TRAIN_SLOPE, EVAL_SLOPE, scale=scale, strategic=True)\n",
    "# strategic_model.fit(path, X, Y, Xval, Yval,\n",
    "#                     opt=torch.optim.Adam, opt_kwargs={\"lr\": 5*(1e-1)},\n",
    "#                     batch_size=batch_size, epochs=epochs, verbose=True, \n",
    "#                    comment=\"strategic\") \n",
    "\n",
    "# strategic_model = MyStrategicModel(x_dim, funcs, TRAIN_SLOPE, EVAL_SLOPE, scale=scale, strategic=True)\n",
    "# strategic_model.load_model(path + \"/strategic/model.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:/Users/sagil/Desktop/nir_project/models/vanilla_vs_hardt_check_smaller_eps\" + str(SEED)\n",
    "\n",
    "path = PATH\n",
    "\n",
    "# spam dataset\n",
    "X, Y = load_spam_data()\n",
    "X, Y = X[:500], Y[:500]\n",
    "X, Y, Xval, Yval = split_data(X, Y, 0.4)\n",
    "Xval, Yval, Xtest, Ytest = split_data(Xval, Yval, 0.5)\n",
    "\n",
    "# save dataset splits\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "pd.DataFrame(X.numpy()).to_csv(path + '/X.csv')\n",
    "pd.DataFrame(Y.numpy()).to_csv(path + '/Y.csv')\n",
    "pd.DataFrame(Xval.numpy()).to_csv(path + '/Xval.csv')\n",
    "pd.DataFrame(Yval.numpy()).to_csv(path + '/Yval.csv')\n",
    "pd.DataFrame(Xtest.numpy()).to_csv(path + '/Xtest.csv')\n",
    "pd.DataFrame(Ytest.numpy()).to_csv(path + '/Ytest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "x_dim = len(X[0])\n",
    "epochs = 6\n",
    "batch_size = 128\n",
    "\n",
    "small_eps = 0.01\n",
    "v_true = np.array([-1,-1,-1,-1,-1,-1,-1,1,1,0.1,1,0.1,0.1,1,0.1])\n",
    "funcs = {\"f\": f, \"g\": g, \"f_derivative\": f_derivative, \"c\": c, \"score\": score}\n",
    "\n",
    "# strategic classification\n",
    "print(\"---------- training strategically----------\")\n",
    "strategic_model_approx = MyStrategicModel(x_dim, funcs, TRAIN_SLOPE, EVAL_SLOPE, v_true, small_eps, small_eps, strategic=True)\n",
    "strategic_model_approx.fit(PATH, X, Y, Xval, Yval,\n",
    "                    opt=torch.optim.Adam, opt_kwargs={\"lr\": 5*(1e-1)},\n",
    "                    batch_size=batch_size, epochs=epochs, verbose=True, \n",
    "                   comment=\"strategic_approx\")\n",
    "\n",
    "strategic_model_approx = MyStrategicModel(x_dim, funcs, TRAIN_SLOPE, EVAL_SLOPE, v_true, small_eps, small_eps, strategic=True)\n",
    "strategic_model_approx.load_model(PATH + \"/strategic_approx/model.pt\")\n",
    "\n",
    "\n",
    "strategic_model_approx.eps_true = 0.01\n",
    "Xtest_opt = strategic_model_approx.optimize_X(Xtest)\n",
    "test_scores = strategic_model_approx.score(Xtest_opt)\n",
    "print(strategic_model_approx.calc_accuracy(Ytest, test_scores))\n",
    "\n",
    "strategic_model_approx.eps_true = 0.1\n",
    "Xtest_opt = strategic_model_approx.optimize_X(Xtest)\n",
    "test_scores = strategic_model_approx.score(Xtest_opt)\n",
    "print(strategic_model_approx.calc_accuracy(Ytest, test_scores))\n",
    "\n",
    "strategic_model_approx.eps_true = 0.5\n",
    "Xtest_opt = strategic_model_approx.optimize_X(Xtest)\n",
    "test_scores = strategic_model_approx.score(Xtest_opt)\n",
    "print(strategic_model_approx.calc_accuracy(Ytest, test_scores))\n",
    "\n",
    "strategic_model_approx.eps_true = 1\n",
    "Xtest_opt = strategic_model_approx.optimize_X(Xtest)\n",
    "test_scores = strategic_model_approx.score(Xtest_opt)\n",
    "print(strategic_model_approx.calc_accuracy(Ytest, test_scores))\n",
    "    \n",
    "    \n",
    "raise\n",
    "\n",
    "epsilons = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "for eps in epsilons:\n",
    "    path = PATH + \"/\" + str(eps)\n",
    "    print(\"------------------------- {} -------------------------\".format(eps))\n",
    "    \n",
    "    # non-strategic classification\n",
    "    print(\"---------- training non-strategically----------\")\n",
    "    non_strategic_model = MyStrategicModel(x_dim, funcs, TRAIN_SLOPE, EVAL_SLOPE, v_true, eps, eps, strategic=False)\n",
    "    non_strategic_model.fit(path, X, Y, Xval, Yval,\n",
    "                            opt=torch.optim.Adam, opt_kwargs={\"lr\": 5*(1e-1)},\n",
    "                            batch_size=batch_size, epochs=epochs, verbose=False,\n",
    "                           comment=\"non_strategic\")\n",
    "\n",
    "    non_strategic_model = MyStrategicModel(x_dim, funcs, TRAIN_SLOPE, EVAL_SLOPE, v_true, eps, eps, strategic=False)\n",
    "    non_strategic_model.load_model(path + \"/non_strategic/model.pt\")\n",
    "    non_strategic_model.normalize_weights()\n",
    "    \n",
    "    # strategic classification\n",
    "    print(\"---------- training strategically----------\")\n",
    "    strategic_model_real = MyStrategicModel(x_dim, funcs, TRAIN_SLOPE, EVAL_SLOPE, v_true, eps, eps, strategic=True)\n",
    "    strategic_model_real.fit(path, X, Y, Xval, Yval,\n",
    "                        opt=torch.optim.Adam, opt_kwargs={\"lr\": 5*(1e-1)},\n",
    "                        batch_size=batch_size, epochs=epochs, verbose=False, \n",
    "                       comment=\"strategic_real\") \n",
    "\n",
    "    strategic_model_real = MyStrategicModel(x_dim, funcs, TRAIN_SLOPE, EVAL_SLOPE, v_true, eps, eps, strategic=True)\n",
    "    strategic_model_real.load_model(path + \"/strategic_real/model.pt\")\n",
    "\n",
    "    strategic_model_approx = MyStrategicModel(x_dim, funcs, TRAIN_SLOPE, EVAL_SLOPE, v_true, small_eps, eps, strategic=True)\n",
    "    strategic_model_approx.load_model(PATH + \"/strategic_approx/model.pt\")\n",
    "    print(strategic_model_approx.eps_train)\n",
    "    \n",
    "    # calculate results\n",
    "    accuracies = np.zeros(4)\n",
    "    # non strategic model & non strategic data\n",
    "    accuracies[0] = (non_strategic_model.evaluate(Xtest, Ytest))\n",
    "    # approx strategic model & strategic data\n",
    "    Xtest_opt = strategic_model_approx.optimize_X(Xtest)\n",
    "    test_scores = strategic_model_approx.score(Xtest_opt)\n",
    "    accuracies[1] = (strategic_model_approx.calc_accuracy(Ytest, test_scores))\n",
    "    # real strategic model & strategic data\n",
    "    Xtest_opt = strategic_model_real.optimize_X(Xtest)\n",
    "    test_scores = strategic_model_real.score(Xtest_opt)\n",
    "    accuracies[2] = (strategic_model_real.calc_accuracy(Ytest, test_scores))\n",
    "    # non strategic model & strategic data\n",
    "    Xtest_opt = non_strategic_model.optimize_X(Xtest)\n",
    "    accuracies[3] = (non_strategic_model.evaluate(Xtest_opt, Ytest))\n",
    "\n",
    "    pd.DataFrame(accuracies).to_csv(path + '/results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "x_dim = len(X[0])\n",
    "epochs = 7\n",
    "batch_size = 128\n",
    "\n",
    "small_eps = 0.05\n",
    "v_true = np.array([-1,-1,-1,-1,-1,-1,-1,1,1,0.1,1,0.1,0.1,1,0.1])\n",
    "funcs = {\"f\": f, \"g\": g, \"f_derivative\": f_derivative, \"c\": c, \"score\": score}\n",
    "\n",
    "epsilons = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "for eps in epsilons:\n",
    "    path = PATH + \"/\" + str(eps)\n",
    "    print(\"------------------------- {} -------------------------\".format(eps))\n",
    "\n",
    "    strategic_model_approx = MyStrategicModel(x_dim, funcs, TRAIN_SLOPE, EVAL_SLOPE, v_true, small_eps, eps, strategic=True)\n",
    "    strategic_model_approx.load_model(PATH + \"/strategic_approx/model.pt\")\n",
    "    \n",
    "    print(strategic_model_approx.eps_true)\n",
    "    strategic_model_approx.eps_true = eps\n",
    "    print(strategic_model_approx.eps_true)\n",
    "    \n",
    "   # approx strategic model & strategic data\n",
    "    Xtest_opt = strategic_model_approx.optimize_X(Xtest)\n",
    "    test_scores = strategic_model_approx.score(Xtest_opt)\n",
    "    print(strategic_model_approx.calc_accuracy(Ytest, test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "funcPred",
   "language": "python",
   "name": "funcpred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
