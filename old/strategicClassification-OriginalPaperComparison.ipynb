{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import cvxpy as cp\n",
    "import dccp\n",
    "import torch\n",
    "import numpy as np\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import zero_one_loss, confusion_matrix\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "XDIM = 15\n",
    "GAMING = 0.5\n",
    "EPSILON = 0.1\n",
    "SLOPE_C = 1\n",
    "X_LOWER_BOUND = -4\n",
    "X_UPPER_BOUND = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(N, informative_frac=1, shift_range=1, scale_range=1, noise_frac=0.01, seed=None):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    n_informative = informative_frac*XDIM\n",
    "    n_redundant = XDIM - n_informative\n",
    "    shift_arr = shift_range*np.random.randn(XDIM)\n",
    "    scale_arr = scale_range*np.random.randn(XDIM)\n",
    "    X, Y = make_classification(n_samples=N, n_features=XDIM, n_informative=n_informative, n_redundant=n_redundant,\n",
    "                               flip_y=noise_frac, shift=shift_arr, scale=scale_arr, random_state=seed)\n",
    "    Y[Y == 0] = -1\n",
    "    return torch.from_numpy(X), torch.from_numpy(Y)\n",
    "    \n",
    "def split_data(X, Y, percentage):\n",
    "    num_val = int(len(X)*percentage)\n",
    "    return X[num_val:], Y[num_val:], X[:num_val], Y[:num_val]\n",
    "\n",
    "def shuffle(X, Y):\n",
    "    data = torch.cat((X, Y), 1)\n",
    "    data = data[torch.randperm(data.size()[0])]\n",
    "    X = data[:, :2]\n",
    "    Y = data[:, 2]\n",
    "    return X, Y\n",
    "\n",
    "def conf_mat(Y1, Y2):\n",
    "    num_of_samples = len(Y1)\n",
    "    mat = confusion_matrix(Y1, Y2, labels=[-1, 1])*100/num_of_samples\n",
    "    acc = np.trace(mat)\n",
    "    return mat, acc\n",
    "\n",
    "def pred(X, w, b):\n",
    "    return torch.sign(score(X, w, b))\n",
    "\n",
    "def calc_accuracy(Y, Ypred):\n",
    "    num = len(Y)\n",
    "    temp = Y - Ypred\n",
    "    acc = len(temp[temp == 0])*1./num\n",
    "    return acc\n",
    "\n",
    "def evaluate_model(X, Y, w, b, ccp, strategic):\n",
    "    if not strategic:\n",
    "        Xopt = X\n",
    "    else:\n",
    "        Xopt = ccp.optimize_X(X, w, b)\n",
    "    Ypred = pred(Xopt, w, b)\n",
    "    return calc_accuracy(Y, Ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCP classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCP:\n",
    "    def __init__(self, funcs):\n",
    "        self.f_derivative = funcs[\"f_derivative\"]\n",
    "        self.g = funcs[\"g\"]\n",
    "        self.c = funcs[\"c\"]\n",
    "        \n",
    "        self.x = cp.Variable(XDIM)\n",
    "        self.xt = cp.Parameter(XDIM)\n",
    "        self.r = cp.Parameter(XDIM)\n",
    "        self.w = cp.Parameter(XDIM)\n",
    "        self.b = cp.Parameter(1)\n",
    "\n",
    "        target = self.x@self.f_derivative(self.xt, self.w, self.b) - self.g(self.x, self.w, self.b) - self.c(self.x, self.r)\n",
    "        constraints = [self.x >= X_LOWER_BOUND,\n",
    "                       self.x <= X_UPPER_BOUND]\n",
    "        self.prob = cp.Problem(cp.Maximize(target), constraints)\n",
    "        \n",
    "    def ccp(self, r, w, b):\n",
    "        \"\"\"\n",
    "        numpy to numpy\n",
    "        \"\"\"\n",
    "        self.w.value = w\n",
    "        self.b.value = b\n",
    "        self.xt.value = r\n",
    "        self.r.value = r\n",
    "        \n",
    "        result = self.prob.solve()\n",
    "        diff = np.linalg.norm(self.xt.value - self.x.value)\n",
    "        while diff > 0.0001:\n",
    "            self.xt.value = self.x.value\n",
    "            result = self.prob.solve()\n",
    "            diff = np.linalg.norm(self.x.value - self.xt.value)\n",
    "        return self.x.value\n",
    "    \n",
    "    def optimize_X(self, X, w, b):\n",
    "        \"\"\"\n",
    "        tensor to tensor\n",
    "        \"\"\"\n",
    "        w = w.detach().numpy()\n",
    "        b = b.detach().numpy()\n",
    "        X = X.numpy()\n",
    "        return torch.stack([torch.from_numpy(self.ccp(x, w, b)) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gain & Cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.array([-1,-1,-1,-1,-1,-1,-1,1,1,0.1,1,0.1,0.1,1,0.1])\n",
    "\n",
    "def score(x, w, b):\n",
    "    return x@w + b\n",
    "\n",
    "def f(x, w, b):\n",
    "    return 0.5*cp.norm(cp.hstack([1, (SLOPE_C*score(x, w, b) + 1)]), 2)\n",
    "\n",
    "def g(x, w, b):\n",
    "    return 0.5*cp.norm(cp.hstack([1, (SLOPE_C*score(x, w, b) - 1)]), 2)\n",
    "\n",
    "def c_true(x, r):\n",
    "    print(GAMING, EPSILON)\n",
    "    return 2*(1./GAMING)*(EPSILON*cp.sum_squares(x-r) + (1-EPSILON)*cp.pos((x-r) @ v))\n",
    "\n",
    "def c(x, r):\n",
    "    return 2*(1./GAMING)*(cp.pos((x-r) @ v))\n",
    "\n",
    "def f_derivative(x, w, b):\n",
    "    return 0.5*SLOPE_C*((SLOPE_C*score(x, w, b) + 1)/cp.sqrt((SLOPE_C*score(x, w, b) + 1)**2 + 1))*w\n",
    "\n",
    "def g_derivative(x, w, b):\n",
    "    return 0.5*SLOPE_C*((SLOPE_C*score(x, w, b) - 1)/cp.sqrt((SLOPE_C*score(x, w, b) - 1)**2 + 1))*w\n",
    "\n",
    "funcs = {\"f\": f, \"g\": g, \"f_derivative\": f_derivative, \"g_derivative\": g_derivative, \"c\": c, \"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spam_dataset():\n",
    "    path = r\"C:\\Users\\sagil\\Desktop\\nir project\\tip_spam_data\\IS_journal_tip_spam.arff\"\n",
    "    data, meta = arff.loadarff(path)\n",
    "    df = pd.DataFrame(data)\n",
    "    most_disc = ['qTips_plc', 'rating_plc', 'qEmail_tip', 'qContacts_tip', 'qURL_tip', 'qPhone_tip', 'qNumeriChar_tip', 'sentistrength_tip', 'combined_tip', 'qWords_tip', 'followers_followees_gph', 'qunigram_avg_tip', 'qTips_usr', 'indeg_gph', 'qCapitalChar_tip', 'class1']\n",
    "    df = df[most_disc]\n",
    "    df[\"class1\"].replace({b'spam': -1, b'notspam': 1}, inplace=True)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    Y = df['class1'].values\n",
    "    X = df.drop('class1', axis = 1).values\n",
    "    X -= np.mean(X, axis=0)\n",
    "    X /= np.std(X, axis=0)\n",
    "    return torch.from_numpy(X), torch.from_numpy(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent of positive samples: 49.44356120826709%\n"
     ]
    }
   ],
   "source": [
    "X, Y = load_spam_dataset()\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "assert(len(X[0]) == XDIM)\n",
    "X, Y, Xval, Yval = split_data(X, Y, 0.20)\n",
    "print(\"percent of positive samples: {}%\".format(100 * len(Y[Y == 1]) / len(Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(evaluate, loss, params, X, Y, Xval, Yval, opt, opt_kwargs={\"lr\":1e-3}, batch_size=128, epochs=100, verbose=False, callback=None, calc_train_errors=False):\n",
    "    \n",
    "    train_dset = TensorDataset(X, Y)\n",
    "    train_loader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "    opt = opt(params, **opt_kwargs)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "    \n",
    "    total_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        t1 = time.time()\n",
    "        \n",
    "        batch = 1\n",
    "        train_losses.append([])\n",
    "        train_errors.append([])\n",
    "        for Xbatch, Ybatch in train_loader:\n",
    "            opt.zero_grad()\n",
    "            l = loss(Xbatch, Ybatch)\n",
    "            l.backward()\n",
    "            opt.step()\n",
    "            train_losses[-1].append(l.item())\n",
    "            if calc_train_errors:\n",
    "                with torch.no_grad():\n",
    "                    e = evaluate(Xbatch, Ybatch)\n",
    "                    train_errors[-1].append(1-e)\n",
    "                if verbose:\n",
    "                    print(\"batch %03d / %03d | loss: %3.5f | err: %3.5f\" % \n",
    "                          (batch, len(train_loader), np.mean(train_losses[-1]), np.mean(train_errors[-1])))\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(\"batch %03d / %03d | loss: %3.5f\" %\n",
    "                          (batch, len(train_loader), np.mean(train_losses[-1])))\n",
    "            batch += 1\n",
    "            if callback is not None:\n",
    "                callback()\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            val_losses.append(loss(Xval, Yval).item())\n",
    "            val_errors.append(1-evaluate(Xval, Yval))\n",
    "            \n",
    "        t2 = time.time()\n",
    "        if verbose:\n",
    "            # print(t2-t1)\n",
    "            print(\"----- epoch %03d / %03d | time: %03d sec | loss: %3.5f | err: %3.5f\" % (epoch + 1, epochs, t2-t1, val_losses[-1], val_errors[-1]))\n",
    "    print(\"training time: {} seconds\".format(time.time()-total_time)) \n",
    "    return train_errors, val_errors, train_losses, val_losses\n",
    "\n",
    "def generate_delta_layer(funcs):\n",
    "    g = funcs[\"g\"]\n",
    "    c = funcs[\"c\"]\n",
    "    \n",
    "    x = cp.Variable(XDIM)\n",
    "    w = cp.Parameter(XDIM, value = np.random.randn(XDIM))\n",
    "    b = cp.Parameter(1, value = np.random.randn(1))\n",
    "    r = cp.Parameter(XDIM, value = np.random.randn(XDIM))\n",
    "    f_der = cp.Parameter(XDIM, value = np.random.randn(XDIM))\n",
    "\n",
    "    target = x@f_der - g(x, w, b) - c(x, r)\n",
    "    constraints = [x >= X_LOWER_BOUND,\n",
    "                   x <= X_UPPER_BOUND]\n",
    "    objective = cp.Maximize(target)\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    layer = CvxpyLayer(problem, parameters=[f_der, w, b, r], variables=[x])\n",
    "    \n",
    "    return layer\n",
    "\n",
    "def get_f_ders(XT, w, b):\n",
    "    \"\"\"\n",
    "    tensor to tensor\n",
    "    \"\"\"\n",
    "    return torch.stack([0.5*SLOPE_C*((SLOPE_C*score(xt, w, b) + 1)/torch.sqrt((SLOPE_C*score(xt, w, b) + 1)**2 + 1))*w for xt in XT])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccp = CCP(funcs)\n",
    "delta_layer = generate_delta_layer(funcs)\n",
    "\n",
    "def loss(X, Y, w, b, strategic=True):\n",
    "    if strategic:\n",
    "        XT = ccp.optimize_X(X, w, b)\n",
    "        f_der = get_f_ders(XT, w, b)\n",
    "        Xopt = delta_layer(f_der, w, b, X)[0] # Xopt should equal to XT but we do it again for the gradients\n",
    "        output = score(Xopt, w, b)\n",
    "        loss = torch.mean(torch.clamp(1 - output * Y, min=0))\n",
    "    else:\n",
    "        output = score(X, w, b)\n",
    "        loss = torch.mean(torch.clamp(1 - output * Y, min=0))\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on t:0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:163: UserWarning: You are solving a parameterized problem that is not DPP. Because the problem is not DPP, subsequent solves will not be faster than the first one. For more information, see the documentation on Discplined Parametrized Programming, at\n",
      "\thttps://www.cvxpy.org/tutorial/advanced/index.html#disciplined-parametrized-programming\n",
      "  warnings.warn(dpp_error_msg)\n",
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\diffcp\\cone_program.py:259: UserWarning: Solved/Inaccurate.\n",
      "  warnings.warn(\"Solved/Inaccurate.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:0.3\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:0.3\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagil\\Anaconda3\\envs\\funcPred\\lib\\site-packages\\cvxpy\\problems\\problem.py:1055: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:0.3\n",
      "training time: 2139.547792196274 seconds\n",
      "training on t:0.6\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:0.6\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:0.6\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:0.6\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:0.6\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:0.6\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:0.6\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:0.6\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:0.6\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:0.6\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:0.6\n",
      "training time: 2013.3601953983307 seconds\n",
      "training on t:0.9\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:0.9\n",
      "training time: 2180.625711917877 seconds\n",
      "training on t:1.2\n",
      "training time: 2228.230710029602 seconds\n",
      "training on t:1.5\n",
      "training time: 2361.614536523819 seconds\n",
      "training on t:1.8\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:1.8\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:1.8\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:1.8\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:1.8\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:1.8\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:1.8\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:1.8\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:1.8\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:1.8\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:1.8\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:1.8\n",
      "training time: 2158.1426842212677 seconds\n",
      "training on t:2.1\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:2.1\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:2.1\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:2.1\n",
      "training time: 2689.740416765213 seconds\n",
      "training on t:2.4\n",
      "training time: 1912.7725868225098 seconds\n",
      "training on t:2.7\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:2.7\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:2.7\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:2.7\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:2.7\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:2.7\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:2.7\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:2.7\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:2.7\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:2.7\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:2.7\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:2.7\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:2.7\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:2.7\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:2.7\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:2.7\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:2.7\n",
      "training time: 1909.122126340866 seconds\n",
      "training on t:3\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:3\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:3\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:3\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:3\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:3\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:3\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:3\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:3\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:3\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:3\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:3\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:3\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:3\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Failed\n",
      "training on t:3\n",
      "Failed\n",
      "training on t:3\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 7\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "gaming_list = [0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7, 3]\n",
    "for t in gaming_list:\n",
    "    failed = True\n",
    "    while failed:\n",
    "        try:\n",
    "            failed = False\n",
    "            print(\"training on t:{}\".format(t))\n",
    "            GAMING = t\n",
    "\n",
    "            w_strategic = torch.zeros(XDIM, requires_grad=True)\n",
    "            b_strategic = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "            train_errors, val_errors, train_losses, val_losses = fit(lambda X, Y: evaluate_model(X, Y, w_strategic, b_strategic, ccp, strategic=True), \n",
    "                                           lambda X, Y: loss(X, Y, w_strategic, b_strategic, strategic=True), [w_strategic, b_strategic], X, Y, Xval, Yval,\n",
    "                                           opt=torch.optim.Adam, opt_kwargs={\"lr\": (1e-2)},\n",
    "                                           batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=False, calc_train_errors=False)\n",
    "\n",
    "            w_strategic.requires_grad = False\n",
    "            b_strategic.requires_grad = False\n",
    "        except:\n",
    "            print(\"Failed\")\n",
    "            failed = True\n",
    "        \n",
    "    with open(\"classifiers_parameters.txt\", \"a+\") as file:\n",
    "        file.write(\"{}|{}|{}|{}|{}\".format(t, w_strategic, b_strategic, val_errors[-1], val_losses[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_ones(X):\n",
    "#     n = X.size()[0]\n",
    "#     ONES = torch.ones((n,1))\n",
    "#     return torch.cat((X, ONES), 1)\n",
    "\n",
    "# clf = svm.LinearSVC()\n",
    "\n",
    "# clf.fit(add_ones(X), Y)\n",
    "# w_svm = clf.coef_[0]\n",
    "# b_svm = w_svm[-1]\n",
    "# w_svm = w_svm[:-1]\n",
    "# print(w_svm, b_svm)\n",
    "# with open(\"classifiers_parameters.txt\", \"a+\") as file:\n",
    "#             file.write(\"svm|svm|{}|{}|NA|NA\".format(w_svm, b_svm))\n",
    "\n",
    "models = {}\n",
    "with open(\"classifier_params.txt\", \"r+\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for line in lines:\n",
    "\n",
    "    lst = line.split(\"|\")\n",
    "    gaming = lst[0]\n",
    "    w = lst[1].replace(\"\\n\", \"\").split(\",\")\n",
    "    w = list(map(float, w))\n",
    "    b = [float(lst[2])]\n",
    "    w = torch.Tensor(w)\n",
    "    b = torch.Tensor(b)\n",
    "    models[gaming] = (torch.Tensor(w), torch.Tensor(b))\n",
    "    \n",
    "funcs[\"c\"] = c_true\n",
    "gaming_list = [0.5, 1, 1.5, 2, 2.5, 3]\n",
    "epsilon_list = [0.1, 0.3]\n",
    "for e in epsilon_list:\n",
    "    for t in gaming_list:\n",
    "        print(\"evaluating \", e, t) \n",
    "        GAMING = t\n",
    "        EPSILON = e\n",
    "        ccp = CCP(funcs)\n",
    "        our_w, our_b = models[str(t)]\n",
    "        svm_w, svm_b = models['svm']\n",
    "        our_accuracy = evaluate_model(Xval, Yval, our_w, our_b, ccp, strategic=True)\n",
    "        svm_accuracy = evaluate_model(Xval, Yval, svm_w, svm_b, ccp, strategic=True)\n",
    "        \n",
    "        with open(\"experiment_results.txt\", \"a+\") as file:\n",
    "            file.write(\"ours|{}|{}|{}\\n\".format(e, t, our_accuracy))\n",
    "            file.write(\"svm|{}|{}|{}\\n\".format(e, t, svm_accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "funcPred",
   "language": "python",
   "name": "funcpred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
